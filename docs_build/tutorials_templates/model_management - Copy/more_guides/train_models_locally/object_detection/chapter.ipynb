{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Training an Object Detection Model with YOLOv8  \n", "In this tutorial we will download a public model from the marketplace to inference and train on custom data locally.  \n", "Here we will use a YOLOv8 model.  \n", "  \n", "Create a venv and install the requirements for the yolov8 package [here](https://github.com/dataloop-ai-apps/yolov8/blob/master/requirements.txt/)  \n", "Then, import the modules required for the scripts in this tutorial.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# !pip install torch torchvision imgaug \"scikit-image<0.18\"\n", "import matplotlib.pyplot as plt\n", "from PIL import Image\n", "import numpy as np\n", "import dtlpy as dl\n", "import json\n", "from dtlpy.ml import train_utils\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create a Project and a Dataset  \n", "We will use a public fruits dataset. We create a project and a dataset and upload the data with 3 labels of fruit.  \n", "NOTE: You might need to change the location of the items, which currently points to the root of the documentation repository. If you downloaded the dtlpy documentation repo locally, this should work as-is.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["project = dl.projects.create('Fruit - Model Mgmt')\n", "dataset = project.datasets.create('Fruit')\n", "dataset.to_df()\n", "_ = dataset.items.upload(local_path='../../../../assets/sample_datasets/FruitImage/items/*',\n", "                         local_annotations_path='../../../../assets/sample_datasets/FruitImage/json')\n", "dataset.add_labels(label_list=['orange', 'banana', 'apple'])\n", "dataset.open_in_web()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we'll add the train and validation sets to the dataset metadata:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["subsets = {'train': json.dumps(dl.Filters(field='dir', values='/train').prepare()),\n", "           'validation': json.dumps(dl.Filters(field='dir', values='/validation').prepare())}\n", "dataset.metadata['system']['subsets'] = subsets\n", "dataset.update(True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clone the Public Model Into Your Project  \n", "We'll get and clone the public yolo pretrained model (you can view the public models in the public Dataloop Github).  \n", "You can view all publicly available models by using a Filter. Here we will use a YOLOv8 model pretrained on the COCO dataset.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "filters = dl.Filters(resource=dl.FiltersResource.MODEL, use_defaults=False)\n", "filters.add(field='scope', values='public')\n", "dl.models.list(filters=filters).to_df()\n", "# get the public model\n", "pretrained_model = dl.models.get(model_name='pretrained-yolo-v8')\n", "", "model = pretrained_model.clone(model_name='fruits-model',\n", "                               dataset=dataset,\n", "                               project_id=project.id,\n", "                               configuration={'epochs': 10,\n", "                                              'batch_size': 4,\n", "                                              'imgz': 640,\n", "                                              'id_to_label_map': {(v - 1): k for k, v in\n", "                                                                  dataset.instance_map.items()}\n", "                                              })\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train on Your Dataset  \n", "We'll load the new, untrained model into the adapter and prepare the local dataset to be used for training.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["adapter = dl.packages.build(package=model.package,\n", "                            init_inputs={'model_entity': model},\n", "                            module_name='model-adapter')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Start the Training  \n", "The package, model, and data are now prepared. We are ready to train!  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["print(\"Training {!r} on dataset {!r}\".format(model.name, dataset.name))\n", "adapter.train_model(model=model)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can list all Artifacts associated with this Package, and add more files that are needed to load or run the model.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["adapter.model.artifacts.list_content()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Predict Your Newly Trained Model  \n", "With everything in place, we will load our model and view an item's prediction.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["item = dl.items.get(item_id='6110d4a41467ded7a8c2a23d')\n", "annotations = adapter.predict_items([item], with_upload=True)\n", "image = Image.open(item.download())\n", "plt.imshow(item.annotations.show(image,\n", "                                 thickness=5))\n", "print('Classes found: {}'.format([ann.label for ann in annotations[0]]))\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}