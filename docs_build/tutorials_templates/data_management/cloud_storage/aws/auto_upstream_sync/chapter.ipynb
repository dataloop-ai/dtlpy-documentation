{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Dataset Binding with AWS  \n", "  \n", "We will create an AWS Lambda to continuously sync a bucket with Dataloop's dataset  \n", "  \n", "If you want to catch events from the AWS bucket and update the Dataloop Dataset you need to set up a Lambda.  \n", "The Lambda will catch the AWS bucket events and will reflect them into the Dataloop Platform.  \n", "  \n", "We have prepared an environment zip file with our SDK for python3.8, so you don't need to create anything else to use dtlpy in the lambda.  \n", "  \n", "NOTE: For any other custom use (e.g. other python version or more packages) try creating your own layer (We used [this](https://www.geeksforgeeks.org/how-to-install-python-packages-for-aws-lambda-layers) tutorial and the python:3.8 docker image).  \n", "  \n", "### Create the Lambda  \n", "1. Create a new Lambda  \n", "2. The default timeout is 3[s] so we'll need to change to 1[m] (1 Minute):  \n", "    Configuration \u2192 General configuration \u2192 Edit \u2192 Timeout  \n", "  \n", "3. Go to the Lambda console -> Select your function -> Configuration -> (Left-side panel) Environment variables -> Edit -> Add environment variable  \n", "       * Add the 3 secrets vars `DATASET_ID`, `DTLPY_USERNAME`, `DTLPY_PASSWORD`  \n", "    To populate the values for the vars: `DTLPY_USERNAME`, `DTLPY_PASSWORD` you'll need to create a **DataLoop Bot** on your Dataloop project using the following code:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "dl.login()\n", "project = dl.projects.get(project_name='project name')\n", "bot = project.bots.create(name='serviceAccount', return_credentials=True)\n", "print('username: ', bot.id)\n", "print('password: ', bot.password)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Copy the following code:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import os\n", "import urllib.parse\n", "# Set dataloop path to tmp (to read/write from the lambda)\n", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "import dtlpy as dl\n", "DATASET_ID = os.environ.get('DATASET_ID')\n", "DTLPY_USERNAME = os.environ.get('DTLPY_USERNAME')\n", "DTLPY_PASSWORD = os.environ.get('DTLPY_PASSWORD')\n", "", "def lambda_handler(event, context):\n", "    dl.login_m2m(email=DTLPY_USERNAME, password=DTLPY_PASSWORD)\n", "    dataset = dl.datasets.get(dataset_id=DATASET_ID)\n", "    driver_path = dl.drivers.get(driver_id=dataset.driver).path\n", "    for record in event['Records']:\n", "        # Get the bucket name\n", "        bucket = record['s3']['bucket']['name']\n", "        # Get the file name\n", "        filename = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')\n", "        remote_path = None\n", "        if driver_path == '/':\n", "            driver_path = None\n", "        if driver_path is not None and driver_path not in filename:\n", "            return\n", "        if driver_path:\n", "            remote_path = filename.replace(driver_path, '')\n", "        else:\n", "            remote_path = filename\n", "        if 'ObjectRemoved' in record['eventName']:\n", "            # On delete event - delete the item from Dataloop\n", "            try:\n", "                dtlpy_filename = '/' + remote_path\n", "                filters = dl.Filters(field='filename', values=dtlpy_filename)\n", "                dataset.items.delete(filters=filters)\n", "            except Exception as e:\n", "                raise e\n", "        elif 'ObjectCreated' in record['eventName']:\n", "            # On create event - add a new item to the Dataset\n", "            try:\n", "                # upload the file\n", "                path = 'external://' + filename\n", "                # dataset.items.upload(local_path=path, overwrite=True) # if overwrite is required\n", "                dataset.items.upload(local_path=path, remote_path=remote_path)\n", "            except Exception as e:\n", "                raise e\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Add a Layer to the Lambda  \n", "We have created an AWS Layer with the Dataloop SDK ready. Click [here](https://storage.googleapis.com/dtlpy/aws-python3.8-lambda-layer/layer.zip) to download the zip file.  \n", "Because the layer's size is larger than 50MB you cannot use it directly (AWS restrictions), but need to upload it to a bucket first.  \n", "Once uploaded, create a new layer for the dtlpy env:  \n", "1. Go to the layers screen and \"click Add Layer\".  \n", "![add_layer](../../../../../assets/bind_aws/create_layer.png)  \n", "2. Choose a name (dtlpy-env).  \n", "3. Use the link to the bucket layer.zip.  \n", "4. Select the env (x86_64, python3.8).  \n", "5. Click \"Create\" and the bottom on the page.  \n", "  \n", "Go back to your lambda and add the layer:  \n", "1. Select the \"Add Layer\".  \n", "![add_layer](../../../../../assets/bind_aws/add_layer.png)  \n", "2. Choose \"Custom layer\" and select the Layer you've added and the version.  \n", "3. click \"Add\" at the bottom.  \n", "  \n", "### Create the Bucket Events  \n", "Go to the bucket you are using, and create the event:  \n", "1. Go to Properties \u2192 Event notifications \u2192 Create event notification  \n", "2. Choose a name for the Event  \n", "3. For Event types choose: All object create events, All object delete events  \n", "4. Destination - Lambda function \u2192 Choose from your Lambda functions \u2192 choose the function you build \u2192 SAVE  \n", "  \n", "Deploy and you're good to go!  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}