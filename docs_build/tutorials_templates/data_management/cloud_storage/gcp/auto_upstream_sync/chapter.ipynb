{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Dataset Binding with Google Cloud Storage  \n", "  \n", "We will create a GCP cloud function to continuously sync a bucket with Dataloop's Dataset  \n", "  \n", "If you want to catch events from the GCS bucket and update the Dataloop Dataset you need to set up a Cloud function.  \n", "The function will catch the GCS bucket events and will reflect them into the Dataloop Platform.  \n", "  \n", "If you are familiar with [GCP Cloud Functions](https://cloud.google.com/functions), you can just use our integration function below.  \n", "  \n", "We assume you already have a GCP account. If you don't, follow the [GCP docs](https://cloud.google.com/docs/get-started) and create it.  \n", "  \n", "### Create the Cloud Function  \n", "1. Create a GCS bucket  \n", "  \n", "    NOTE: This bucket should be used as the external storage for the Dataloop dataset.  \n", "  \n", "2. Go to Cloud Functions and click Create Function -> to create a new function  \n", "3. Basic  \n", "   * Environment -> 1st gen Subscription  \n", "   * Choose Function Name  \n", "   * Choose the function region  \n", "4. Trigger  \n", "   * Trigger type -> Cloud Storage  \n", "   * Event type ->  On (finalizing/creating) file in the selected bucket  \n", "   * Bucket -> Choose the bucket you would like allow auto sync with Dataloop  \n", "   * Click Save  \n", "5. Runtime, build, connections and security settings  \n", "   * Choose the Runtime tab  \n", "   * Runtime environment variable -> Add variable  \n", "   * Add the 3 environment variables `DATASET_ID`, `DTLPY_USERNAME` and `DTLPY_PASSWORD`  \n", "     * To populate the values for the vars: `DTLPY_USERNAME`, `DTLPY_PASSWORD` you'll need to create a **DataLoop Bot** on your Dataloop project using the following code:  \n", "    * The output:  \n", "      * username -> `DTLPY_USERNAME`  \n", "      * password -> `DTLPY_PASSWORD`  \n", "    * After adding all environment variables -> Next  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "dl.login()\n", "project = dl.projects.get(project_name='project name')\n", "bot = project.bots.create(name='serviceAccount', return_credentials=True)\n", "print('username: ', bot.id)\n", "print('password: ', bot.password)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["6. Code  \n", "   * Runtimes: => python 3.7  \n", "   * Entry point: Your function name from the code snippet (default `create_gcs`)  \n", "   * In the requirements.txt file -> add `dtlpy`  \n", "   * Copy the following code to the main.py file:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import os\n", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "import dtlpy as dl\n", "dataset_id = os.environ.get('DATASET_ID')\n", "dtlpy_username = os.environ.get('DTLPY_USERNAME')\n", "dtlpy_password = os.environ.get('DTLPY_PASSWORD')\n", "", "def create_gcs(event, context):\n", "    \"\"\"Triggered by a change to a Cloud Storage bucket.\n", "    Args:\n", "         event (dict): Event payload.\n", "         context (google.cloud.functions.Context): Metadata for the event.\n", "    \"\"\"\n", "    file = event\n", "    dl.login_m2m(email=dtlpy_username, password=dtlpy_password)\n", "    dataset = dl.datasets.get(dataset_id=dataset_id,\n", "                              fetch=False  # to avoid GET the dataset each time\n", "                              )\n", "    driver_path = dl.drivers.get(driver_id=dataset.driver).path\n", "    remote_path = None\n", "    if driver_path == '/':\n", "        driver_path = None\n", "    if driver_path is not None and driver_path not in file['name']:\n", "        return\n", "    if driver_path:\n", "        remote_path = file['name'].replace(driver_path, '')\n", "    file_name = 'external://' + file['name']\n", "    dataset.items.upload(local_path=file_name, remote_path=remote_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* Click -> Deploy  \n", "  \n", "  \n", "7. Add another function for delete actions  \n", "   * Repeat the process  \n", "   * Create another function for `delete` with `delete event` with the following code and the same settings  \n", "   * Trigger -> Event type ->  On (deleting) file in the selected bucket  \n", "   * Entry point: Your function name from the code snippet (default `delete_gcs`)  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import os\n", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "import dtlpy as dl\n", "dataset_id = os.environ.get('DATASET_ID')\n", "dtlpy_username = os.environ.get('DTLPY_USERNAME')\n", "dtlpy_password = os.environ.get('DTLPY_PASSWORD')\n", "", "def delete_gcs(event, context):\n", "    \"\"\"Triggered by a change to a Cloud Storage bucket.\n", "    Args:\n", "         event (dict): Event payload.\n", "         context (google.cloud.functions.Context): Metadata for the event.\n", "    \"\"\"\n", "    file = event\n", "    dl.login_m2m(email=dtlpy_username, password=dtlpy_password)\n", "    dataset = dl.datasets.get(dataset_id=dataset_id,\n", "                              fetch=False  # to avoid GET the dataset each time\n", "                              )\n", "    driver_path = dl.drivers.get(driver_id=dataset.driver).path\n", "    if driver_path == '/':\n", "        driver_path = None\n", "    if driver_path is not None and driver_path not in file['name']:\n", "        return\n", "    if driver_path:\n", "        remote_path = file['name'].replace(driver_path, '')\n", "    else:\n", "        remote_path = file['name']\n", "    dataset.items.delete(filename=remote_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### You're good to go!  \n", "  \n", "  \n", "  \n", "#### For pictures examples:  \n", "  \n", "![add_layer](../../../../../assets/bind_gcs/create_function.png)  \n", "![add_layer](../../../../../assets/bind_gcs/settings.png)  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}