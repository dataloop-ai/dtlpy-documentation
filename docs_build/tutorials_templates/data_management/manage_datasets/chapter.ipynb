{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Manage Datasets  \n", "  \n", "Datasets are buckets in the dataloop system that hold a collection of data items of any type, regardless of their  \n", "storage location (on Dataloop storage or external cloud storage).  \n", "  \n", "## Create Dataset  \n", "  \n", "You can create datasets within a project. There are no limits to the number of dataset a project can have, which  \n", "correlates with data versioning where datasets can be cloned and merged.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["dataset_name = 'my-dataset-name'\n", "", "", "dataset = project.datasets.create(dataset_name=dataset_name)\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create Dataset With Cloud Storage Driver  \n", "  \n", "If you\u2019ve created an integration and driver to your cloud storage, you can create a dataset connected to that driver. A  \n", "single integration (for example: S3) can have multiple drivers (per bucket or even per folder), so you need to specify  \n", "that.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["project_name = 'my-project-name'\n", "driver = 'my_driver_name'\n", "dataset_name = 'my_dataset_name'\n", "", "", "project = dl.projects.get(project_name=project_name)\n", "# Get your drivers list\n", "project.drivers.list().print()\n", "# Create a dataset from a driver name. You can also create by the driver ID.\n", "dataset = project.datasets.create(driver=driver, dataset_name=dataset_name)\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["  \n", "## Retrieve Datasets  \n", "  \n", "You can query over datasets with a DQL, filter over fields to get a project's datasets:  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["dataset_id = 'my-dataset-id'\n", "", "", "filters = dl.Filters(resource=dl.FiltersResource.DATASET)\n", "filters.add(field='name', values='my dataset')\n", "datasets = project.datasets.list(filters=filters)\n", "datasets.print()\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["  \n", "## Create Directory  \n", "  \n", "A dataset can have multiple directories, allowing you to manage files by context, such as upload time, working batch,  \n", "source, etc.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["directory = '/directory/name'\n", "", "", "dataset.items.make_dir(directory=directory)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Deep Copy a Folder to Another Dataset  \n", "  \n", "You can create a clone of a folder into a new dataset, but if you want to actually move between datasets a folder with  \n", "files that are stored in the Dataloop system, you\u2019ll need to download the files and upload again to the destination dataset.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["source_folder = '/source_folder'\n", "destination_folder = '/destination_folder'\n", "source_project_name = 'source_project_name'\n", "source_dataset_name = 'source_dataset_name'\n", "destination_project_name = 'destination_project_name'\n", "destination_dataset_name = 'destination_dataset_name'\n", "", "", "copy_annotations = True\n", "flat_copy = False  # if true, it copies all dir files and sub dir files to the destination folder without sub directories\n", "# Get source project dataset\n", "project = dl.projects.get(project_name=source_project_name)\n", "dataset_from = project.datasets.get(dataset_name=source_dataset_name)\n", "source_folder = source_folder.rstrip('/')\n", "# Filter to get all files of a specific folder\n", "filters = dl.Filters()\n", "filters.add(field='filename', values=source_folder + '/**')  # Get all items in folder (recursive)\n", "pages = dataset_from.items.list(filters=filters)\n", "# Get destination project and dataset\n", "project = dl.projects.get(project_name=destination_project_name)\n", "dataset_to = project.datasets.get(dataset_name=destination_dataset_name)\n", "# Go over all projects and copy file from src to dst\n", "for page in pages:\n", "    for item in page:\n", "        # Download item (without save to disk)\n", "        buffer = item.download(save_locally=False)\n", "        # Give the item's name to the buffer\n", "        if flat_copy:\n", "            buffer.name = item.name\n", "        else:\n", "            buffer.name = item.filename[len(source_folder) + 1:]\n", "        # Upload item\n", "        print(\"Going to add {} to {} dir\".format(buffer.name, destination_folder))\n", "        new_item = dataset_to.items.upload(local_path=buffer, remote_path=destination_folder)\n", "        if not isinstance(new_item, dl.Item):\n", "            print('The file {} could not be upload to {}'.format(buffer.name, destination_folder))\n", "            continue\n", "        print(\"{} has been uploaded\".format(new_item.filename))\n", "        if copy_annotations:\n", "            new_item.annotations.upload(item.annotations.list())\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}