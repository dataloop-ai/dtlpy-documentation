{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP (Contrastive Language-Image Pre-Training) Model Adapter Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide on fine-tuning a CLIP model using the Dataloop platform and its Python SDK. CLIP models are powerful for tasks involving understanding the relationship between images and text, such as zero-shot image classification, image-text retrieval, and generating image embeddings for semantic search.\n",
    "\n",
    "Fine-tuning allows you to adapt a pre-trained CLIP model to your specific dataset and domain, potentially improving its performance on tasks relevant to your data. This tutorial will walk you through:\n",
    "\n",
    "1. Preparing a dataset with images and their corresponding textual descriptions.\n",
    "2. Using the Dataloop CLIP model adapter to fine-tune the model on your prepared dataset.\n",
    "3. Deploying the fine-tuned model and using it to generate embeddings for your images.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Install Dependencies](#install-dependencies)\n",
    "2. [Import Required Libraries](#import-libraries)\n",
    "3. [Set Up Dataloop Environment](#setup-environment)\n",
    "4. [Prepare Dataset for Fine-Tuning](#prepare-dataset)\n",
    "    *   [4.1 Option A: Use Public Dataset (Mars Surface Images)](#use-public-dataset)\n",
    "        *   [4.1.1 Install Mars Surface Images DPK](#install-mars-dpk)\n",
    "        *   [4.1.2 Get Captioned Dataset and Split for ML](#get-mars-dataset-split)\n",
    "    *   [4.2 Option B: (Alternative) Upload and Prepare Your Custom Dataset](#upload-custom-dataset)\n",
    "    *   [4.3 Convert Image Dataset to Prompt Item Format](#convert-to-prompt-items)\n",
    "5. [Fine-Tune and Deploy CLIP Model](#finetune-deploy-clip)\n",
    "    *   [5.1 Install CLIP Model Package (DPK)](#install-clip-model-dpk)\n",
    "    *   [5.2 Configure and Clone Pretrained CLIP Model](#configure-clone-clip-model)\n",
    "    *   [5.3 Train the Model](#train-clip-model)\n",
    "    *   [5.4 Deploy the Fine-Tuned Model](#deploy-clip-model)\n",
    "    *   [5.5 Embed Datasets using the Fine-Tuned Model](#embed-datasets-clip)\n",
    "6. [Conclusion](#conclusion)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='install-dependencies'></a>1. Install Dependencies\n",
    "\n",
    "First, ensure that the necessary Python libraries are installed. This notebook requires `dtlpy` for interacting with the Dataloop platform and `pandas` for data manipulation. The following cell will install or upgrade them.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtlpy pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='import-libraries'></a>2. Import Required Libraries\n",
    "\n",
    "Now, we import all the Python libraries that will be used throughout this tutorial.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import dtlpy as dl\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='setup-environment'></a>3. Set Up Dataloop Environment\n",
    "\n",
    "We need to set up our Dataloop environment and get our project. You'll need to replace project and dataset names with your own values.\n",
    "\n",
    "> **_NOTE:_**  This tutorial assumes you are working in a new project which does NOT have the CLIP model previously installed. If it's an existing project and you already have CLIP installed, you will need to get the appropriate app and base CLIP model entity for the rest of the code to work correctly.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "\n",
    "PROJECT_NAME = \"<your project name here>\"\n",
    "project = dl.projects.create(project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Required:** In the cell above, replace `\"<your project name here>\"` with the desired name for your Dataloop project. If a project with this name already exists, the SDK will retrieve it; otherwise, a new project will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='prepare-dataset'></a>4. Prepare Dataset for Fine-Tuning\n",
    "\n",
    "Fine-tuning a CLIP model requires a dataset of images paired with relevant textual descriptions. This section covers two ways to prepare such a dataset:\n",
    "1.  **Option A:** Use a publicly available dataset from the Dataloop Marketplace (Mars Surface Images with Captions).\n",
    "2.  **Option B:** Upload your own custom dataset of images and descriptions.\n",
    "\n",
    "Once the image dataset is ready, we will convert it into a special \"prompt item\" format suitable for training the CLIP model adapter.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='use-public-dataset'></a>4.1 Option A: Use Public Dataset (Mars Surface Images)\n",
    "\n",
    "For this tutorial we will install the Mars Surface Images Datasets from the Dataloop Marketplace. This dataset includes images with descriptions pre-loaded in the item metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='install-mars-dpk'></a>4.1.1 Install Mars Surface Images DPK\n",
    "\n",
    "This Dataloop Package (DPK) contains datasets related to Mars surface imagery, including one with captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpk = dl.dpks.get(dpk_name=\"mars-surface-images\")\n",
    "app = project.apps.install(dpk=dpk)\n",
    "print(f\"Mars Surface Datasets installed: {app.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='get-mars-dataset-split'></a>4.1.2 Get Captioned Dataset and Split for ML\n",
    "\n",
    "After installing the DPK, the \"Mars Surface Images with Captions\" dataset should be available in your project. We will retrieve this dataset and split its items into training, validation, and test subsets. This splitting is crucial for proper model training and evaluation.\n",
    "\n",
    "You may need to wait a few minutes after installing the app until the dataset has completed loading into your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "project = dl.projects.get(\"test clip FT8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.datasets.get(dataset_name=\"Mars Surface Images with Captions\")\n",
    "\n",
    "SUBSET_PERCENTAGES = {'train': 80, 'validation': 10, 'test': 10}\n",
    "dataset.split_ml_subsets(\n",
    "        percentages=SUBSET_PERCENTAGES\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='upload-custom-dataset'></a>4.2 Option B: (Alternative) Upload and Prepare Your Custom Dataset\n",
    "\n",
    "If you have your own dataset of images and corresponding text descriptions, you can use the function below to create a new Dataloop dataset, upload your images, and associate the descriptions. \n",
    "\n",
    "The function expects a Pandas DataFrame (`pairs_df`) with two columns:\n",
    "-   `'filepath'`: The local path to each image file.\n",
    "-   `'description'`: The text description for that image.\n",
    "\n",
    "It also assumes that for each image file (e.g., `items/image1.jpg`), there is a corresponding JSON annotation file (e.g., `json/image1.json`) if you have existing annotations to upload. If not, the `local_annotations_path` part can be adapted.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(dataset_name, pairs_df, subset_percentages={'train': 80, 'validation': 10, 'test': 10}):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from a CSV file containing image paths and descriptions\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to create\n",
    "        pairs_df (pd.DataFrame): DataFrame containing 'filepath' and 'img_description' columns\n",
    "        subset_percentages (dict): Dictionary containing the percentages for each subset\n",
    "        default is 80% train, 10% validation, 10% test\n",
    "        can be changed to any other percentages as long as the sum is 100\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dataset = project.datasets.create(dataset_name=dataset_name)\n",
    "    except dl.exceptions.BadRequest:\n",
    "        # Generate 5 random alphanumeric characters\n",
    "        suffix = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "        dataset = project.datasets.create(dataset_name=f\"{dataset_name}_{suffix}\")\n",
    "\n",
    "    def upload_item(row):\n",
    "        file_path = row[\"filepath\"]\n",
    "        annots_path = file_path.replace(\"items\", \"json\") # This assumes a specific structure for annotation files\n",
    "        \n",
    "        # Upload item with annotations\n",
    "        item = dataset.items.upload(\n",
    "            local_path=file_path,\n",
    "            local_annotations_path=annots_path,\n",
    "            item_metadata=dl.ExportMetadata.FROM_JSON,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        # Set description and update\n",
    "        item.set_description(text=row[\"description\"])\n",
    "        item.update()\n",
    "\n",
    "    # Use ThreadPoolExecutor to upload items in parallel with progress bar\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        from tqdm import tqdm\n",
    "        list(tqdm(\n",
    "            executor.map(upload_item, [row for _, row in pairs_df.iterrows()]),\n",
    "            total=len(pairs_df),\n",
    "            desc=\"Uploading items\",\n",
    "            unit=\"item\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "        ))\n",
    "\n",
    "    # Since model training requires labels, we create a dummy label for the recipe\n",
    "    dataset.add_labels(label_list=['free-text'])\n",
    "    # After uploading, you would also split this custom dataset similarly to Option A:\n",
    "    # dataset.split_ml_subsets(percentages=subset_percentages)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='convert-to-prompt-items'></a>4.3 Convert Image Dataset to Prompt Item Format\n",
    "\n",
    "The Dataloop CLIP model adapter expects data in a specific \"prompt item\" format. A prompt item typically links an image (the prompt) to its textual description (the response or annotation).\n",
    "\n",
    "The `ClipPrepare` class below, adapted from the [CLIP model adapter repository](https://github.com/dataloop-ai-apps/clip-model-adapter/blob/main/utils/prepare_dataset.py), provides the necessary functions to convert your image dataset (whether from Option A or B) into this prompt item format. It creates a new dataset containing these prompt items.\n",
    "\n",
    "Images are uploaded as regular items first, and then prompt items are created that reference these images and include their descriptions as text annotations.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipPrepare:\n",
    "    @staticmethod\n",
    "    def convert_dataset(dataset, keep_subsets=None):\n",
    "        dataset_to = ClipPrepare.convert_to_prompt_dataset(dataset_from=dataset, keep_subsets=keep_subsets)\n",
    "        return dataset_to\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_prompt_dataset(dataset_from: dl.Dataset, keep_subsets):\n",
    "        items = dataset_from.items.list()\n",
    "        try:\n",
    "            dataset_to = dataset_from.project.datasets.create(dataset_name=f\"{dataset_from.name} prompt items\")\n",
    "        except Exception as e:\n",
    "            print(f\"Prompt item dataset already exists or error: {e}. Creating new prompt item dataset with suffix.\")\n",
    "            suffix = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n",
    "            dataset_to = dataset_from.project.datasets.create(dataset_name=f\"{dataset_from.name} prompt items-{suffix}\")\n",
    "\n",
    "        # use thread multiprocessing to get items and convert them to prompt items\n",
    "        all_items = items.all()\n",
    "        print(f\"Converting {len(all_items)} items from dataset '{dataset_from.name}' to prompt items in dataset '{dataset_to.name}'...\")\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Using a simple loop with print statements for progress instead of tqdm if it causes issues in some environments\n",
    "            # For progress with tqdm, ensure it's installed and handles exceptions properly.\n",
    "            results = list(executor.map(lambda item: ClipPrepare._convert_item(item_id=item.id, dataset=dataset_to, existing_subsets=keep_subsets), all_items))\n",
    "        print(f\"Conversion completed. {len(results)} prompt items created.\")\n",
    "\n",
    "        # Copy recipe from original dataset\n",
    "        if dataset_from.get_recipe_ids():\n",
    "            new_recipe_id = dataset_from.get_recipe_ids()[0]\n",
    "            dataset_to.switch_recipe(recipe_id=new_recipe_id)\n",
    "            print(f\"Switched recipe for '{dataset_to.name}' to recipe ID: {new_recipe_id}\")\n",
    "        else:\n",
    "            print(f\"Warning: Original dataset '{dataset_from.name}' has no recipes. Prompt dataset '{dataset_to.name}' may need a recipe manually.\")\n",
    "\n",
    "        return dataset_to\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_item(item_id, dataset: dl.Dataset, existing_subsets=True):\n",
    "        item = dl.items.get(item_id=item_id)\n",
    "        if item.description is not None and item.description.strip() != '':\n",
    "            caption = item.description\n",
    "        else:\n",
    "            # Fallback if description is empty or missing\n",
    "            # print(f\"Item {item.id} ('{item.name}') has no valid description. Trying directory name or using placeholder.\")\n",
    "            item_dir_name = Path(item.dir).name\n",
    "            if item_dir_name and item_dir_name != '/' and item_dir_name.strip() != '':\n",
    "                # print(f\"Using directory name for item {item.id}: {item_dir_name}\")\n",
    "                caption = f\"a photo of a {item_dir_name.replace('_', ' ')}\" # Basic caption from directory\n",
    "            else:\n",
    "                # print(f\"Item {item.id} ('{item.name}') has no description or usable directory name. Using placeholder caption.\")\n",
    "                caption = \"an image\" # Placeholder caption\n",
    "        \n",
    "        new_name = Path(item.name).stem + '.json'\n",
    "\n",
    "        prompt_item_payload = dl.PromptItem(name=new_name)\n",
    "        # User (prompt) part: the image\n",
    "        prompt_item_payload.add(message={\"content\": [{\"mimetype\": dl.PromptType.IMAGE, \"value\": item.stream}]})\n",
    "        \n",
    "        # Assistant (response) part: the caption\n",
    "        prompt_item_payload.add(message={\"role\": \"assistant\", \n",
    "                                          \"content\": [{\"mimetype\": dl.PromptType.TEXT, \"value\": caption}]})\n",
    "        \n",
    "        new_metadata = item.metadata.copy() if item.metadata else {}\n",
    "        if existing_subsets and \"system\" in item.metadata and \"subsets\" in item.metadata[\"system\"]:\n",
    "            new_metadata.setdefault(\"system\", {}).setdefault(\"subsets\", item.metadata[\"system\"][\"subsets\"])\n",
    "        \n",
    "        try:\n",
    "            new_item = dataset.items.upload(\n",
    "                local_path=prompt_item_payload, # Upload the PromptItem object directly\n",
    "                remote_name=new_name,\n",
    "                remote_path=item.dir, # Store in the same directory structure\n",
    "                overwrite=True,\n",
    "                item_metadata=new_metadata,\n",
    "            )\n",
    "            return new_item\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading prompt item for original item {item.id} ('{item.name}'): {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset (either public or custom) is prepared and the `ClipPrepare` class is defined, you can execute the conversion. This step will create a new dataset suffixed with \"prompt items\", containing the data in the format required for CLIP fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'dataset' variable refers to your prepared image dataset (from Option A or B)\n",
    "# For example, if you used Option A (Mars dataset):\n",
    "# dataset = project.datasets.get(dataset_name=\"Mars Surface Images with Captions\")\n",
    "# Or if you used Option B (custom dataset) and named it 'MyCustomImageData':\n",
    "# dataset = project.datasets.get(dataset_name='MyCustomImageData')\n",
    "\n",
    "if 'dataset' in locals() and dataset is not None:\n",
    "    print(f\"Starting conversion for dataset: '{dataset.name}' (ID: {dataset.id})\")\n",
    "    prompt_dataset = ClipPrepare.convert_dataset(dataset=dataset, keep_subsets=True)\n",
    "    print(f\"Successfully created prompt item dataset: '{prompt_dataset.name}' (ID: {prompt_dataset.id})\")\n",
    "    prompt_dataset.open_in_web() # Open the new dataset in the Dataloop platform\n",
    "else:\n",
    "    print(\"Error: 'dataset' variable is not defined. Please ensure you have run the dataset preparation steps (Option A or B).\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, you should have two datasets in your Dataloop project:\n",
    "1.  The original dataset with images and their descriptions (e.g., \"Mars Surface Images with Captions\").\n",
    "2.  A new dataset with prompt items (e.g., \"Mars Surface Images with Captions prompt items\"), which will be used for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='finetune-deploy-clip'></a>5. Fine-Tune and Deploy CLIP Model\n",
    "\n",
    "With the prompt item dataset prepared, we can now proceed to fine-tune the CLIP model.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='install-clip-model-dpk'></a>5.1 Install CLIP Model Package (DPK)\n",
    "\n",
    "First, we install the CLIP model DPK from the Dataloop Marketplace. This package provides the necessary components for CLIP model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpk = dl.dpks.get(dpk_name='clip-model-pretrained')\n",
    "app = project.apps.install(dpk=dpk)\n",
    "print(f\"CLIP App installed: {app.name} (ID: {app.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='configure-clone-clip-model'></a>5.2 Configure and Clone Pretrained CLIP Model\n",
    "\n",
    "Next, we retrieve the base pre-trained CLIP model entity (\"openai-clip\") provided by the DPK. We'll then configure its metadata, specifying which subsets of our `prompt_dataset` to use for training and validation. \n",
    "\n",
    "You can also adjust model hyperparameters like learning rate, batch size, and number of epochs in the `base_model.configuration` dictionary. The example settings below are a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = project.models.get(model_name=\"openai-clip\")\n",
    "\n",
    "# Configure model metadata and subsets\n",
    "# This tells the model which items from the PROMPT_DATASET to use for training and validation\n",
    "base_model.metadata[\"system\"] = {}\n",
    "base_model.metadata[\"system\"][\"subsets\"] = {}\n",
    "\n",
    "train_filters = dl.Filters(field=\"metadata.system.subsets.train\", values=True) # Assuming subsets were marked during prompt_dataset creation\n",
    "val_filters = dl.Filters(field=\"metadata.system.subsets.validation\", values=True)\n",
    "\n",
    "base_model.metadata[\"system\"][\"subsets\"][\"train\"] = train_filters.prepare()\n",
    "base_model.metadata[\"system\"][\"subsets\"][\"validation\"] = val_filters.prepare()\n",
    "\n",
    "# Set model configuration (hyperparameters)\n",
    "base_model.configuration = {\n",
    "    \"model_name\": \"ViT-B/32\",      # CLIP model architecture (e.g., ViT-B/32, RN50)\n",
    "    \"embeddings_size\": 512,       # Output embedding dimension for ViT-B/32\n",
    "    \"num_epochs\": 10,             # Number of training epochs (adjust based on dataset size and convergence)\n",
    "    \"batch_size\": 64,             # Batch size for training (adjust based on GPU memory)\n",
    "    \"learning_rate\": 5e-6,        # Learning rate (often smaller for fine-tuning)\n",
    "    \"early_stopping\": True,       # Enable early stopping\n",
    "    \"early_stopping_epochs\": 3,   # Number of epochs with no improvement before stopping\n",
    "    \"weight_decay\": 0.01          # Weight decay for regularization (optional)\n",
    "}\n",
    "base_model.output_type = \"text\" # For CLIP, this usually indicates it's working with text-image pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='train-clip-model'></a>5.3 Train the Model\n",
    "\n",
    "Now we clone the configured base model. This creates a new model entity in your project that will be fine-tuned. We associate our `prompt_dataset` (created in [Section 4.3](#convert-to-prompt-items)) with this new model and then start the training process.\n",
    "\n",
    "> **NOTE**: The training process can take a significant amount of time, depending on your dataset size, model configuration, and the available compute resources (GPU type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'prompt_dataset' is defined from the conversion step\n",
    "if 'prompt_dataset' not in locals() or prompt_dataset is None:\n",
    "    raise ValueError(\"Error: 'prompt_dataset' is not defined. Please run section 4.3 to create it.\")\n",
    "\n",
    "finetuned_model_name = base_model.name + \"-finetuned-\" + ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "print(f\"Cloning model to create: '{finetuned_model_name}' using dataset '{prompt_dataset.name}'\")\n",
    "finetuned_model = base_model.clone(model_name=finetuned_model_name, dataset_id=prompt_dataset.id)\n",
    "\n",
    "print(f\"Starting training for model: '{finetuned_model.name}' (ID: {finetuned_model.id}). This may take a while...\")\n",
    "execution = finetuned_model.train()\n",
    "print(f\"Training initiated. Execution ID: {execution.id}. You can monitor progress in the Dataloop platform.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will periodically check the status of the training execution. You can also monitor the training progress, view logs, and see performance metrics directly in the Dataloop platform by navigating to your project, then Models, finding your `finetuned_model`, and checking its 'Executions' or 'Training' tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for training to complete\n",
    "print(f\"Waiting for training execution {execution.id} to complete...\")\n",
    "\n",
    "while execution.status not in [dl.ExecutionStatus.SUCCESS, dl.ExecutionStatus.FAILED, dl.ExecutionStatus.CANCELED]:\n",
    "    print(f\"Training in progress (Status: {execution.status})... checking again in 5 minutes\")\n",
    "    time.sleep(300)  # Sleep for 5 minutes\n",
    "    execution = dl.executions.get(execution_id=execution.id) # Refresh execution status\n",
    "\n",
    "if execution.status == dl.ExecutionStatus.SUCCESS:\n",
    "    print(f\"Training completed successfully! Model ID: {finetuned_model.id}\")\n",
    "    # Update the local model object with the latest status and artifacts from the platform\n",
    "    finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "elif execution.status == dl.ExecutionStatus.FAILED:\n",
    "    print(f\"Training failed. Execution ID: {execution.id}. Check logs in Dataloop platform for details.\")\n",
    "else:\n",
    "    print(f\"Training ended with status: {execution.status}. Execution ID: {execution.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='deploy-clip-model'></a>5.4 Deploy the Fine-Tuned Model\n",
    "\n",
    "Once the model has successfully trained, you can deploy it as a service. This makes the model available for inference tasks, such as generating embeddings for new images.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'finetuned_model' in locals() and finetuned_model is not None and execution.status == dl.ExecutionStatus.SUCCESS:\n",
    "    print(f\"Deploying fine-tuned model: '{finetuned_model.name}' (ID: {finetuned_model.id})\")\n",
    "    # The service will be created with default settings (e.g., 1 replica, default GPU if needed by model)\n",
    "    # You can customize deployment configuration if necessary via finetuned_model.deploy(service_config={...})\n",
    "    service = finetuned_model.deploy()\n",
    "    print(f\"Model deployment initiated. Service ID: {service.id}. Waiting for service to be ready...\")\n",
    "    service.wait_for_ready_state() # This will block until the service is ready or deployment fails\n",
    "    print(f\"Service '{service.name}' is now ready.\")\n",
    "else:\n",
    "    print(\"Skipping deployment: Model training was not successful or 'finetuned_model' is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='embed-datasets-clip'></a>5.5 Embed Datasets using the Fine-Tuned Model\n",
    "\n",
    "After the fine-tuned model is deployed and its service is ready, you can use it to generate embeddings for images in any dataset. These embeddings capture the semantic content of the images as understood by your fine-tuned model and can be used for tasks like semantic search or similarity comparison.\n",
    "\n",
    "We will typically want to embed the original image dataset (e.g., \"Mars Surface Images with Captions\" or your custom image dataset), not the `prompt_dataset`.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'finetuned_model' in locals() and finetuned_model is not None and 'service' in locals() and service.is_ready:\n",
    "    # Get the latest version of the model entity, which now includes deployment details\n",
    "    finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "    \n",
    "    # IMPORTANT: Select the dataset you want to embed. This should be your ORIGINAL image dataset,\n",
    "    # not the prompt_dataset used for training.\n",
    "    # For example, if you used the Mars dataset:\n",
    "    dataset_to_embed = project.datasets.get(dataset_name=\"Mars Surface Images with Captions\") \n",
    "    # Or, if you used a custom dataset named 'MyCustomImageData':\n",
    "    # dataset_to_embed = project.datasets.get(dataset_name='MyCustomImageData')\n",
    "\n",
    "    if dataset_to_embed:\n",
    "        print(f\"Starting embedding for dataset: '{dataset_to_embed.name}' (ID: {dataset_to_embed.id}) using model '{finetuned_model.name}'.\")\n",
    "        embedding_execution = finetuned_model.embed_datasets(dataset_ids=[dataset_to_embed.id])\n",
    "        print(f\"Embedding process initiated. Execution ID: {embedding_execution.id}. This may take some time.\")\n",
    "        # You can wait for this execution to complete similarly to the training execution if needed.\n",
    "        # embedding_execution.wait() or a loop with time.sleep()\n",
    "    else:\n",
    "        print(\"Error: 'dataset_to_embed' is not defined. Please specify the correct original image dataset.\")\n",
    "else:\n",
    "    print(\"Skipping embedding: Model is not successfully deployed or 'finetuned_model'/'service' are not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='conclusion'></a>6. Conclusion\n",
    "\n",
    "Congratulations! You have successfully walked through the process of fine-tuning a CLIP model using the Dataloop platform. This included:\n",
    "\n",
    "1.  **Setting up your Dataloop environment.**\n",
    "2.  **Preparing a dataset** of images and descriptions, either by using a public dataset or your own custom data, and converting it to the required prompt item format.\n",
    "3.  **Installing the CLIP model package.**\n",
    "4.  **Configuring and cloning** a pretrained CLIP model.\n",
    "5.  **Training (fine-tuning)** the model on your dataset.\n",
    "6.  **Deploying** the fine-tuned model as a service.\n",
    "7.  **Generating embeddings** for an image dataset using your fine-tuned model.\n",
    "\n",
    "### Next Steps\n",
    "*   **Experiment with Hyperparameters:** Adjust learning rate, batch size, number of epochs, and CLIP model architecture (e.g., `ViT-L/14` if available and supported) to potentially improve performance.\n",
    "*   **Evaluate Your Model:** While this tutorial focuses on the fine-tuning process, a crucial next step is to evaluate your fine-tuned model's performance on a held-out test set for tasks like image-text retrieval or zero-shot classification.\n",
    "*   **Use Embeddings:** Explore applications of the generated embeddings, such as building a semantic image search engine or performing image clustering.\n",
    "*   **Explore Advanced Features:** Dataloop offers many more features for MLOps, data management, and AI development. Check out the [Dataloop Developer Documentation](https://developers.dataloop.ai/) for more.\n",
    "\n",
    "[Back to Top](#clip-contrastive-language-image-pre-training-model-adapter-tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
