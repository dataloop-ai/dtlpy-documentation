{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# CLIP (Contrastive Language-Image Pre-Training) Model Adapter Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide on fine-tuning a CLIP model using the Dataloop platform and its Python SDK. CLIP models are powerful for tasks involving understanding the relationship between images and text, such as zero-shot image classification, image-text retrieval, and generating image embeddings for semantic search.\n",
    "\n",
    "Fine-tuning allows you to adapt a pre-trained CLIP model to your specific dataset and domain, potentially improving its performance on tasks relevant to your data. This tutorial demonstrates the complete workflow from data preparation through model deployment.\n",
    "\n",
    "### Prerequisites:\n",
    "* **Dataloop Account:** You should have access to a Dataloop platform account.\n",
    "* **Python Environment:** Ensure you have Python 3.7+ installed with pip.\n",
    "* **Project Access:** Ability to create projects and install apps from the Dataloop Marketplace.\n",
    "* **Basic ML Knowledge:** Understanding of machine learning concepts and multimodal models.\n",
    "\n",
    "### Navigate through the following sections:\n",
    "1. [Dependencies & Setup](#dependencies-setup)\n",
    "2. [Environment Setup](#environment-setup)\n",
    "3. [Prepare Dataset for Fine-Tuning](#prepare-dataset)\n",
    "4. [Fine-Tune and Deploy CLIP Model](#finetune-deploy-clip)\n",
    "5. [Conclusion and Next Steps](#conclusion)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependencies-setup",
   "metadata": {},
   "source": [
    "## <a id='dependencies-setup'></a>1. Dependencies & Setup\n",
    "\n",
    "First, ensure that the necessary Python libraries are installed. This notebook requires `dtlpy` for interacting with the Dataloop platform and `pandas` for data manipulation. The following cell will install or upgrade them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtlpy pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## <a id='environment-setup'></a>2. Environment Setup\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "Now, we import all the Python libraries that will be used throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import dtlpy as dl\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(f\"Dataloop SDK Version: {dl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-dataloop-environment",
   "metadata": {},
   "source": [
    "### Set Up Dataloop Environment\n",
    "\n",
    "We need to set up our Dataloop environment and get our project. You'll need to replace project and dataset names with your own values.\n",
    "\n",
    "> **_NOTE:_** This tutorial assumes you are working in a new project which does NOT have the CLIP model previously installed. If it's an existing project and you already have CLIP installed, you will need to get the appropriate app and base CLIP model entity for the rest of the code to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "\n",
    "# Define a unique name for your project or change it to your own project name\n",
    "user_email = dl.info()['user_email']\n",
    "user_prefix = user_email.split('@')[0].replace('.', '').replace('-', '') # Simple prefix from email\n",
    "project_name = f'{user_prefix}-clip-tutorial'\n",
    "\n",
    "# Check if the project exists, if not, create it\n",
    "try:\n",
    "    project = dl.projects.get(project_name=project_name)\n",
    "    print(f\"Successfully retrieved project: '{project.name}' (ID: {project.id})\")\n",
    "except dl.exceptions.NotFound:\n",
    "    project = dl.projects.create(project_name=project_name)\n",
    "    print(f\"Successfully created project: '{project.name}' (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "action-required-note",
   "metadata": {},
   "source": [
    "**Action Required:** In the cell above, you can modify `\"{user_prefix}-clip-tutorial\"` with the desired name for your Dataloop project. If a project with this name already exists, the SDK will retrieve it; otherwise, a new project will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-dataset",
   "metadata": {},
   "source": [
    "## <a id='prepare-dataset'></a>3. Prepare Dataset for Fine-Tuning\n",
    "\n",
    "Fine-tuning a CLIP model requires a dataset of images paired with relevant textual descriptions. This section covers two ways to prepare such a dataset:\n",
    "1. **Option A:** Use a publicly available dataset from the Dataloop Marketplace (Mars Surface Images with Captions).\n",
    "2. **Option B:** Upload your own custom dataset of images and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "use-public-dataset",
   "metadata": {},
   "source": [
    "### 3.1 Option A: Use Public Dataset (Mars Surface Images)\n",
    "\n",
    "For this tutorial we will install the Mars Surface Images Datasets from the Dataloop Marketplace. This dataset includes images with descriptions pre-loaded in the item metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-mars-dpk",
   "metadata": {},
   "source": [
    "#### 3.1.1 Install Mars Surface Images DPK\n",
    "\n",
    "This Dataloop Package (DPK) contains datasets related to Mars surface imagery, including one with captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-mars-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpk = dl.dpks.get(dpk_name=\"mars-surface-images\")\n",
    "try:\n",
    "    app = project.apps.install(\n",
    "        app_name=dpk.display_name, \n",
    "        dpk=dpk, \n",
    "        custom_installation=dpk.to_json()\n",
    "    )\n",
    "    print(f\"Installed {dpk.display_name} app: {app.name}\")\n",
    "except dl.exceptions.BadRequest as e:\n",
    "    print(f\"{dpk.display_name} app already installed, getting existing app\")\n",
    "    app = project.apps.get(app_name=dpk.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-for-dataset",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You may need to wait a few minutes after installing the app until the dataset has completed loading into your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-dataset-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the dataset to load\n",
    "print(\"Waiting for dataset to load...\")\n",
    "time.sleep(150)\n",
    "print(\"Dataset should now be available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "get-mars-dataset-split",
   "metadata": {},
   "source": [
    "#### 3.1.2 Get Captioned Dataset and Split for ML\n",
    "\n",
    "After installing the DPK, the \"Mars Surface Images with Captions\" dataset should be available in your project. We will retrieve this dataset and split its items into training, validation, and test subsets. The Dataloop SDK provides a convenient method to do this by automatically tagging items. This splitting is crucial for proper model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-and-split-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.datasets.get(dataset_name=\"Mars Surface Images with Captions\")\n",
    "print(f\"Retrieved dataset: {dataset.name} (ID: {dataset.id})\")\n",
    "print(f\"Dataset contains {dataset.items_count} items\")\n",
    "\n",
    "SUBSET_PERCENTAGES = {'train': 80, 'validation': 10, 'test': 10}\n",
    "dataset.split_ml_subsets(percentages=SUBSET_PERCENTAGES)\n",
    "print(f\"Dataset split completed with percentages: {SUBSET_PERCENTAGES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upload-custom-dataset",
   "metadata": {},
   "source": [
    "### 3.2 Option B: (Alternative) Upload and Prepare Your Custom Dataset\n",
    "\n",
    "If you have your own dataset of images and corresponding text descriptions, you can use the function below to create a new Dataloop dataset and upload your images with descriptions.\n",
    "\n",
    "The function expects a Pandas DataFrame (`pairs_df`) with two columns:\n",
    "- `'filepath'`: The local path to each image file.\n",
    "- `'description'`: The text description for that image.\n",
    "\n",
    "It also assumes that for each image file (e.g., `items/image1.jpg`), there is a corresponding JSON annotation file (e.g., `json/image1.json`) if you have existing annotations to upload. If not, the `local_annotations_path` part can be adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-dataset-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(dataset_name, pairs_df, subset_percentages={'train': 80, 'validation': 10, 'test': 10}):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from a DataFrame containing image paths and descriptions\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to create\n",
    "        pairs_df (pd.DataFrame): DataFrame containing 'filepath' and 'description' columns\n",
    "        subset_percentages (dict): Dictionary containing the percentages for each subset\n",
    "        default is 80% train, 10% validation, 10% test\n",
    "        can be changed to any other percentages as long as the sum is 100\n",
    "    \n",
    "    Returns:\n",
    "        dl.Dataset: The created dataset\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dataset = project.datasets.create(dataset_name=dataset_name)\n",
    "    except dl.exceptions.BadRequest:\n",
    "        # Generate 5 random alphanumeric characters\n",
    "        suffix = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "        dataset = project.datasets.create(dataset_name=f\"{dataset_name}_{suffix}\")\n",
    "\n",
    "    def upload_item(row):\n",
    "        file_path = row[\"filepath\"]\n",
    "        annots_path = file_path.replace(\"items\", \"json\") # This assumes a specific structure for annotation files\n",
    "        \n",
    "        # Upload item with annotations\n",
    "        item = dataset.items.upload(\n",
    "            local_path=file_path,\n",
    "            local_annotations_path=annots_path,\n",
    "            item_metadata=dl.ExportMetadata.FROM_JSON,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        # Set description and update\n",
    "        item.set_description(text=row[\"description\"])\n",
    "        item.update()\n",
    "\n",
    "    # Use ThreadPoolExecutor to upload items in parallel with progress bar\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        from tqdm import tqdm\n",
    "        list(tqdm(\n",
    "            executor.map(upload_item, [row for _, row in pairs_df.iterrows()]),\n",
    "            total=len(pairs_df),\n",
    "            desc=\"Uploading items\",\n",
    "            unit=\"item\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "        ))\n",
    "\n",
    "    # Since model training requires labels, we create a dummy label for the recipe\n",
    "    dataset.add_labels(label_list=['free-text'])\n",
    "    \n",
    "    # Split the dataset into ML subsets\n",
    "    dataset.split_ml_subsets(percentages=subset_percentages)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Example usage (commented out):\n",
    "# pairs_df = pd.DataFrame({\n",
    "#     'filepath': ['path/to/image1.jpg', 'path/to/image2.jpg'],\n",
    "#     'description': ['Description for image 1', 'Description for image 2']\n",
    "# })\n",
    "# custom_dataset = create_new_dataset('my-custom-clip-dataset', pairs_df)\n",
    "\n",
    "print(\"Custom dataset creation function defined. Use it if you have your own image-text pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finetune-deploy-clip",
   "metadata": {},
   "source": [
    "## <a id='finetune-deploy-clip'></a>4. Fine-Tune and Deploy CLIP Model\n",
    "\n",
    "With the dataset prepared, we can now proceed to fine-tune the CLIP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-clip-model-dpk",
   "metadata": {},
   "source": [
    "### 4.1 Install CLIP Model Package (DPK)\n",
    "\n",
    "First, we install the CLIP model DPK from the Dataloop Marketplace. This package provides the necessary components for CLIP model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-clip-dpk",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_dpk = dl.dpks.get(dpk_name=\"clip-model-pretrained\")\n",
    "try:\n",
    "    model_app = project.apps.install(\n",
    "        app_name=clip_model_dpk.display_name, \n",
    "        dpk=clip_model_dpk, \n",
    "        custom_installation=clip_model_dpk.to_json()\n",
    "    )\n",
    "    print(f\"Installed {clip_model_dpk.display_name} app: {model_app.name}, ID: {model_app.id}\")\n",
    "except dl.exceptions.BadRequest as e:\n",
    "    print(f\"{clip_model_dpk.display_name} app already installed, getting existing app\")\n",
    "    model_app = project.apps.get(app_name=clip_model_dpk.display_name)\n",
    "    print(f\"Retrieved existing app: {model_app.name}, ID: {model_app.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configure-clone-clip-model",
   "metadata": {},
   "source": [
    "### 4.2 Configure and Clone Pretrained CLIP Model\n",
    "\n",
    "Next, we retrieve the base pre-trained CLIP model entity (\"openai-clip\") provided by the DPK. We'll then configure its metadata, specifying which subsets of our `dataset` to use for training and validation.\n",
    "\n",
    "You can also adjust model hyperparameters like learning rate, batch size, and number of epochs in the `base_model.configuration` dictionary. The example settings below are a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-clip-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = project.models.get(model_name=\"openai-clip\")\n",
    "print(f\"Retrieved base CLIP model: {base_model.name} (ID: {base_model.id})\")\n",
    "\n",
    "# Configure model metadata and subsets\n",
    "# Use the correct field names for ML subset tags\n",
    "train_filters = dl.Filters(field=\"metadata.system.tags.train\", values=True)\n",
    "val_filters = dl.Filters(field=\"metadata.system.tags.validation\", values=True)\n",
    "\n",
    "# Add subsets to the model using the correct method\n",
    "base_model.add_subset(subset_name=\"train\", subset_filter=train_filters)\n",
    "base_model.add_subset(subset_name=\"validation\", subset_filter=val_filters)\n",
    "\n",
    "# Set model configuration (hyperparameters)\n",
    "base_model.configuration = {\n",
    "    \"model_name\": \"ViT-B/32\",      # CLIP model architecture (e.g., ViT-B/32, RN50)\n",
    "    \"embeddings_size\": 512,       # Output embedding dimension for ViT-B/32\n",
    "    \"num_epochs\": 10,             # Number of training epochs (adjust based on dataset size and convergence)\n",
    "    \"batch_size\": 64,             # Batch size for training (adjust based on GPU memory)\n",
    "    \"learning_rate\": 5e-6,        # Learning rate (often smaller for fine-tuning)\n",
    "    \"early_stopping\": True,       # Enable early stopping\n",
    "    \"early_stopping_epochs\": 3,   # Number of epochs with no improvement before stopping\n",
    "    \"weight_decay\": 0.01          # Weight decay for regularization (optional)\n",
    "}\n",
    "base_model.output_type = \"text\" # For CLIP, this usually indicates it's working with text-image pairs\n",
    "\n",
    "# Update the model to save the subset configurations\n",
    "base_model.update(system_metadata=True)\n",
    "\n",
    "print(f\"Base model configured with:\")\n",
    "print(f\"  Architecture: {base_model.configuration['model_name']}\")\n",
    "print(f\"  Epochs: {base_model.configuration['num_epochs']}\")\n",
    "print(f\"  Batch size: {base_model.configuration['batch_size']}\")\n",
    "print(f\"  Learning rate: {base_model.configuration['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-clip-model",
   "metadata": {},
   "source": [
    "### 4.3 Train the Model\n",
    "\n",
    "Now we clone the configured base model. This creates a new model entity in your project that will be fine-tuned. We associate our `dataset` with this new model and then start the training process.\n",
    "\n",
    "> **NOTE**: The training process can take a significant amount of time, depending on your dataset size, model configuration, and the available compute resources (GPU type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-and-train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'dataset' is defined from the dataset preparation step\n",
    "if 'dataset' not in locals() or dataset is None:\n",
    "    raise ValueError(\"Error: 'dataset' is not defined. Please run the dataset preparation section first.\")\n",
    "\n",
    "# Create a unique name for the fine-tuned model\n",
    "finetuned_model_name = base_model.name + \"-finetuned-\" + ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "print(f\"Cloning model to create: '{finetuned_model_name}' using dataset '{dataset.name}'\")\n",
    "\n",
    "# Clone the base model\n",
    "finetuned_model = base_model.clone(model_name=finetuned_model_name, dataset=dataset)\n",
    "print(f\"Model cloned successfully: '{finetuned_model.name}' (ID: {finetuned_model.id})\")\n",
    "\n",
    "# Start training\n",
    "print(f\"Starting training for model: '{finetuned_model.name}'. This may take a while...\")\n",
    "execution = finetuned_model.train()\n",
    "print(f\"Training initiated. Execution ID: {execution.id}\")\n",
    "print(f\"You can monitor progress in the Dataloop platform at: {finetuned_model.platform_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-training",
   "metadata": {},
   "source": [
    "### 4.4 Monitor Training Progress\n",
    "\n",
    "The cell below will periodically check the status of the training execution. You can also monitor the training progress, view logs, and see performance metrics directly in the Dataloop platform by navigating to your project, then Models, finding your `finetuned_model`, and checking its 'Executions' or 'Training' tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-for-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for training to complete\n",
    "print(f\"Waiting for training execution {execution.id} to complete...\")\n",
    "start_time = time.time()\n",
    "\n",
    "execution_status = dl.executions.get(execution_id=execution.id).get_latest_status().get('status')\n",
    "\n",
    "while execution_status not in [dl.ExecutionStatus.SUCCESS, dl.ExecutionStatus.FAILED, dl.ExecutionStatus.CANCELED]:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Training in progress (Status: {execution_status}, Elapsed: {elapsed_time/60:.1f} min)... checking again in 5 minutes\")\n",
    "    time.sleep(300)  # Sleep for 5 minutes\n",
    "    execution_status = dl.executions.get(execution_id=execution.id).get_latest_status().get('status') # Refresh execution status\n",
    "\n",
    "# Check final status\n",
    "total_time = time.time() - start_time\n",
    "if execution_status == dl.ExecutionStatus.SUCCESS:\n",
    "    print(f\"üéâ Training completed successfully! (Total time: {total_time/60:.1f} minutes)\")\n",
    "    print(f\"Model ID: {finetuned_model.id}\")\n",
    "    # Update the local model object with the latest status and artifacts from the platform\n",
    "    finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "elif execution_status == dl.ExecutionStatus.FAILED:\n",
    "    print(f\"‚ùå Training failed. Execution ID: {execution.id}. Check logs in Dataloop platform for details.\")\n",
    "else:\n",
    "    print(f\"Training ended with status: {execution_status}. Execution ID: {execution.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy-clip-model",
   "metadata": {},
   "source": [
    "### 4.5 Deploy the Fine-Tuned Model\n",
    "\n",
    "Once the model has successfully trained, you can deploy it as a service. This makes the model available for inference tasks, such as generating embeddings for new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'finetuned_model' in locals() and finetuned_model is not None and execution_status == dl.ExecutionStatus.SUCCESS:\n",
    "    print(f\"Deploying fine-tuned model: '{finetuned_model.name}' (ID: {finetuned_model.id})\")\n",
    "    \n",
    "    # The service will be created with default settings (e.g., 1 replica, default GPU if needed by model)\n",
    "    # You can customize deployment configuration if necessary via finetuned_model.deploy(service_config={...})\n",
    "    service = finetuned_model.deploy()\n",
    "    print(f\"Deployment initiated. Service ID: {service.id}\")\n",
    "    \n",
    "    # Get the latest version of the model entity, which now includes deployment details\n",
    "    finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "    \n",
    "    # Wait for deployment to complete\n",
    "    print(\"Waiting for deployment to complete...\")\n",
    "    while finetuned_model.status not in [dl.ModelStatus.DEPLOYED, dl.ModelStatus.FAILED]:\n",
    "        print(f\"Model '{finetuned_model.name}' is deploying (Status: {finetuned_model.status}). Waiting for service to be ready...\")\n",
    "        time.sleep(180)  # Wait 3 minutes between checks\n",
    "        finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "    \n",
    "    if finetuned_model.status == dl.ModelStatus.DEPLOYED:\n",
    "        print(f\"üéâ Model successfully deployed!\")\n",
    "        print(f\"Model URL: {finetuned_model.platform_url}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Deployment failed. Model status: {finetuned_model.status}\")\n",
    "else:\n",
    "    print(\"Skipping deployment: Model training was not successful or 'finetuned_model' is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embed-datasets-clip",
   "metadata": {},
   "source": [
    "### 4.6 Embed Datasets using the Fine-Tuned Model\n",
    "\n",
    "After the fine-tuned model is deployed and its service is ready, you can use it to generate embeddings for images in any dataset. These embeddings capture the semantic content of the images as understood by your fine-tuned model and can be used for tasks like semantic search or similarity comparison.\n",
    "\n",
    "We will typically want to embed the original image dataset (e.g., \"Mars Surface Images with Captions\" or your custom image dataset), though you can also embed other datasets with your finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embed-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'finetuned_model' in locals() and finetuned_model.status == dl.ModelStatus.DEPLOYED:\n",
    "    # IMPORTANT: Select the dataset you want to embed. This should be your ORIGINAL image dataset,\n",
    "    # not the prompt_dataset used for training.\n",
    "\n",
    "    # You can embed the original image dataset or another dataset with the fine-tuned model.\n",
    "    # For example, if you used the Mars dataset and want to embed the uncaptioned dataset (that the model was *not* trained on):\n",
    "    try:\n",
    "        dataset_to_embed = project.datasets.get(dataset_name=\"Mars Surface Images Unannotated\")\n",
    "        print(f\"Found unannotated dataset: {dataset_to_embed.name}\")\n",
    "    except dl.exceptions.NotFound:\n",
    "        # If the unannotated dataset doesn't exist, use the training dataset\n",
    "        dataset_to_embed = dataset\n",
    "        print(f\"Using training dataset for embedding: {dataset_to_embed.name}\")\n",
    "    \n",
    "    # Or, if you used a custom dataset named 'MyCustomImageData' and want to embed it:\n",
    "    # dataset_to_embed = project.datasets.get(dataset_name='MyCustomImageData')\n",
    "\n",
    "    if dataset_to_embed:\n",
    "        print(f\"Starting embedding for dataset: '{dataset_to_embed.name}' (ID: {dataset_to_embed.id}) using model '{finetuned_model.name}'.\")\n",
    "        embedding_execution = finetuned_model.embed_datasets(dataset_ids=[dataset_to_embed.id])\n",
    "        print(f\"Embedding process initiated. Execution ID: {embedding_execution.id}. This may take some time.\")\n",
    "        print(f\"You can monitor the embedding progress in the Dataloop platform.\")\n",
    "        \n",
    "        # You can wait for this execution to complete similarly to the training execution if needed.\n",
    "        # embedding_execution.wait() or a loop with time.sleep()\n",
    "    else:\n",
    "        print(\"Error: 'dataset_to_embed' is not defined. Please specify the correct original image dataset.\")\n",
    "else:\n",
    "    print(f\"Skipping embedding: Model is not successfully deployed. Current status: {finetuned_model.status if 'finetuned_model' in locals() else 'Model not defined'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## <a id='conclusion'></a>5. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully walked through the process of fine-tuning a CLIP model using the Dataloop platform.\n",
    "\n",
    "### Summary of What You've Accomplished:\n",
    "- **Environment Setup:** Connected to Dataloop and configured your project for CLIP model fine-tuning\n",
    "- **Dataset Preparation:** Prepared a dataset of images and descriptions, either by using a public dataset or learning how to upload custom data\n",
    "- **Model Installation:** Installed the CLIP model package from the Dataloop Marketplace\n",
    "- **Model Configuration:** Configured and cloned a pretrained CLIP model with appropriate hyperparameters\n",
    "- **Model Training:** Fine-tuned the model on your dataset with proper monitoring and error handling\n",
    "- **Model Deployment:** Deployed the fine-tuned model as a service for inference\n",
    "- **Embedding Generation:** Generated embeddings for an image dataset using your fine-tuned model\n",
    "\n",
    "### Next Steps:\n",
    "- **Experiment with Hyperparameters:** Adjust learning rate, batch size, number of epochs, and CLIP model architecture (e.g., `ViT-L/14` if available and supported) to potentially improve performance\n",
    "- **Evaluate Your Model:** While this tutorial focuses on the fine-tuning process, a crucial next step is to evaluate your fine-tuned model's performance on a held-out test set for tasks like image-text retrieval or zero-shot classification\n",
    "- **Use Embeddings:** Explore applications of the generated embeddings, such as building a semantic image search engine or performing image clustering\n",
    "- **Advanced Fine-tuning:** Experiment with different fine-tuning strategies, such as freezing certain layers or using different learning rates for different parts of the model\n",
    "- **Multi-modal Applications:** Build applications that leverage both image and text understanding capabilities of your fine-tuned CLIP model\n",
    "- **Integration with Pipelines:** Incorporate your CLIP model into automated workflows for content analysis, recommendation systems, or quality assessment\n",
    "\n",
    "### Additional Resources:\n",
    "- **[Dataloop Model Management](https://developers.dataloop.ai/tutorials/model_management):** Learn more about managing and versioning models\n",
    "- **[CLIP Research Paper](https://arxiv.org/abs/2103.00020):** Understand the theoretical foundations of CLIP\n",
    "- **[Dataloop Developer Documentation](https://developers.dataloop.ai/):** Explore many more features for MLOps, data management, and AI development\n",
    "\n",
    "This tutorial demonstrates the power of combining state-of-the-art multimodal models with a comprehensive MLOps platform. Your fine-tuned CLIP model is now ready to power intelligent image-text applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
