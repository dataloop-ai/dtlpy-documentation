{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP (Contrastive Language-Image Pre-Training) Model Adapter Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide on fine-tuning a CLIP model using the Dataloop platform and its Python SDK. CLIP models are powerful for tasks involving understanding the relationship between images and text, such as zero-shot image classification, image-text retrieval, and generating image embeddings for semantic search.\n",
    "\n",
    "Fine-tuning allows you to adapt a pre-trained CLIP model to your specific dataset and domain, potentially improving its performance on tasks relevant to your data. This tutorial will walk you through:\n",
    "\n",
    "1. Preparing a dataset with images and their corresponding textual descriptions.\n",
    "2. Using the Dataloop CLIP model adapter to fine-tune the model on your prepared dataset.\n",
    "3. Deploying the fine-tuned model and using it to generate embeddings for your images.\n",
    "\n",
    "### Navigate through the following sections:\n",
    "1. [Install Dependencies](#install-dependencies)\n",
    "2. [Import Required Libraries](#import-libraries)\n",
    "3. [Set Up Dataloop Environment](#setup-environment)\n",
    "4. [Prepare Dataset for Fine-Tuning](#prepare-dataset)\n",
    "    *   [4.1 Option A: Use Public Dataset (Mars Surface Images)](#use-public-dataset)\n",
    "        *   [4.1.1 Install Mars Surface Images DPK](#install-mars-dpk)\n",
    "        *   [4.1.2 Get Captioned Dataset and Split for ML](#get-mars-dataset-split)\n",
    "    *   [4.2 Option B: (Alternative) Upload and Prepare Your Custom Dataset](#upload-custom-dataset)\n",
    "5. [Fine-Tune and Deploy CLIP Model](#finetune-deploy-clip)\n",
    "    *   [5.1 Install CLIP Model Package (DPK)](#install-clip-model-dpk)\n",
    "    *   [5.2 Configure and Clone Pretrained CLIP Model](#configure-clone-clip-model)\n",
    "    *   [5.3 Train the Model](#train-clip-model)\n",
    "    *   [5.4 Deploy the Fine-Tuned Model](#deploy-clip-model)\n",
    "    *   [5.5 Embed Datasets using the Fine-Tuned Model](#embed-datasets-clip)\n",
    "6. [Conclusion](#conclusion)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"install-dependencies\"></a>1. Install Dependencies\n",
    "\n",
    "First, ensure that the necessary Python libraries are installed. This notebook requires `dtlpy` for interacting with the Dataloop platform and `pandas` for data manipulation. The following cell will install or upgrade them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtlpy pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"import-libraries\"></a>2. Import Required Libraries\n",
    "\n",
    "Now, we import all the Python libraries that will be used throughout this tutorial.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import string\n",
    "import dtlpy as dl\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"setup-environment\"></a>3. Set Up Dataloop Environment\n",
    "\n",
    "We need to set up our Dataloop environment and get our project. You'll need to replace project and dataset names with your own values.\n",
    "\n",
    "> **_NOTE:_**  This tutorial assumes you are working in a new project which does NOT have the CLIP model previously installed. If it's an existing project and you already have CLIP installed, you will need to get the appropriate app and base CLIP model entity for the rest of the code to work correctly.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "\n",
    "# Define a unique name for your project or change it to your own project name\n",
    "user_email = dl.info()['user_email']\n",
    "user_prefix = user_email.split('@')[0].replace('.', '').replace('-', '') # Simple prefix from email\n",
    "project_name = f'{user_prefix}-clip-tutorial'\n",
    "\n",
    "# Check if the project exists, if not, create it\n",
    "try:\n",
    "    project = dl.projects.get(project_name=project_name)\n",
    "    print(f\"Successfully retrieved project: '{project.name}' (ID: {project.id})\")\n",
    "except dl.exceptions.NotFound:\n",
    "    project = dl.projects.create(project_name=project_name)\n",
    "    print(f\"Successfully created project: '{project.name}' (ID: {project.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Required:** In the cell above, replace if you want `\"{user_prefix}-clip-tutorial\"` with the desired name for your Dataloop project. If a project with this name already exists, the SDK will retrieve it; otherwise, a new project will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"prepare-dataset\"></a>4. Prepare Dataset for Fine-Tuning\n",
    "\n",
    "Fine-tuning a CLIP model requires a dataset of images paired with relevant textual descriptions. This section covers two ways to prepare such a dataset:\n",
    "1.  **Option A:** Use a publicly available dataset from the Dataloop Marketplace (Mars Surface Images with Captions).\n",
    "2.  **Option B:** Upload your own custom dataset of images and descriptions.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"use-public-dataset\"></a>4.1 Option A: Use Public Dataset (Mars Surface Images)\n",
    "\n",
    "For this tutorial we will install the Mars Surface Images Datasets from the Dataloop Marketplace. This dataset includes images with descriptions pre-loaded in the item metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"install-mars-dpk\"></a>4.1.1 Install Mars Surface Images DPK\n",
    "\n",
    "This Dataloop Package (DPK) contains datasets related to Mars surface imagery, including one with captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpk = dl.dpks.get(dpk_name=\"mars-surface-images\")\n",
    "try:\n",
    "    app = project.apps.install(\n",
    "        app_name=dpk.display_name, \n",
    "        dpk=dpk, \n",
    "        custom_installation=dpk.to_json()\n",
    "    )\n",
    "    print(f\"Installed {dpk.display_name} app: {app.name}\")\n",
    "except dl.exceptions.BadRequest as e:\n",
    "    print(f\"{dpk.display_name} app already installed, getting existing app\")\n",
    "    app = project.apps.get(app_name=dpk.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You may need to wait a few minutes after installing the app until the dataset has completed loading into your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the dataset to load\n",
    "time.sleep(150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"get-mars-dataset-split\"></a>4.1.2 Get Captioned Dataset and Split for ML\n",
    "\n",
    "After installing the DPK, the \"Mars Surface Images with Captions\" dataset should be available in your project. We will retrieve this dataset and split its items into training, validation, and test subsets. The Dataloop SDK provides a convenient method to do this by automatically tagging items. This splitting is crucial for proper model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.datasets.get(dataset_name=\"Mars Surface Images with Captions\")\n",
    "\n",
    "SUBSET_PERCENTAGES = {'train': 80, 'validation': 10, 'test': 10}\n",
    "dataset.split_ml_subsets(\n",
    "        percentages=SUBSET_PERCENTAGES\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"upload-custom-dataset\"></a>4.2 Option B: (Alternative) Upload and Prepare Your Custom Dataset\n",
    "\n",
    "If you have your own dataset of images and corresponding text descriptions, you can use the function below to create a new Dataloop dataset and upload your images with descriptions. \n",
    "\n",
    "The function expects a Pandas DataFrame (`pairs_df`) with two columns:\n",
    "-   `'filepath'`: The local path to each image file.\n",
    "-   `'description'`: The text description for that image.\n",
    "\n",
    "It also assumes that for each image file (e.g., `items/image1.jpg`), there is a corresponding JSON annotation file (e.g., `json/image1.json`) if you have existing annotations to upload. If not, the `local_annotations_path` part can be adapted.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(dataset_name, pairs_df, subset_percentages={'train': 80, 'validation': 10, 'test': 10}):\n",
    "    \"\"\"\n",
    "    Creates a new dataset from a CSV file containing image paths and descriptions\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to create\n",
    "        pairs_df (pd.DataFrame): DataFrame containing 'filepath' and 'img_description' columns\n",
    "        subset_percentages (dict): Dictionary containing the percentages for each subset\n",
    "        default is 80% train, 10% validation, 10% test\n",
    "        can be changed to any other percentages as long as the sum is 100\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dataset = project.datasets.create(dataset_name=dataset_name)\n",
    "    except dl.exceptions.BadRequest:\n",
    "        # Generate 5 random alphanumeric characters\n",
    "        suffix = ''.join(random.choices(string.ascii_letters + string.digits, k=5))\n",
    "        dataset = project.datasets.create(dataset_name=f\"{dataset_name}_{suffix}\")\n",
    "\n",
    "    def upload_item(row):\n",
    "        file_path = row[\"filepath\"]\n",
    "        annots_path = file_path.replace(\"items\", \"json\") # This assumes a specific structure for annotation files\n",
    "        \n",
    "        # Upload item with annotations\n",
    "        item = dataset.items.upload(\n",
    "            local_path=file_path,\n",
    "            local_annotations_path=annots_path,\n",
    "            item_metadata=dl.ExportMetadata.FROM_JSON,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "        # Set description and update\n",
    "        item.set_description(text=row[\"description\"])\n",
    "        item.update()\n",
    "\n",
    "    # Use ThreadPoolExecutor to upload items in parallel with progress bar\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        from tqdm import tqdm\n",
    "        list(tqdm(\n",
    "            executor.map(upload_item, [row for _, row in pairs_df.iterrows()]),\n",
    "            total=len(pairs_df),\n",
    "            desc=\"Uploading items\",\n",
    "            unit=\"item\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "        ))\n",
    "\n",
    "    # Since model training requires labels, we create a dummy label for the recipe\n",
    "    dataset.add_labels(label_list=['free-text'])\n",
    "    # After uploading, you would also split this custom dataset similarly to Option A:\n",
    "    # dataset.split_ml_subsets(percentages=subset_percentages)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"finetune-deploy-clip\"></a>5. Fine-Tune and Deploy CLIP Model\n",
    "\n",
    "With the dataset prepared, we can now proceed to fine-tune the CLIP model.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"install-clip-model-dpk\"></a>5.1 Install CLIP Model Package (DPK)\n",
    "\n",
    "First, we install the CLIP model DPK from the Dataloop Marketplace. This package provides the necessary components for CLIP model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_dpk = dl.dpks.get(dpk_name=\"clip-model-pretrained\")\n",
    "try:\n",
    "    model_app = project.apps.install(\n",
    "        app_name=clip_model_dpk.display_name, \n",
    "        dpk=clip_model_dpk, \n",
    "        custom_installation=clip_model_dpk.to_json()\n",
    "    )\n",
    "    print(f\"Installed {clip_model_dpk.display_name} app: {model_app.name}, ID: {model_app.id}\")\n",
    "except dl.exceptions.BadRequest as e:\n",
    "    print(f\"{clip_model_dpk.display_name} app already installed, getting existing app\")\n",
    "    model_app = project.apps.get(app_name=clip_model_dpk.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"configure-clone-clip-model\"></a>5.2 Configure and Clone Pretrained CLIP Model\n",
    "\n",
    "Next, we retrieve the base pre-trained CLIP model entity (\"openai-clip\") provided by the DPK. We'll then configure its metadata, specifying which subsets of our `dataset` to use for training and validation. \n",
    "\n",
    "You can also adjust model hyperparameters like learning rate, batch size, and number of epochs in the `base_model.configuration` dictionary. The example settings below are a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = project.models.get(model_name=\"openai-clip\")\n",
    "\n",
    "# Configure model metadata and subsets\n",
    "# Use the correct field names for ML subset tags\n",
    "train_filters = dl.Filters(field=\"metadata.system.tags.train\", values=True)\n",
    "val_filters = dl.Filters(field=\"metadata.system.tags.validation\", values=True)\n",
    "\n",
    "# Add subsets to the model using the correct method\n",
    "base_model.add_subset(subset_name=\"train\", subset_filter=train_filters)\n",
    "base_model.add_subset(subset_name=\"validation\", subset_filter=val_filters)\n",
    "\n",
    "# Set model configuration (hyperparameters)\n",
    "base_model.configuration = {\n",
    "    \"model_name\": \"ViT-B/32\",      # CLIP model architecture (e.g., ViT-B/32, RN50)\n",
    "    \"embeddings_size\": 512,       # Output embedding dimension for ViT-B/32\n",
    "    \"num_epochs\": 10,             # Number of training epochs (adjust based on dataset size and convergence)\n",
    "    \"batch_size\": 64,             # Batch size for training (adjust based on GPU memory)\n",
    "    \"learning_rate\": 5e-6,        # Learning rate (often smaller for fine-tuning)\n",
    "    \"early_stopping\": True,       # Enable early stopping\n",
    "    \"early_stopping_epochs\": 3,   # Number of epochs with no improvement before stopping\n",
    "    \"weight_decay\": 0.01          # Weight decay for regularization (optional)\n",
    "}\n",
    "base_model.output_type = \"text\" # For CLIP, this usually indicates it's working with text-image pairs\n",
    "\n",
    "# Update the model to save the subset configurations\n",
    "base_model.update(system_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"train-clip-model\"></a>5.3 Train the Model\n",
    "\n",
    "Now we clone the configured base model. This creates a new model entity in your project that will be fine-tuned. We associate our `dataset` (created in [Section 4.3](#convert-to-prompt-items)) with this new model and then start the training process.\n",
    "\n",
    "> **NOTE**: The training process can take a significant amount of time, depending on your dataset size, model configuration, and the available compute resources (GPU type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'dataset' is defined from the conversion step\n",
    "if 'dataset' not in locals() or dataset is None:\n",
    "    raise ValueError(\"Error: 'dataset' is not defined. Please run section 4.3 to create it.\")\n",
    "\n",
    "finetuned_model_name = base_model.name + \"-finetuned-\" + ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n",
    "print(f\"Cloning model to create: '{finetuned_model_name}' using dataset '{dataset.name}'\")\n",
    "finetuned_model = base_model.clone(model_name=finetuned_model_name, dataset=dataset)\n",
    "\n",
    "print(f\"Starting training for model: '{finetuned_model.name}' (ID: {finetuned_model.id}). This may take a while...\")\n",
    "execution = finetuned_model.train()\n",
    "print(f\"Training initiated. Execution ID: {execution.id}. You can monitor progress in the Dataloop platform.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will periodically check the status of the training execution. You can also monitor the training progress, view logs, and see performance metrics directly in the Dataloop platform by navigating to your project, then Models, finding your `finetuned_model`, and checking its 'Executions' or 'Training' tabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for training to complete\n",
    "print(f\"Waiting for training execution {execution.id} to complete...\")\n",
    "execution_status = dl.executions.get(execution_id=execution.id).get_latest_status().get('status')\n",
    "\n",
    "while execution_status not in [dl.ExecutionStatus.SUCCESS, dl.ExecutionStatus.FAILED, dl.ExecutionStatus.CANCELED]:\n",
    "    print(f\"Training in progress (Status: {execution_status})... checking again in 5 minutes\")\n",
    "    time.sleep(300)  # Sleep for 5 minutes\n",
    "    execution_status = dl.executions.get(execution_id=execution.id).get_latest_status().get('status') # Refresh execution status\n",
    "\n",
    "    if execution_status == dl.ExecutionStatus.SUCCESS:\n",
    "        print(f\"Training completed successfully! Model ID: {finetuned_model.id}\")\n",
    "        # Update the local model object with the latest status and artifacts from the platform\n",
    "        finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "    elif execution_status == dl.ExecutionStatus.FAILED:\n",
    "        print(f\"Training failed. Execution ID: {execution.id}. Check logs in Dataloop platform for details.\")\n",
    "    else:\n",
    "        print(f\"Training ended with status: {execution_status}. Execution ID: {execution.id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"deploy-clip-model\"></a>5.4 Deploy the Fine-Tuned Model\n",
    "\n",
    "Once the model has successfully trained, you can deploy it as a service. This makes the model available for inference tasks, such as generating embeddings for new images.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'finetuned_model' in locals() and finetuned_model is not None and execution_status == dl.ExecutionStatus.SUCCESS:\n",
    "    print(f\"Deploying fine-tuned model: '{finetuned_model.name}' (ID: {finetuned_model.id})\")\n",
    "    # The service will be created with default settings (e.g., 1 replica, default GPU if needed by model)\n",
    "    # You can customize deployment configuration if necessary via finetuned_model.deploy(service_config={...})\n",
    "    service = finetuned_model.deploy()\n",
    "    # Get the latest version of the model entity, which now includes deployment details\n",
    "    finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "    while finetuned_model.status not in [dl.ModelStatus.DEPLOYED, dl.ModelStatus.FAILED]:\n",
    "        print(f\"Model '{finetuned_model.name}' is deploying in service {service.id}. Waiting for service to be ready...\")\n",
    "        time.sleep(180)\n",
    "        finetuned_model = project.models.get(model_id=finetuned_model.id)\n",
    "else:\n",
    "    print(\"Skipping deployment: Model training was not successful or 'finetuned_model' is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"embed-datasets-clip\"></a>5.5 Embed Datasets using the Fine-Tuned Model\n",
    "\n",
    "After the fine-tuned model is deployed and its service is ready, you can use it to generate embeddings for images in any dataset. These embeddings capture the semantic content of the images as understood by your fine-tuned model and can be used for tasks like semantic search or similarity comparison.\n",
    "\n",
    "We will typically want to embed the original image dataset (e.g., \"Mars Surface Images with Captions\" or your custom image dataset), though you can also embed other datasets with your finetuned model.\n",
    "\n",
    "[Back to Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'finetuned_model' in locals() and finetuned_model.status == dl.ModelStatus.DEPLOYED:\n",
    "    # IMPORTANT: Select the dataset you want to embed. This should be your ORIGINAL image dataset,\n",
    "    # not the prompt_dataset used for training.\n",
    "\n",
    "    # You can embed the original image dataset or another dataset with the fine-tuned model.\n",
    "    # For example, if you used the Mars dataset and want to embed the uncaptioned dataset (that the model was *not* trained on):\n",
    "    dataset_to_embed = project.datasets.get(dataset_name=\"Mars Surface Images Unannotated\") \n",
    "    # Or, if you used a custom dataset named 'MyCustomImageData' and want to embed it:\n",
    "    # dataset_to_embed = project.datasets.get(dataset_name='MyCustomImageData')\n",
    "\n",
    "    if dataset_to_embed:\n",
    "        print(f\"Starting embedding for dataset: '{dataset_to_embed.name}' (ID: {dataset_to_embed.id}) using model '{finetuned_model.name}'.\")\n",
    "        embedding_execution = finetuned_model.embed_datasets(dataset_ids=[dataset_to_embed.id])\n",
    "        print(f\"Embedding process initiated. Execution ID: {embedding_execution.id}. This may take some time.\")\n",
    "        # You can wait for this execution to complete similarly to the training execution if needed.\n",
    "        # embedding_execution.wait() or a loop with time.sleep()\n",
    "    else:\n",
    "        print(\"Error: 'dataset_to_embed' is not defined. Please specify the correct original image dataset.\")\n",
    "else:\n",
    "    print(f\"Skipping embedding: Model {finetuned_model} is not successfully deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"conclusion\"></a>6. Conclusion\n",
    "\n",
    "Congratulations! You have successfully walked through the process of fine-tuning a CLIP model using the Dataloop platform. This included:\n",
    "\n",
    "1.  **Setting up your Dataloop environment.**\n",
    "2.  **Preparing a dataset** of images and descriptions, either by using a public dataset or your own custom data.\n",
    "3.  **Installing the CLIP model package.**\n",
    "4.  **Configuring and cloning** a pretrained CLIP model.\n",
    "5.  **Training (fine-tuning)** the model on your dataset.\n",
    "6.  **Deploying** the fine-tuned model as a service.\n",
    "7.  **Generating embeddings** for an image dataset using your fine-tuned model.\n",
    "\n",
    "### Next Steps\n",
    "*   **Experiment with Hyperparameters:** Adjust learning rate, batch size, number of epochs, and CLIP model architecture (e.g., `ViT-L/14` if available and supported) to potentially improve performance.\n",
    "*   **Evaluate Your Model:** While this tutorial focuses on the fine-tuning process, a crucial next step is to evaluate your fine-tuned model's performance on a held-out test set for tasks like image-text retrieval or zero-shot classification.\n",
    "*   **Use Embeddings:** Explore applications of the generated embeddings, such as building a semantic image search engine or performing image clustering.\n",
    "*   **Explore Advanced Features:** Dataloop offers many more features for MLOps, data management, and AI development. Check out the [Dataloop Developer Documentation](https://developers.dataloop.ai/) for more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
