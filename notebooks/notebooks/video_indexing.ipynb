{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Comprehensive Video Annotation with Generative AI\n",
       "\n",
       "This notebook provides a detailed walkthrough of an advanced, automated video annotation system. It leverages powerful Generative AI models (like Gemini) through the Dataloop platform to process and analyze videos of home activities, generating high-quality, structured annotations.\n",
       "\n",
       "### Overview\n",
       "\n",
       "This guide will walk you through the following key stages:\n",
       "\n",
       "1. **[Environment Setup](#environment-setup):** Installing dependencies and connecting to your Dataloop environment.\n",
       "2. **[Project, Model, and Dataset Setup](#project-setup):** Configuring your Dataloop project, dataset, and the GenAI model for the annotation task.\n",
       "3. **[VideoAnnotations Class Definition](#class-definition):** Exploring the comprehensive Python class that encapsulates the entire video processing logic.\n",
       "4. **[Initialize Video Processor](#initialize-processor):** Creating an instance of the processor to run on our data.\n",
       "5. **[Test Single Video Processing](#test-single-video):** Performing a dry run on a single video to verify the setup.\n",
       "6. **[Process Test Video](#process-single-video):** Executing the full, comprehensive analysis on the test video and reviewing the results.\n",
       "7. **[Batch Processing Function](#batch-function):** Defining a function to apply the annotation process to multiple videos efficiently.\n",
       "8. **[Run Full Batch Processing (Optional)](#run-batch):** Executing the pipeline on a larger set of videos from your dataset.\n",
       "9. **[Conclusion](#conclusion):** Summarizing the process and suggesting next steps."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='environment-setup'></a>1. Environment Setup\n",
       "\n",
       "We begin by importing the necessary Python libraries. These packages provide the tools for interacting with the Dataloop platform (`dtlpy`), handling video and image data (`cv2`, `Pillow`), performing numerical operations (`numpy`), and managing files and data structures (`os`, `json`, etc.).\n",
       "\n",
       "The script will then authenticate with the Dataloop platform. If your security token is expired, `dl.login()` will open a browser window for you to log in."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import time\n",
       "import dtlpy as dl\n",
       "import cv2\n",
       "import numpy as np\n",
       "import json\n",
       "import os\n",
       "import tempfile\n",
       "from PIL import Image\n",
       "from typing import List, Dict, Any\n",
       "\n",
       "dl.setenv('prod')\n",
       "\n",
       "# Login to Dataloop\n",
       "if dl.token_expired():\n",
       "    dl.login()\n",
       "print('Successfully logged into Dataloop')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='project-setup'></a>2. Project, Model, and Dataset Setup\n",
       "\n",
       "In this section, we'll connect to the required Dataloop entities. We will get the project and dataset, and then install and configure the Gemini model that will perform the analysis.\n",
       "\n",
       "### 2.1. Connect to Project and Dataset\n",
       "\n",
       "**Action Required:** You must replace the placeholder values in the cell below with your specific Dataloop project and dataset names."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "PROJECT_NAME = '<your-project-name>'\n",
       "DATASET_NAME = '<your-dataset-name>'\n",
       "MODEL_NAME = 'gemini-2.5-flash'\n",
       "\n",
       "\n",
       "# Get project, dataset, and model\n",
       "project = dl.projects.get(project_name=PROJECT_NAME)\n",
       "dataset = project.datasets.get(dataset_name=DATASET_NAME)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.2. Install and Retrieve the Model\n",
       "\n",
       "Now, we will install the generative model into our project. We fetch the `gemini-2.5` DPK (Dataloop Package), which contains the model definition, and install it as an App in the project. The `try/except` block gracefully handles cases where the app is already installed. Finally, we retrieve the model entity associated with this app."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Install model for video frame analysis\n",
       "model_dpk = dl.dpks.get(dpk_name='gemini-2.5')\n",
       "\n",
       "try:\n",
       "    model_app = project.apps.install(\n",
       "        app_name=model_dpk.display_name, \n",
       "        dpk=model_dpk, \n",
       "        custom_installation=model_dpk.to_json()\n",
       "    )\n",
       "    print(f\"Installed {model_dpk.display_name} app: {model_app.name}\")\n",
       "except dl.exceptions.BadRequest as e:\n",
       "    print(f\"{model_dpk.display_name} app already installed, getting existing app\")\n",
       "    model_app = project.apps.get(app_name=model_dpk.display_name)\n",
       "\n",
       "# Get VLM model\n",
       "filters = dl.Filters(resource=dl.FiltersResource.MODEL)\n",
       "filters.add(field='app.id', values=model_app.id)\n",
       "model = project.models.list(filters=filters)[0][0]\n",
       "\n",
       "print(f\"{model_dpk.display_name} Model: {model.name}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.3. Verify Model Deployment\n",
       "\n",
       "Models in Dataloop must be deployed as a live service to be used for inference. The following cell checks the model's status and provides a warning and a direct link to the model's page in the web UI if it's not yet deployed."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Check if model is deployed\n",
       "if model.status != \"deployed\":\n",
       "    print(\"\\n WARNING: Model is not deployed!\")\n",
       "    print(\"Please deploy the model through the platform interface before proceeding.\")\n",
       "    if model.status != \"deployed\":\n",
       "        print(\"Deploy model:\")\n",
       "        model.open_in_web()\n",
       "else:\n",
       "    print(\"âœ“ Model is deployed and ready for processing\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.4. Configure the Model\n",
       "\n",
       "To tailor the model's responses to our specific needs, we will update its configuration. We'll set a `system_prompt` to instruct the model to act as an expert video analyst specializing in home activities. We also adjust the `max_tokens` to allow for detailed and comprehensive responses."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configure model for video frame analysis\n",
       "model.configuration['system_prompt'] = \"\"\"You are an expert video analyst specializing in home activity recognition. \n",
       "Your task is to analyze video frames showing home environments and activities. Focus on:\n",
       "1. Room identification and layout\n",
       "2. People and their activities\n",
       "3. Objects and furniture\n",
       "4. Spatial relationships and context\n",
       "\n",
       "Provide detailed, accurate descriptions that capture the key elements visible in the frames. \n",
       "Be specific about room types, activities, and objects. If you're uncertain about any aspect, \n",
       "acknowledge your uncertainty rather than making assumptions.\"\"\"\n",
       "\n",
       "model.configuration['max_tokens'] = 4096\n",
       "model.configuration['max_output_tokens'] = 4096\n",
       "model.update(True)\n",
       "\n",
       "print(f\"{model.name} model configured for video analysis\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2.5. Verify Setup\n",
       "\n",
       "Let's print the names of our configured project, dataset, and model to confirm everything is connected correctly before proceeding."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "print(f'Connected to project: {project.name}')\n",
       "print(f'Connected to dataset: {dataset.name}')\n",
       "print(f'Using model: {model.name}')\n",
       "print(f'Dataset contains {dataset.items_count} items')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='class-definition'></a>3. VideoAnnotations Class Definition\n",
       "\n",
       "The core logic of our video processing pipeline is encapsulated in the `VideoAnnotations` class defined below. This comprehensive class handles every step of the process, from downloading the video to generating and uploading structured annotations.\n",
       "\n",
       "Here is a brief overview of its key methods:\n",
       "- **`process_video`**: The main orchestrator that calls all other methods in sequence for a single video.\n",
       "- **`analyze_video_characteristics`**: Gathers basic video metadata like duration, FPS, and aspect ratio.\n",
       "- **`extract_keyframes`**: Implements an advanced strategy to select diverse and high-quality frames for analysis, avoiding blurry or uninformative ones.\n",
       "- **`create_enhanced_montage`**: Stitches the selected keyframes into a single montage image, applying enhancements to improve clarity for the model.\n",
       "- **`create_adaptive_summary`**: Creates different types of montages based on video length and orientation.\n",
       "- **`analyze_with_model`**: Sends the final montage(s) to the configured Gemini model with a detailed prompt, requesting a comprehensive analysis.\n",
       "- **`generate_structured_annotation`**: Takes the raw text analysis from the model and uses a series of targeted prompts to convert it into structured JSON for different annotation types (room, participant, actions, objects).\n",
       "- **`create_dataloop_annotations`**: Converts the final JSON data into Dataloop Annotation objects, ready for upload.\n",
       "- **`clean_malformed_json`**: A robust helper function to clean and fix common JSON formatting issues that can occur in model responses."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class VideoAnnotations:\n",
       "    \"\"\"\n",
       "    Video annotation processor for home activity videos using comprehensive analysis.\n",
       "    \"\"\"\n",
       "    \n",
       "    def __init__(self, dataset, model):\n",
       "        self.dataset = dataset\n",
       "        self.model = model\n",
       "        \n",
       "    def process_video(self, video_item: dl.Item, progress_callback=None) -> Dict[str, Any]:\n",
       "        \"\"\"\n",
       "        Process video with comprehensive analysis.\n",
       "        \n",
       "        Args:\n",
       "            video_item: Video item to process\n",
       "            progress_callback: Optional callback for progress updates\n",
       "        \"\"\"\n",
       "        start_time = time.time()\n",
       "        timing_data = {}\n",
       "        \n",
       "        results = {\n",
       "            \"video_id\": video_item.id,\n",
       "            \"video_name\": video_item.name,\n",
       "            \"status\": \"processing\",\n",
       "            \"annotations_created\": 0,\n",
       "            \"room_annotation\": None,\n",
       "            \"participant_annotation\": None,\n",
       "            \"action_annotation\": None,\n",
       "            \"object_annotations\": [],\n",
       "            \"errors\": [],\n",
       "            \"timing\": timing_data,\n",
       "            \"processing_mode\": \"comprehensive\"\n",
       "        }\n",
       "        \n",
       "        try:\n",
       "            # 1. Download video\n",
       "            download_start = time.time()\n",
       "            if progress_callback:\n",
       "                progress_callback(f\"Downloading {video_item.name}...\")\n",
       "            \n",
       "            video_path = video_item.download()\n",
       "            timing_data[\"download_time\"] = time.time() - download_start\n",
       "            \n",
       "            # 2. Advanced frame extraction and analysis\n",
       "            extract_start = time.time()\n",
       "            if progress_callback:\n",
       "                progress_callback(f\"Analyzing video characteristics...\")\n",
       "            \n",
       "            # Analyze video characteristics\n",
       "            characteristics = self.analyze_video_characteristics(video_path)\n",
       "            timing_data[\"characteristics_analysis\"] = time.time() - extract_start\n",
       "            \n",
       "            # 3. Create adaptive summaries\n",
       "            summary_start = time.time()\n",
       "            if progress_callback:\n",
       "                progress_callback(f\"Creating adaptive summaries...\")\n",
       "            \n",
       "            summaries = self.create_adaptive_summary(video_path, characteristics)\n",
       "            timing_data[\"summary_creation\"] = time.time() - summary_start\n",
       "            \n",
       "            # 4. Multi-strategy model analysis\n",
       "            model_start = time.time()\n",
       "            if progress_callback:\n",
       "                progress_callback(f\"Comprehensive model analysis...\")\n",
       "            \n",
       "            analysis = self.analyze_with_model(video_item, summaries)\n",
       "            timing_data[\"model_analysis\"] = time.time() - model_start\n",
       "            \n",
       "            # 5. Enhanced annotation generation\n",
       "            annotation_start = time.time()\n",
       "            if progress_callback:\n",
       "                progress_callback(f\"Generating enhanced annotations...\")\n",
       "            \n",
       "            annotations_data = self.generate_annotations(analysis)\n",
       "            timing_data[\"annotation_generation\"] = time.time() - annotation_start\n",
       "            \n",
       "            # 6. Upload annotations\n",
       "            upload_start = time.time()\n",
       "            annotations_to_create = self.create_dataloop_annotations(annotations_data)\n",
       "            \n",
       "            if annotations_to_create:\n",
       "                video_item.annotations.upload(annotations_to_create)\n",
       "                results[\"annotations_created\"] = len(annotations_to_create)\n",
       "            \n",
       "            timing_data[\"annotation_upload\"] = time.time() - upload_start\n",
       "            \n",
       "            # 7. Update results\n",
       "            results.update({\n",
       "                \"room_annotation\": annotations_data.get(\"room\"),\n",
       "                \"participant_annotation\": annotations_data.get(\"participant\"),\n",
       "                \"action_annotation\": annotations_data.get(\"action\"),\n",
       "                \"object_annotations\": annotations_data.get(\"objects\", []),\n",
       "                \"status\": \"completed\",\n",
       "                \"video_characteristics\": characteristics\n",
       "            })\n",
       "            \n",
       "            # 8. Cleanup\n",
       "            cleanup_start = time.time()\n",
       "            if os.path.exists(video_path):\n",
       "                os.remove(video_path)\n",
       "            timing_data[\"cleanup\"] = time.time() - cleanup_start\n",
       "            \n",
       "        except Exception as e:\n",
       "            results[\"status\"] = \"error\"\n",
       "            results[\"errors\"].append(f\"Processing exception: {str(e)}\")\n",
       "        \n",
       "        timing_data[\"total_time\"] = time.time() - start_time\n",
       "        return results\n",
       "    \n",
       "    def extract_keyframes(self, video_path: str, num_frames: int = 8) -> List[np.ndarray]:\n",
       "        \"\"\"\n",
       "        Advanced keyframe extraction with motion analysis and scene detection.\n",
       "        \"\"\"\n",
       "        cap = cv2.VideoCapture(video_path)\n",
       "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
       "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
       "        \n",
       "        if total_frames < num_frames:\n",
       "            num_frames = total_frames\n",
       "        \n",
       "        # Strategy 1: Extract frames from different segments of the video\n",
       "        segment_size = total_frames // num_frames\n",
       "        frame_indices = []\n",
       "        \n",
       "        for i in range(num_frames):\n",
       "            # Take frame from middle of each segment\n",
       "            segment_start = i * segment_size\n",
       "            segment_middle = segment_start + segment_size // 2\n",
       "            frame_indices.append(min(segment_middle, total_frames - 1))\n",
       "        \n",
       "        # Strategy 2: Add some variation within segments\n",
       "        if num_frames > 4:\n",
       "            # Replace some middle frames with quarter and three-quarter points\n",
       "            for i in range(1, min(num_frames-1, 4)):\n",
       "                segment_start = i * segment_size\n",
       "                quarter_point = segment_start + segment_size // 4\n",
       "                frame_indices[i] = min(quarter_point, total_frames - 1)\n",
       "        \n",
       "        frames = []\n",
       "        for frame_idx in frame_indices:\n",
       "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
       "            ret, frame = cap.read()\n",
       "            if ret:\n",
       "                # Quality check - skip very dark or very bright frames\n",
       "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
       "                mean_brightness = np.mean(gray)\n",
       "                \n",
       "                # Skip frames that are too dark (<30) or too bright (>220)\n",
       "                if 30 < mean_brightness < 220:\n",
       "                    frames.append(frame)\n",
       "        \n",
       "        cap.release()\n",
       "        \n",
       "        # If we don't have enough good frames, fall back to simple linear sampling\n",
       "        if len(frames) < 3:\n",
       "            cap = cv2.VideoCapture(video_path)\n",
       "            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
       "            frames = []\n",
       "            for frame_idx in frame_indices:\n",
       "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
       "                ret, frame = cap.read()\n",
       "                if ret:\n",
       "                    frames.append(frame)\n",
       "            cap.release()\n",
       "        \n",
       "        return frames\n",
       "    \n",
       "    def create_enhanced_montage(self, frames: List[np.ndarray], max_width: int = 1920, target_height: int = 400) -> np.ndarray:\n",
       "        \"\"\"\n",
       "        Create enhanced montage with quality improvements.\n",
       "        \"\"\"\n",
       "        if not frames:\n",
       "            return None\n",
       "        \n",
       "        # Filter out very similar frames to improve diversity\n",
       "        filtered_frames = []\n",
       "        for i, frame in enumerate(frames):\n",
       "            if i == 0:\n",
       "                filtered_frames.append(frame)\n",
       "            else:\n",
       "                # Calculate similarity with previous frame\n",
       "                prev_frame = cv2.resize(frames[i-1], (100, 100))\n",
       "                curr_frame = cv2.resize(frame, (100, 100))\n",
       "                diff = cv2.absdiff(prev_frame, curr_frame)\n",
       "                similarity = np.mean(diff)\n",
       "                \n",
       "                # Only add frame if it's sufficiently different\n",
       "                if similarity > 10:\n",
       "                    filtered_frames.append(frame)\n",
       "        \n",
       "        # Use filtered frames, but ensure we have at least 3 frames\n",
       "        if len(filtered_frames) < 3:\n",
       "            filtered_frames = frames[:min(len(frames), 5)]\n",
       "        \n",
       "        # Resize frames to consistent height with better interpolation\n",
       "        resized_frames = []\n",
       "        for frame in filtered_frames:\n",
       "            h, w = frame.shape[:2]\n",
       "            aspect_ratio = w / h\n",
       "            new_width = int(target_height * aspect_ratio)\n",
       "            \n",
       "            # Use INTER_LANCZOS4 for better quality when upscaling\n",
       "            if new_width > w:\n",
       "                resized_frame = cv2.resize(frame, (new_width, target_height), interpolation=cv2.INTER_LANCZOS4)\n",
       "            else:\n",
       "                resized_frame = cv2.resize(frame, (new_width, target_height), interpolation=cv2.INTER_AREA)\n",
       "            \n",
       "            # Apply slight sharpening to improve clarity\n",
       "            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
       "            sharpened = cv2.filter2D(resized_frame, -1, kernel)\n",
       "            # Blend original and sharpened (70% original, 30% sharpened)\n",
       "            enhanced_frame = cv2.addWeighted(resized_frame, 0.7, sharpened, 0.3, 0)\n",
       "            \n",
       "            resized_frames.append(enhanced_frame)\n",
       "        \n",
       "        # Create montage with spacing between frames\n",
       "        spacing = 10  # pixels between frames\n",
       "        total_width = sum(frame.shape[1] for frame in resized_frames) + spacing * (len(resized_frames) - 1)\n",
       "        \n",
       "        # Create montage with white background\n",
       "        montage = np.ones((target_height, total_width, 3), dtype=np.uint8) * 255\n",
       "        \n",
       "        # Place frames with spacing\n",
       "        x_offset = 0\n",
       "        for frame in resized_frames:\n",
       "            h, w = frame.shape[:2]\n",
       "            montage[0:h, x_offset:x_offset+w] = frame\n",
       "            x_offset += w + spacing\n",
       "        \n",
       "        # Resize if too wide, using high-quality interpolation\n",
       "        if montage.shape[1] > max_width:\n",
       "            scale = max_width / montage.shape[1]\n",
       "            new_height = int(montage.shape[0] * scale)\n",
       "            montage = cv2.resize(montage, (max_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
       "        \n",
       "        # Apply final enhancement\n",
       "        alpha = 1.1  # Contrast control\n",
       "        beta = 5     # Brightness control\n",
       "        montage = cv2.convertScaleAbs(montage, alpha=alpha, beta=beta)\n",
       "        \n",
       "        return montage\n",
       "    \n",
       "    def analyze_video_characteristics(self, video_path: str) -> Dict[str, Any]:\n",
       "        \"\"\"\n",
       "        Analyze video characteristics for adaptive processing.\n",
       "        \"\"\"\n",
       "        cap = cv2.VideoCapture(video_path)\n",
       "        \n",
       "        characteristics = {\n",
       "            'aspect_ratio': cap.get(cv2.CAP_PROP_FRAME_WIDTH) / cap.get(cv2.CAP_PROP_FRAME_HEIGHT),\n",
       "            'total_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),\n",
       "            'fps': cap.get(cv2.CAP_PROP_FPS),\n",
       "            'duration': int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) / cap.get(cv2.CAP_PROP_FPS),\n",
       "            'motion_level': 'unknown',\n",
       "            'scene_changes': 0,\n",
       "            'dominant_orientation': 'landscape'\n",
       "        }\n",
       "        \n",
       "        # Determine orientation\n",
       "        if characteristics['aspect_ratio'] < 1.0:\n",
       "            characteristics['dominant_orientation'] = 'portrait'\n",
       "        elif characteristics['aspect_ratio'] > 1.5:\n",
       "            characteristics['dominant_orientation'] = 'wide_landscape'\n",
       "        \n",
       "        cap.release()\n",
       "        return characteristics\n",
       "    \n",
       "    def create_adaptive_summary(self, video_path: str, characteristics: Dict[str, Any]) -> Dict[str, np.ndarray]:\n",
       "        \"\"\"\n",
       "        Create adaptive summary based on video characteristics.\n",
       "        \"\"\"\n",
       "        summaries = {}\n",
       "        \n",
       "        if characteristics['duration'] > 300:  # Long video (>5 minutes)\n",
       "            # Long video: create hierarchical summary\n",
       "            overview_frames = self.extract_keyframes(video_path, num_frames=6)\n",
       "            summaries['overview'] = self.create_enhanced_montage(overview_frames, max_width=1920, target_height=300)\n",
       "            \n",
       "            detailed_frames = self.extract_keyframes(video_path, num_frames=12)\n",
       "            summaries['detailed'] = self.create_enhanced_montage(detailed_frames, max_width=2400, target_height=200)\n",
       "        else:\n",
       "            # Standard video: enhanced montages\n",
       "            frames = self.extract_keyframes(video_path, num_frames=8)\n",
       "            \n",
       "            # Choose layout based on orientation\n",
       "            if characteristics['dominant_orientation'] == 'portrait':\n",
       "                summaries['primary'] = self.create_enhanced_montage(frames, max_width=1200, target_height=500)\n",
       "            else:\n",
       "                summaries['primary'] = self.create_enhanced_montage(frames, max_width=1920, target_height=400)\n",
       "        \n",
       "        return summaries\n",
       "    \n",
       "    def analyze_with_model(self, video_item: dl.Item, summaries: Dict[str, np.ndarray]) -> str:\n",
       "        \"\"\"\n",
       "        Enhanced video analysis using multiple summaries.\n",
       "        \"\"\"\n",
       "        analyses = []\n",
       "        \n",
       "        # Process each summary type\n",
       "        for summary_type, montage in summaries.items():\n",
       "            if montage is not None:\n",
       "                # Convert montage to PIL Image\n",
       "                montage_rgb = cv2.cvtColor(montage, cv2.COLOR_BGR2RGB)\n",
       "                pil_image = Image.fromarray(montage_rgb)\n",
       "                \n",
       "                # Save and upload montage\n",
       "                with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:\n",
       "                    pil_image.save(temp_file.name, format='JPEG', quality=95)\n",
       "                    temp_file_path = temp_file.name\n",
       "                \n",
       "                try:\n",
       "                    # Upload montage\n",
       "                    montage_item = self.dataset.items.upload(\n",
       "                        local_path=temp_file_path,\n",
       "                        remote_path=f\"/.dataloop/temp_montages/\",\n",
       "                        remote_name=f\"{video_item.name}_{summary_type}_montage.jpg\",\n",
       "                        overwrite=True,\n",
       "                        item_metadata={\"user\": {\"source_video\": video_item.id, \"temp_file\": True}}\n",
       "                    )\n",
       "                    \n",
       "                    # Create analysis prompt\n",
       "                    model_prompt = f\"\"\"Analyze this {summary_type} video montage from {video_item.name}.\n",
       "                    \n",
       "                                        This montage shows {'detailed frames' if 'detailed' in summary_type else 'key moments'} from the video.\n",
       "\n",
       "                                        Please provide comprehensive analysis focusing on:\n",
       "                                        1. Room identification and layout\n",
       "                                        2. People and their activities  \n",
       "                                        3. Objects and furniture visible\n",
       "                                        4. Actions being performed\n",
       "                                        5. Context and temporal progression\n",
       "                                        6. Specific details for annotation\n",
       "\n",
       "                                        Be thorough and specific in your observations.\"\"\"\n",
       "                    \n",
       "                    # Process with model\n",
       "                    analysis = self.process_single_montage_with_model(montage_item, model_prompt)\n",
       "                    analyses.append(f\"=== {summary_type.upper()} ANALYSIS ===\\n{analysis}\")\n",
       "                    \n",
       "                finally:\n",
       "                    if os.path.exists(temp_file_path):\n",
       "                        os.remove(temp_file_path)\n",
       "        \n",
       "        # Combine all analyses\n",
       "        combined_analysis = \"\\n\\n\".join(analyses)\n",
       "        \n",
       "        return combined_analysis\n",
       "    \n",
       "    def process_single_montage_with_model(self, montage_item: dl.Item, model_prompt: str) -> str:\n",
       "        \"\"\"\n",
       "        Process a single montage image with model.\n",
       "        \"\"\"\n",
       "        try:\n",
       "            # Create prompt item\n",
       "            prompt_item = dl.PromptItem(name=f\"model_montage_analysis_{montage_item.name}\")\n",
       "            prompt = dl.Prompt(key='1')\n",
       "            prompt.add_element(mimetype=dl.PromptType.TEXT, value=model_prompt)\n",
       "            prompt.add_element(mimetype=dl.PromptType.IMAGE, value=montage_item.id)\n",
       "            prompt_item.prompts.append(prompt)\n",
       "            \n",
       "            # Upload and process\n",
       "            temp_prompt_item = self.dataset.items.upload(\n",
       "                prompt_item,\n",
       "                overwrite=True,\n",
       "                remote_path=f\"/.dataloop/temp_model_analysis/\",\n",
       "                item_metadata={\"user\": {\"source_montage\": montage_item.id}}\n",
       "            )\n",
       "            \n",
       "            # Get model analysis\n",
       "            execution = self.model.predict(item_ids=[temp_prompt_item.id])\n",
       "            execution.wait()\n",
       "            \n",
       "            # Extract response\n",
       "            annotations = temp_prompt_item.annotations.list()\n",
       "            if annotations:\n",
       "                analysis = annotations[0].coordinates\n",
       "            else:\n",
       "                analysis = \"No analysis generated\"\n",
       "            \n",
       "            # Clean up temporary item\n",
       "            temp_prompt_item.delete()\n",
       "            \n",
       "            return analysis\n",
       "            \n",
       "        except Exception as e:\n",
       "            return f\"Error processing montage: {str(e)}\"\n",
       "    \n",
       "    def generate_annotations(self, analysis: str) -> Dict[str, Any]:\n",
       "        \"\"\"\n",
       "        Generate annotations with comprehensive analysis approach.\n",
       "        \"\"\"\n",
       "        results = {}\n",
       "        \n",
       "        # Enhanced prompts with more context\n",
       "        annotation_types = [\n",
       "            (\"room_classification\", \"room\"),\n",
       "            (\"participant_description\", \"participant\"),\n",
       "            (\"action_description\", \"action\"),\n",
       "            (\"object_detection\", \"objects\")\n",
       "        ]\n",
       "        \n",
       "        for annotation_type, result_key in annotation_types:\n",
       "            try:\n",
       "                data = self.generate_structured_annotation(analysis, annotation_type)\n",
       "                if \"error\" not in data:\n",
       "                    if result_key == \"objects\" and \"objects\" in data:\n",
       "                        results[result_key] = data[\"objects\"][:10]\n",
       "                    else:\n",
       "                        results[result_key] = data\n",
       "                else:\n",
       "                    results[result_key] = None if result_key != \"objects\" else []\n",
       "            except Exception as e:\n",
       "                results[result_key] = None if result_key != \"objects\" else []\n",
       "        \n",
       "        return results\n",
       "    \n",
       "    def clean_malformed_json(self, json_text: str) -> str:\n",
       "        \"\"\"\n",
       "        Enhanced JSON cleaning - handles both objects and arrays.\n",
       "        \"\"\"\n",
       "        import re\n",
       "        import json\n",
       "        \n",
       "        # Remove common markdown artifacts\n",
       "        json_text = json_text.strip()\n",
       "        \n",
       "        # First, try to parse as-is - if it's already valid, don't fix it\n",
       "        try:\n",
       "            json.loads(json_text)\n",
       "            return json_text  # Already valid JSON, return as-is\n",
       "        except json.JSONDecodeError:\n",
       "            pass  # Need to clean it\n",
       "        \n",
       "        # Check if this looks like an array or object\n",
       "        if json_text.startswith('[') and json_text.endswith(']'):\n",
       "            # This is an array - don't modify the structure, just clean trailing commas\n",
       "            json_text = re.sub(r',(\\s*[}\\]])', r'\\1', json_text)\n",
       "        elif json_text.startswith('{') and json_text.endswith('}'):\n",
       "            # This is an object - don't modify the structure, just clean trailing commas  \n",
       "            json_text = re.sub(r',(\\s*[}\\]])', r'\\1', json_text)\n",
       "        else:\n",
       "            # Extract JSON from mixed content\n",
       "            start_idx = json_text.find('{')\n",
       "            end_idx = json_text.rfind('}')\n",
       "            \n",
       "            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
       "                json_text = json_text[start_idx:end_idx + 1]\n",
       "                # Clean trailing commas\n",
       "                json_text = re.sub(r',(\\s*[}\\]])', r'\\1', json_text)\n",
       "        \n",
       "        return json_text\n",
       "    \n",
       "    def generate_structured_annotation(self, analysis: str, annotation_type: str) -> dict:\n",
       "        \"\"\"\n",
       "        Generate annotations with updated schemas.\n",
       "        \"\"\"\n",
       "        # Get schema based on type\n",
       "        schema_map = {\n",
       "            \"room_classification\": {\n",
       "                \"type\": \"object\",\n",
       "                \"properties\": {\n",
       "                    \"reference_rooms\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
       "                    \"room_description\": {\"type\": \"string\"},\n",
       "                    \"participant_activity_description\": {\"type\": \"string\"}\n",
       "                }\n",
       "            },\n",
       "            \"participant_description\": {\n",
       "                \"type\": \"array\",\n",
       "                \"items\": {\n",
       "                    \"type\": \"object\", \n",
       "                    \"properties\": {\n",
       "                        \"participant_description\": {\"type\": \"string\"}\n",
       "                    },\n",
       "                    \"required\": [\"participant_description\"]\n",
       "                },\n",
       "                \"description\": \"Array of participant objects, one per participant\"\n",
       "            },\n",
       "            \"action_description\": {\n",
       "                \"type\": \"object\",\n",
       "                \"properties\": {\n",
       "                    \"action_description\": {\"type\": \"string\"}\n",
       "                }\n",
       "            },\n",
       "            \"object_detection\": {\n",
       "                \"type\": \"object\",\n",
       "                \"properties\": {\n",
       "                    \"objects\": {\n",
       "                        \"type\": \"array\",\n",
       "                        \"items\": {\n",
       "                            \"type\": \"object\",\n",
       "                            \"properties\": {\n",
       "                                \"object\": {\"type\": \"string\"},\n",
       "                                \"object_color\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\n",
       "                                        \"Basic: white, black, brown, gray, beige\",\n",
       "                                        \"Wood tones: mahogany, oak, walnut, cherry\",\n",
       "                                        \"Metals: silver, bronze, golden, copper\",\n",
       "                                        \"Other: cream, navy, burgundy, sage\"\n",
       "                                    ]\n",
       "                                },\n",
       "                                \"object_size_dimension\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Tall\", \"Short\", \"Wide\", \"Narrow\", \"Deep\", \"Shallow\"]\n",
       "                                },\n",
       "                                \"object_size_overall\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Large\", \"Medium\", \"Small\", \"Compact\", \"Massive\"]\n",
       "                                },\n",
       "                                \"object_size_space_occupation\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Bulky\", \"Slim\", \"Spacious\"]\n",
       "                                },\n",
       "                                \"object_shape_basic_forms\": {\n",
       "                                    \"type\": \"array\",\n",
       "                                    \"items\": {\n",
       "                                        \"type\": \"string\",\n",
       "                                        \"enum\": [\"Rectangular\", \"Square\", \"Round\", \"Oval\", \"Circular\"]\n",
       "                                    }\n",
       "                                },\n",
       "                                \"object_shape_structural\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Curved\", \"Straight\", \"Angular\", \"Cylindrical\"]\n",
       "                                },\n",
       "                                \"object_shape_design\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Symmetrical\", \"Irregular\", \"Geometric\"]\n",
       "                                },\n",
       "                                \"object_materials\": {\n",
       "                                    \"type\": \"array\",\n",
       "                                    \"items\": {\n",
       "                                        \"type\": \"string\",\n",
       "                                        \"enum\": [\n",
       "                                            \"Natural: wooden, leather, cotton, wool, wicker\",\n",
       "                                            \"Synthetic: plastic, polyester, nylon, vinyl\",\n",
       "                                            \"Metals: metallic, steel, aluminum, brass\",\n",
       "                                            \"Glass: transparent, frosted, tinted\",\n",
       "                                            \"Stone: marble, granite, concrete\"\n",
       "                                        ]\n",
       "                                    }\n",
       "                                },\n",
       "                                \"object_surface_touch\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Smooth\", \"Rough\", \"Textured\", \"Bumpy\"]\n",
       "                                },\n",
       "                                \"object_surface_finish\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Glossy\", \"Matte\", \"Polish\", \"Rustic\"]\n",
       "                                },\n",
       "                                \"object_condition_quality\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"New\", \"Used\", \"Vintage\", \"Modern\"]\n",
       "                                },\n",
       "                                \"object_condition_state\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"Clean\", \"Worn\", \"Pristine\", \"Damaged\"]\n",
       "                                },\n",
       "                                \"object_condition_power\": {\n",
       "                                    \"type\": \"string\",\n",
       "                                    \"enum\": [\"On\", \"Off\"]\n",
       "                                }\n",
       "                            },\n",
       "                            \"required\": [\"object\"]\n",
       "                        }\n",
       "                    }\n",
       "                },\n",
       "                \"required\": [\"objects\"]\n",
       "            }\n",
       "        }\n",
       "        \n",
       "        schema = schema_map.get(annotation_type, {})\n",
       "        \n",
       "        # Enhanced prompt for complex schemas\n",
       "        if annotation_type == \"object_detection\":\n",
       "            model_prompt = f\"\"\"Based on the comprehensive video analysis, generate detailed object detection annotation as valid JSON.\n",
       "\n",
       "                                Video Analysis:\n",
       "                                {analysis}\n",
       "\n",
       "                                Generate a JSON response with \"objects\" array. For each object, include:\n",
       "                                - object: name of the object (required)\n",
       "                                - object_color: choose from [\"Basic: white, black, brown, gray, beige\", \"Wood tones: mahogany, oak, walnut, cherry\", \"Metals: silver, bronze, golden, copper\", \"Other: cream, navy, burgundy, sage\"]\n",
   
       "                                - object_size_dimension: choose from [\"Tall\", \"Short\", \"Wide\", \"Narrow\", \"Deep\", \"Shallow\"]\n",
       "                                - object_size_overall: choose from [\"Large\", \"Medium\", \"Small\", \"Compact\", \"Massive\"]\n",
       "                                - object_size_space_occupation: choose from [\"Bulky\", \"Slim\", \"Spacious\"]\n",
       "                                - object_shape_basic_forms: array from [\"Rectangular\", \"Square\", \"Round\", \"Oval\", \"Circular\"]\n",
       "                                - object_shape_structural: choose from [\"Curved\", \"Straight\", \"Angular\", \"Cylindrical\"]\n",
       "                                - object_shape_design: choose from [\"Symmetrical\", \"Irregular\", \"Geometric\"]\n",
       "                                - object_materials: array from [\"Natural: wooden, leather, cotton, wool, wicker\", \"Synthetic: plastic, polyester, nylon, vinyl\", \"Metals: metallic, steel, aluminum, brass\", \"Glass: transparent, frosted, tinted\", \"Stone: marble, granite, concrete\"]\n",
       "                                - object_surface_touch: choose from [\"Smooth\", \"Rough\", \"Textured\", \"Bumpy\"]\n",
       "                                - object_surface_finish: choose from [\"Glossy\", \"Matte\", \"Polish\", \"Rustic\"]\n",
       "                                - object_condition_quality: choose from [\"New\", \"Used\", \"Vintage\", \"Modern\"]\n",
       "                                - object_condition_state: choose from [\"Clean\", \"Worn\", \"Pristine\", \"Damaged\"]\n",
       "                                - object_condition_power: choose from [\"On\", \"Off\"]\n",
       "\n",
       "                                CRITICAL INSTRUCTIONS: \n",
       "                                - Use EXACT COMPLETE enum values from the lists above\n",
       "                                - Use proper JSON syntax with double quotes\n",
       "                                - Ensure all arrays and objects are properly closed\n",
       "                                - Focus on the most prominent/important objects only\n",
       "                                - Analyze each object carefully and provide detailed attributes\n",
       "\n",
       "                                Example format:\n",
       "                                {{\"objects\": [{{\"object\": \"dining table\", \"object_color\": \"Wood tones: mahogany, oak, walnut, cherry\", \"object_size_overall\": \"Large\", \"object_materials\": [\"Natural: wooden, leather, cotton, wool, wicker\"], \"object_shape_basic_forms\": [\"Rectangular\"], \"object_condition_quality\": \"Used\"}}]}}\n",
       "\n",
       "                                Respond with ONLY valid JSON:\"\"\"\n",
       "        \n",
       "\n",
       "        # Enhanced prompt for participant descriptions\n",
       "        elif annotation_type == \"participant_description\":\n",
       "            model_prompt = f\"\"\"Based on the comprehensive video analysis, identify and describe each individual participant separately.\n",
       "\n",
       "                                Video Analysis:\n",
       "                                {analysis}\n",
       "\n",
       "                                Generate a JSON array where each element is an object with \"participant_description\" field containing one participant description.\n",
       "\n",
       "                                CRITICAL INSTRUCTIONS:\n",
       "                                - Create ONE object per participant you can clearly see\n",
       "                                - Each object MUST have a \"participant_description\" field with detailed description of that individual person\n",
       "                                - Include their appearance, activity, and any notable characteristics\n",
       "                                - If you can only see one person, provide only one object\n",
       "                                - Use proper JSON syntax with double quotes\n",
       "                                - ALWAYS return an array, even for a single participant\n",
       "\n",
       "                                REQUIRED FORMAT - Array of objects:\n",
       "                                [\n",
       "                                {{\"participant_description\": \"Adult woman wearing blue shirt sitting at kitchen table eating breakfast\"}},\n",
       "                                {{\"participant_description\": \"Young child in red pajamas playing with toys on the living room floor\"}}\n",
       "                                ]\n",
       "\n",
       "                                For a single participant, still use array format:\n",
       "                                [\n",
       "                                {{\"participant_description\": \"Middle-aged man in casual clothes cooking at the stove\"}}\n",
       "                                ]\n",
       "\n",
       "                                Respond with ONLY valid JSON array (not an object with participants key):\"\"\"\n",
       "        else:\n",
       "            model_prompt = f\"\"\"Based on the comprehensive video analysis, generate a detailed {annotation_type} annotation in JSON format.\n",
       "\n",
       "                                Video Analysis:\n",
       "                                {analysis}\n",
       "\n",
       "                                Please generate a JSON response that follows this exact schema:\n",
       "                                {json.dumps(schema, indent=2)}\n",
       "\n",
       "                                Instructions:\n",
       "                                - Follow the JSON schema exactly\n",
       "                                - Use only the provided enum values where specified\n",
       "                                - Be specific and accurate based on the analysis\n",
       "                                - Include all required fields\n",
       "                                - Respond with ONLY valid JSON, no additional text\n",
       "\n",
       "                                Generate the {annotation_type} annotation:\"\"\"\n",
       "        \n",
       "        # Create and process prompt\n",
       "        prompt_item = dl.PromptItem(name=f\"enhanced_{annotation_type}\")\n",
       "        prompt = dl.Prompt(key='1')\n",
       "        prompt.add_element(mimetype=dl.PromptType.TEXT, value=model_prompt)\n",
       "        prompt_item.prompts.append(prompt)\n",
       "        \n",
       "        temp_item = self.dataset.items.upload(\n",
       "            prompt_item,\n",
       "            overwrite=True,\n",
       "            remote_path=f\"/.dataloop/temp_model_enhanced/\"\n",
       "        )\n",
       "        \n",
       "        try:\n",
       "            # Get model response\n",
       "            execution = self.model.predict(item_ids=[temp_item.id])\n",
       "            execution.wait()\n",
       "            \n",
       "            # Extract and parse response\n",
       "            annotations = temp_item.annotations.list()\n",
       "            if annotations:\n",
       "                response_text = annotations[0].coordinates.strip()\n",
       "                \n",
       "                # Clean response\n",
       "                if response_text.startswith('```json'):\n",
       "                    response_text = response_text[7:]\n",
       "                if response_text.endswith('```'):\n",
       "                    response_text = response_text[:-3]\n",
       "                response_text = response_text.strip()\n",
       "                \n",
       "                # Enhanced JSON cleaning for malformed responses\n",
       "                response_text = self.clean_malformed_json(response_text)\n",
       "                \n",
       "                parsed_response = json.loads(response_text)\n",
       "                \n",
       "                normalized_response = self.normalize_annotation_response(parsed_response, annotation_type)\n",
       "                \n",
       "                return normalized_response\n",
       "            else:\n",
       "                return {\"error\": \"No response generated\"}\n",
       "                \n",
       "        except json.JSONDecodeError as e:\n",
       "            return {\"error\": f\"Invalid JSON: {str(e)}\"}\n",
       "        except Exception as e:\n",
       "            return {\"error\": f\"Processing error: {str(e)}\"}\n",
       "        finally:\n",
       "            temp_item.delete()\n",
       "\n",
       "    def normalize_annotation_response(self, response: Any, annotation_type: str) -> dict:\n",
       "        \"\"\"\n",
       "        Normalize the response to ensure it matches the expected format.\n",
       "        \"\"\"\n",
       "        if annotation_type == \"participant_description\":\n",
       "            # Ensure we always return a properly formatted array\n",
       "            if isinstance(response, list):\n",
       "                # Validate each item in the list\n",
       "                normalized_list = []\n",
       "                for item in response:\n",
       "                    if isinstance(item, dict) and \"participant_description\" in item:\n",
       "                        normalized_list.append(item)\n",
       "                    elif isinstance(item, str):\n",
       "                        # Convert string to proper format\n",
       "                        normalized_list.append({\"participant_description\": item})\n",
       "                return normalized_list if normalized_list else [{\"participant_description\": \"No participants detected\"}]\n",
       "            \n",
       "            elif isinstance(response, dict):\n",
       "                # Handle legacy format like {\"participants\": [\"desc1\", \"desc2\"]}\n",
       "                if \"participants\" in response:\n",
       "                    participants = response[\"participants\"]\n",
       "                    if isinstance(participants, list):\n",
       "                        return [{\"participant_description\": desc} for desc in participants if isinstance(desc, str)]\n",
       "                # Handle single participant object\n",
       "                elif \"participant_description\" in response:\n",
       "                    return [response]\n",
       "            \n",
       "            # Fallback\n",
       "            return [{\"participant_description\": \"Error: Invalid participant data format\"}]\n",
       "        \n",
       "        elif annotation_type == \"object_detection\":\n",
       "            # Ensure we have the objects key and it's a list\n",
       "            if isinstance(response, dict) and \"objects\" in response:\n",
       "                objects = response[\"objects\"]\n",
       "                if isinstance(objects, list):\n",
       "                    # Validate each object has required fields\n",
       "                    validated_objects = []\n",
       "                    for obj in objects:\n",
       "                        if isinstance(obj, dict) and \"object\" in obj:\n",
       "                            validated_objects.append(obj)\n",
       "                    return {\"objects\": validated_objects}\n",
       "            \n",
       "            # Fallback\n",
       "            return {\"objects\": []}\n",
       "        \n",
       "        elif annotation_type == \"room_classification\":\n",
       "            # Ensure it's a dict with required fields\n",
       "            if isinstance(response, dict):\n",
       "                normalized = {}\n",
       "                # Ensure all required fields exist\n",
       "                normalized[\"reference_rooms\"] = response.get(\"reference_rooms\", [])\n",
       "                normalized[\"room_description\"] = response.get(\"room_description\", \"\")\n",
       "                normalized[\"participant_activity_description\"] = response.get(\"participant_activity_description\", \"\")\n",
       "                return normalized\n",
       "            \n",
       "            # Fallback\n",
       "            return {\n",
       "                \"reference_rooms\": [],\n",
       "                \"room_description\": \"Error: Invalid room data format\",\n",
       "                \"participant_activity_description\": \"\"\n",
       "            }\n",
       "        \n",
       "        elif annotation_type == \"action_description\":\n",
       "            # Ensure it's a dict with action_description field\n",
       "            if isinstance(response, dict) and \"action_description\" in response:\n",
       "                return response\n",
       "            elif isinstance(response, str):\n",
       "                return {\"action_description\": response}\n",
       "            \n",
       "            # Fallback\n",
       "            return {\"action_description\": \"Error: Invalid action data format\"}\n",
       "        \n",
       "        return response\n",
       "\n",
       "    def create_dataloop_annotations(self, annotations_data: Dict[str, Any]) -> List[dl.Annotation]:\n",
       "        \"\"\"\n",
       "        Create Dataloop annotations from the generated data.\n",
       "        Now expects properly normalized data formats.\n",
       "        \"\"\"\n",
       "        annotations_to_create = []\n",
       "        \n",
       "        # Room annotation\n",
       "        if annotations_data.get(\"room\"):\n",
       "            try:\n",
       "                # Ensure room data is a dictionary\n",
       "                room_data = annotations_data[\"room\"]\n",
       "                if not isinstance(room_data, dict):\n",
       "                    print(f\"Warning: Room data is not a dict: {type(room_data)} - {room_data}\")\n",
       "                    room_data = {\"room_description\": str(room_data)}\n",
       "                \n",
       "                room_annotation = dl.Annotation.new(\n",
       "                    annotation_definition=dl.Classification(\n",
       "                        label=\"room\",\n",
       "                        attributes=room_data\n",
       "                    ),\n",
       "                    frame_num=0\n",
       "                )\n",
       "                annotations_to_create.append(room_annotation)\n",
       "            except Exception as e:\n",
       "                print(f\"Error creating room annotation: {e}\")\n",
       "                print(f\"Room data: {annotations_data['room']}\")\n",
       "        \n",
       "        # Individual participant annotations (one per participant)\n",
       "        if annotations_data.get(\"participant\"):\n",
       "            try:\n",
       "                participant_list = annotations_data[\"participant\"]\n",
       "                if not isinstance(participant_list, list):\n",
       "                    print(f\"Warning: Participant data is not a list: {type(participant_list)} - {participant_list}\")\n",
       "                    participant_list = [participant_list] if isinstance(participant_list, dict) else []\n",
       "                \n",
       "                for i, participant in enumerate(participant_list):\n",
       "                    if not isinstance(participant, dict):\n",
       "                        print(f\"Warning: Participant {i} is not a dict: {type(participant)} - {participant}\")\n",
       "                        participant = {\"participant_description\": str(participant)}\n",
       "                    \n",
       "                    participant_annotation = dl.Annotation.new(\n",
       "                        annotation_definition=dl.Classification(\n",
       "                            label=\"participant description\",\n",
       "                            attributes=participant\n",
       "                        ),\n",
       "                        frame_num=0\n",
       "                    )\n",
       "                    annotations_to_create.append(participant_annotation)\n",
       "            except Exception as e:\n",
       "                print(f\"Error creating participant annotations: {e}\")\n",
       "                print(f\"Participant data: {annotations_data['participant']}\")\n",
       "        \n",
       "        # Action annotation\n",
       "        if annotations_data.get(\"action\"):\n",
       "            try:\n",
       "                # Ensure action data is a dictionary\n",
       "                action_data = annotations_data[\"action\"]\n",
       "                if not isinstance(action_data, dict):\n",
       "                    print(f\"Warning: Action data is not a dict: {type(action_data)} - {action_data}\")\n",
       "                    action_data = {\"action_description\": str(action_data)}\n",
       "                \n",
       "                action_annotation = dl.Annotation.new(\n",
       "                    annotation_definition=dl.Classification(\n",
       "                        label=\"actions\",\n",
       "                        attributes=action_data\n",
       "                    ),\n",
       "                    frame_num=0\n",
       "                )\n",
       "                annotations_to_create.append(action_annotation)\n",
       "            except Exception as e:\n",
       "                print(f\"Error creating action annotation: {e}\")\n",
       "                print(f\"Action data: {annotations_data['action']}\")\n",
       "        \n",
       "        # Object annotations\n",
       "        for i, obj in enumerate(annotations_data.get(\"objects\", [])):\n",
       "            try:\n",
       "                if not isinstance(obj, dict):\n",
       "                    print(f\"Warning: Object {i} is not a dict: {type(obj)} - {obj}\")\n",
       "                    obj = {\"object\": str(obj)}\n",
       "                \n",
       "                obj_annotation = dl.Annotation.new(\n",
       "                    annotation_definition=dl.Classification(\n",
       "                        label=\"objects\",\n",
       "                        attributes=obj\n",
       "                    ),\n",
       "                    frame_num=0\n",
       "                )\n",
       "                annotations_to_create.append(obj_annotation)\n",
       "            except Exception as e:\n",
       "                print(f\"Error creating object annotation {i}: {e}\")\n",
       "                print(f\"Object data: {obj}\")\n",
       "        \n",
       "        return annotations_to_create\n",
       "\n",
       "    def print_processing_summary(self, results: Dict[str, Any]):\n",
       "        \"\"\"\n",
       "        Print processing summary.\n",
       "        \"\"\"\n",
       "        timing = results.get(\"timing\", {})\n",
       "        total_time = timing.get(\"total_time\", 0)\n",
       "        mode = results.get(\"processing_mode\", \"unknown\")\n",
       "        \n",
       "        print(f\"\\n=== PROCESSING SUMMARY FOR {results['video_name']} ===\")\n",
       "        print(f\"Processing Mode: {mode.upper()}\")\n",
       "        print(f\"Total Time: {total_time:.2f}s\")\n",
       "        print(f\"Status: {results['status']}\")\n",
       "        print(f\"Annotations Created: {results['annotations_created']}\")\n",
       "        \n",
       "        if results.get(\"room_annotation\"):\n",
       "            print(f\"âœ“ Room classification generated\")\n",
       "        if results.get(\"participant_annotation\") and \"participants\" in results[\"participant_annotation\"]:\n",
       "            participant_count = len(results[\"participant_annotation\"][\"participants\"])\n",
       "            print(f\"âœ“ {participant_count} individual participant annotation(s) generated\")\n",
       "        if results.get(\"action_annotation\"):\n",
       "            print(f\"âœ“ Action description generated\")\n",
       "        if results.get(\"object_annotations\"):\n",
       "            print(f\"âœ“ {len(results['object_annotations'])} object annotations generated\")\n",
       "        \n",
       "        if results.get(\"errors\"):\n",
       "            print(f\"âš ï¸ Errors: {len(results['errors'])}\")\n",
       "            for error in results[\"errors\"]:\n",
       "                print(f\"  - {error}\")\n",
       "\n",
       "print(\"VideoAnnotations class defined successfully\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='initialize-processor'></a>4. Initialize Video Processor\n",
       "\n",
       "With the class defined, we now create an instance of it, passing our Dataloop dataset and model objects. This `processor` object is now ready to be used for annotation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize the video annotation processor\n",
       "processor = VideoAnnotations(dataset, model)\n",
       "print(\"Video annotation processor initialized successfully\")\n",
       "print(f\"Using dataset: {dataset.name}\")\n",
       "print(f\"Using model: {model.name}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='test-single-video'></a>5. Test Single Video Processing\n",
       "\n",
       "Before running the pipeline on the entire dataset, it's a good practice to test it on a single video. This helps ensure that all parts of the process are working correctly without consuming excessive time or resources.\n",
       "\n",
       "**Action Required:** You must provide a valid video `item_id` from your dataset in the cell below. Replace the placeholder with an actual ID."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get test videos from dataset\n",
       "print(\"Getting test video...\")\n",
       "\n",
       "test_video = dataset.items.get(item_id='<YOUR_VIDEO_ITEM_ID>')\n",
       "    \n",
       "print(f\"Testing with video: {test_video.name} (ID: {test_video.id})\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='process-single-video'></a>6. Process Test Video\n",
       "\n",
       "Now, we execute the comprehensive processing pipeline on the single test video. The output will show the step-by-step progress as the video is downloaded, analyzed, and annotated. At the end, it will print a detailed summary of the results, including the generated annotations and a breakdown of the time taken for each step."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Process the test video\n",
       "if test_video:\n",
       "    print(f\"\\n=== PROCESSING {test_video.name} ===\")\n",
       "    \n",
       "    # Define progress callback\n",
       "    def progress_callback(message):\n",
       "        print(f\"  {message}\")\n",
       "    \n",
       "    # Process video\n",
       "    result = processor.process_video(test_video, progress_callback=progress_callback)\n",
       "    \n",
       "    # Print detailed results\n",
       "    processor.print_processing_summary(result)\n",
       "    \n",
       "    print(f\"\\n=== DETAILED RESULTS ===\")\n",
       "    if result.get(\"room_annotation\"):\n",
       "        print(f\"\\nRoom Annotation:\")\n",
       "        print(json.dumps(result[\"room_annotation\"], indent=2))\n",
       "    \n",
       "    if result.get(\"participant_annotation\"):\n",
       "        print(f\"\\nParticipant Annotation:\")\n",
       "        print(json.dumps(result[\"participant_annotation\"], indent=2))\n",
       "    \n",
       "    if result.get(\"action_annotation\"):\n",
       "        print(f\"\\nAction Annotation:\")\n",
       "        print(json.dumps(result[\"action_annotation\"], indent=2))\n",
       "    \n",
       "    if result.get(\"object_annotations\"):\n",
       "        print(f\"\\nObject Annotations ({len(result['object_annotations'])} objects):\")\n",
       "        for i, obj in enumerate(result[\"object_annotations\"][:5]):  # Show first 5 objects\n",
       "            print(f\"  Object {i+1}: {obj.get('object', 'Unknown')}\")\n",
       "            if 'object_color' in obj:\n",
       "                print(f\"    Color: {obj['object_color']}\")\n",
       "            if 'object_size_overall' in obj:\n",
       "                print(f\"    Size: {obj['object_size_overall']}\")\n",
       "    \n",
       "    if result.get(\"video_characteristics\"):\n",
       "        print(f\"\\nVideo Characteristics:\")\n",
       "        print(json.dumps(result[\"video_characteristics\"], indent=2))\n",
       "    \n",
       "    # Show timing breakdown\n",
       "    if result.get(\"timing\"):\n",
       "        print(f\"\\nTiming Breakdown:\")\n",
       "        for step, duration in result[\"timing\"].items():\n",
       "            print(f\"  {step}: {duration:.2f}s\")\n",
       "else:\n",
       "    print(\"No test video available\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='batch-function'></a>7. Batch Processing Function\n",
       "\n",
       "To scale our operation, we define a function `process_video_batch`. This function iterates through a list of video items from our dataset, applies the same comprehensive processing pipeline to each one, and tracks the overall progress. It provides real-time updates, including an estimated time of completion (ETA) and a final summary of successful and failed jobs."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def process_video_batch(dataset, processor, max_videos=None, progress_interval=1):\n",
       "    \"\"\"\n",
       "    Process multiple videos using the comprehensive processor.\n",
       "    \n",
       "    Args:\n",
       "        dataset: Dataloop dataset\n",
       "        processor: VideoAnnotations processor instance\n",
       "        max_videos: Maximum number of videos to process (None for all)\n",
       "        progress_interval: How often to print progress updates\n",
       "    \"\"\"\n",
       "    # Get all video items\n",
       "    video_items = list(dataset.items.list().all())\n",
       "    \n",
       "    if max_videos:\n",
       "        video_items = video_items[:max_videos]\n",
       "    \n",
       "    print(f\"\\n=== BATCH PROCESSING {len(video_items)} VIDEOS ===\")\n",
       "    \n",
       "    results = {\n",
       "        'successful': [],\n",
       "        'failed': [],\n",
       "        'total_time': 0,\n",
       "        'start_time': time.time()\n",
       "    }\n",
       "    \n",
       "    for i, video_item in enumerate(video_items, 1):\n",
       "        print(f\"\\n[{i}/{len(video_items)}] Processing: {video_item.name}\")\n",
       "        \n",
       "        # # Skip if already annotated\n",
       "        # if video_item.annotations.list():\n",
       "        #     continue\n",
       "        \n",
       "        try:\n",
       "            # Process video\n",
       "            result = processor.process_video(\n",
       "                video_item, \n",
       "                progress_callback=lambda msg: print(f\"  {msg}\")\n",
       "            )\n",
       "            \n",
       "            if result['status'] == 'completed':\n",
       "                results['successful'].append({\n",
       "                    'name': video_item.name,\n",
       "                    'id': video_item.id,\n",
       "                    'annotations_created': result['annotations_created'],\n",
       "                    'processing_time': result['timing'].get('total_time', 0)\n",
       "                })\n",
       "                print(f\"  âœ“ Success: {result['annotations_created']} annotations created\")\n",
       "            else:\n",
       "                results['failed'].append({\n",
       "                    'name': video_item.name,\n",
       "                    'id': video_item.id,\n",
       "                    'errors': result.get('errors', ['Unknown error'])\n",
       "                })\n",
       "                print(f\"  âœ— Failed: {result.get('errors', ['Unknown error'])}\")\n",
       "                \n",
       "        except Exception as e:\n",
       "            results['failed'].append({\n",
       "                'name': video_item.name,\n",
       "                'id': video_item.id,\n",
       "                'errors': [str(e)]\n",
       "            })\n",
       "            print(f\"  âœ— Exception: {str(e)}\")\n",
       "        \n",
       "        # Progress update\n",
       "        if i % progress_interval == 0 or i == len(video_items):\n",
       "            elapsed = time.time() - results['start_time']\n",
       "            avg_time = elapsed / i\n",
       "            eta = avg_time * (len(video_items) - i)\n",
       "            success_rate = len(results['successful']) / i * 100\n",
       "            \n",
       "            print(f\"\\n  Progress: {i}/{len(video_items)} ({i/len(video_items)*100:.1f}%)\")\n",
       "            print(f\"  Success rate: {success_rate:.1f}%\")\n",
       "            print(f\"  Elapsed: {elapsed/60:.1f}min, ETA: {eta/60:.1f}min\")\n",
       "    \n",
       "    # Final summary\n",
       "    results['total_time'] = time.time() - results['start_time']\n",
       "    \n",
       "    print(f\"\\n=== BATCH PROCESSING COMPLETE ===\")\n",
       "    print(f\"Total videos: {len(video_items)}\")\n",
       "    print(f\"Successful: {len(results['successful'])}\")\n",
       "    print(f\"Failed: {len(results['failed'])}\")\n",
       "    print(f\"Success rate: {len(results['successful'])/len(video_items)*100:.1f}%\")\n",
       "    print(f\"Total time: {results['total_time']/60:.1f} minutes\")\n",
       "    print(f\"Average time per video: {results['total_time']/len(video_items):.1f} seconds\")\n",
       "    \n",
       "    if results['successful']:\n",
       "        total_annotations = sum(item['annotations_created'] for item in results['successful'])\n",
       "        print(f\"Total annotations created: {total_annotations}\")\n",
       "    \n",
       "    if results['failed']:\n",
       "        print(f\"\\nFailed videos:\")\n",
       "        for item in results['failed']:\n",
       "            print(f\"  - {item['name']}: {item['errors'][0] if item['errors'] else 'Unknown error'}\")\n",
       "    \n",
       "    return results\n",
       "\n",
       "print(\"Batch processing function defined\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='run-batch'></a>8. Run Full Batch Processing (Optional)\n",
       "\n",
       "This final execution cell runs the batch processing function on your dataset.\n",
       "\n",
       "> **Warning:** Running this cell may be time-consuming and could incur costs associated with GenAI model usage, as it will process multiple videos. \n",
       "\n",
       "For a trial run, you can uncomment the `max_videos` parameter and set it to a small number (e.g., `max_videos=5`) to limit the number of videos processed."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Run batch processing on all videos\n",
       "batch_results = process_video_batch(\n",
       "    dataset=dataset,\n",
       "    processor=processor,\n",
       "    # max_videos=5, # Uncomment and set a number to limit the batch size for testing\n",
       "    progress_interval=1\n",
       ")\n",
       "\n",
       "print(\"Batch processing section ready\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## <a id='conclusion'></a>9. Conclusion\n",
       "\n",
       "Congratulations! You have successfully walked through a comprehensive, end-to-end video annotation pipeline. You have learned how to:\n",
       "\n",
       "1.  Set up your Dataloop environment and configure a powerful Generative AI model.\n",
       "2.  Implement a detailed Python class to manage a multi-step video analysis process, including intelligent frame extraction, montage creation, and model interaction.\n",
       "3.  Test the pipeline on a single video and review the structured, multi-faceted annotations.\n",
       "4.  Scale the process using a batch function to annotate an entire dataset.\n",
       "\n",
       "### Next Steps\n",
       "\n",
       "From here, you can expand on this foundation:\n",
       "*   **Analyze the Results:** Review the generated annotations in the Dataloop platform. Use filters and queries to gain insights from your newly structured data.\n",
       "*   **Customize the Ontology:** Modify the `generate_structured_annotation` method to create a different annotation schema that better fits your specific use case.\n",
       "*   **Integrate into a Pipeline:** Convert this notebook into a Dataloop FaaS (Function as a Service) and build an automated pipeline that triggers this annotation process whenever a new video is uploaded to your dataset.\n",
       "*   **Experiment with Different Models:** Swap out the Gemini model for other VLM (Vision-Language Model) or specialized models available on the Dataloop Marketplace to compare performance and results."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }