{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# SAM2 (Segment Anything Model 2) Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide on how to use SAM2 (Segment Anything Model 2), a powerful image segmentation model by Meta AI, with the Dataloop platform and its Python SDK. We will cover the full workflow from setup to model training and evaluation.\n",
    "\n",
    "SAM2 represents a significant advancement in image segmentation, offering improved accuracy and efficiency for various computer vision tasks. This tutorial will help you leverage SAM2's capabilities within the Dataloop ecosystem for your segmentation projects.\n",
    "\n",
    "### Prerequisites:\n",
    "* **Dataloop Account:** You should have access to a Dataloop platform account.\n",
    "* **Python Environment:** Ensure you have Python 3.7+ installed with pip.\n",
    "* **Dataset Access:** A Dataloop project and dataset with images for segmentation tasks.\n",
    "* **GPU Resources:** Recommended for training and inference (though CPU is supported for basic operations).\n",
    "\n",
    "### Navigate through the following sections:\n",
    "1. [Dependencies & Setup](#dependencies-setup)\n",
    "2. [Environment Setup and Dataloop Connection](#environment-setup)\n",
    "3. [Project and Dataset Configuration](#project-dataset-config)\n",
    "4. [Load and Visualize Data](#load-visualize-data)\n",
    "5. [SAM2 Prediction](#sam2-prediction)\n",
    "6. [SAM2 Training](#sam2-training)\n",
    "7. [Model Evaluation](#model-evaluation)\n",
    "8. [Conclusion and Next Steps](#conclusion)\n",
    "\n",
    "For more details about SAM2 in the Dataloop platform, refer to the [Dataloop SAM Github repository](https://github.com/dataloop-ai-apps/grounded-sam-adapter/tree/main/adapters/sam_adapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependencies-setup",
   "metadata": {},
   "source": [
    "## <a id='dependencies-setup'></a>1. Dependencies & Setup\n",
    "\n",
    "First, let's ensure all required Python packages are installed. The cell below will install `dtlpy` for Dataloop SDK interaction, along with `matplotlib`, `numpy`, and `Pillow` for data handling and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtlpy matplotlib numpy Pillow --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## <a id='environment-setup'></a>2. Environment Setup and Dataloop Connection\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "With dependencies installed, we'll import the necessary libraries and establish a connection to the Dataloop platform. If your Dataloop token is expired or not found, you will be prompted to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-and-login",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import dtlpy as dl\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Login to the Dataloop platform\n",
    "# This will check if your current token is valid and prompt for login if necessary.\n",
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "print('Successfully logged into Dataloop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-dataset-config",
   "metadata": {},
   "source": [
    "## <a id='project-dataset-config'></a>3. Project and Dataset Configuration\n",
    "\n",
    "Now, we need to specify the Dataloop project and dataset you'll be working with.\n",
    "\n",
    "**Action Required:** In the code cell below, please replace `'your_project_name'` and `'your_dataset_name'` with the names of your Dataloop project and dataset, respectively. The dataset should contain images, which can be with or without existing annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-project-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the project\n",
    "# IMPORTANT: Modify 'your_project_name' to your specific Dataloop project name.\n",
    "project = dl.projects.get(project_name='your_project_name')\n",
    "print(f'Connected to project: {project.name}')\n",
    "\n",
    "# Get the dataset\n",
    "# IMPORTANT: Modify 'your_dataset_name' to your specific Dataloop dataset name.\n",
    "dataset = project.datasets.get(dataset_name='your_dataset_name')\n",
    "print(f'Connected to dataset: {dataset.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-visualize-data",
   "metadata": {},
   "source": [
    "## <a id='load-visualize-data'></a>4. Load and Visualize Data\n",
    "\n",
    "Let's select a random image from our dataset and visualize it. If the image has existing annotations, we'll display those as well. This helps in understanding the data we're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "select-random-item",
   "metadata": {},
   "source": [
    "### 4.1 Select Random Item\n",
    "\n",
    "We'll randomly select an item from the dataset to work with throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get-random-item",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random item from the dataset\n",
    "items_list = list(dataset.items.list().all)\n",
    "if items_list:\n",
    "    item = items_list[random.randint(0, len(items_list) - 1)]\n",
    "    print(f\"Selected random item: {item.name} (ID: {item.id})\") \n",
    "else:\n",
    "    print(\"No items found in the dataset!\")\n",
    "    item = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-function",
   "metadata": {},
   "source": [
    "### 4.2 Helper Function for Visualization\n",
    "\n",
    "This function will help us display images and their annotations in a consistent format throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_item_with_annotations(item):\n",
    "    \"\"\"\n",
    "    Display an image item and its annotations\n",
    "    \n",
    "    Args:\n",
    "        item: A Dataloop item object\n",
    "    \"\"\"\n",
    "    if not item:\n",
    "        print(\"No item to display\")\n",
    "        return\n",
    "        \n",
    "    # Download the image locally\n",
    "    buffer = item.download(save_locally=False)\n",
    "    \n",
    "    # Convert buffer to image\n",
    "    image = Image.open(io.BytesIO(buffer))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image: {item.name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Get annotations for the item\n",
    "    annotations = item.annotations.list()\n",
    "    if annotations.items:\n",
    "        print(f\"Found {len(annotations.items)} annotations\")\n",
    "        \n",
    "        # Display image with annotations\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        # Plot each annotation\n",
    "        for annotation in annotations.items:\n",
    "            if hasattr(annotation, 'coordinates'):\n",
    "                coords = np.array(annotation.coordinates)\n",
    "                if annotation.type == 'box':\n",
    "                    x, y, w, h = coords\n",
    "                    plt.gca().add_patch(plt.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=2))\n",
    "                    plt.text(x, y, annotation.label, color='white', backgroundcolor='red', fontsize=8)\n",
    "                elif annotation.type == 'polygon':\n",
    "                    plt.plot(coords[:, 0], coords[:, 1], 'r-', linewidth=2)\n",
    "                    plt.text(coords[0, 0], coords[0, 1], annotation.label, color='white', backgroundcolor='red', fontsize=8)\n",
    "                elif annotation.type == 'point':\n",
    "                    plt.scatter(coords[0], coords[1], c='red', s=50)\n",
    "                    plt.text(coords[0], coords[1], annotation.label, color='white', backgroundcolor='red', fontsize=8)\n",
    "                elif annotation.type == 'segmentation':\n",
    "                    # Assuming annotation.geo is a Dataloop geo object for segmentation\n",
    "                    # The geo_to_mask method requires image height and width\n",
    "                    mask = annotation.geo.geo_to_mask((image.height, image.width)) \n",
    "                    plt.imshow(mask, alpha=0.5, cmap='jet')\n",
    "                    \n",
    "        plt.axis('off')\n",
    "        plt.title(\"Image with Annotations\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No annotations found for this item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-selected-item",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Selected Item\n",
    "\n",
    "Display the randomly selected item and its annotations both in the notebook and optionally in the Dataloop platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-item-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the item and item annotations in the notebook\n",
    "display_item_with_annotations(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-item-web",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the item and item annotations in the platform\n",
    "# This will open the item in your web browser, in the Dataloop annotation studio.\n",
    "item.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sam2-prediction",
   "metadata": {},
   "source": [
    "## <a id='sam2-prediction'></a>5. SAM2 Prediction\n",
    "\n",
    "We will now use a pre-trained SAM2 model available on the Dataloop platform to perform image segmentation. SAM2 in Dataloop supports multiple prediction modes:\n",
    "\n",
    "### SAM2 Prediction Modes:\n",
    "\n",
    "1. **Zero-shot Segmentation (No Annotations Provided):**\n",
    "   - If no specific annotations (like boxes or points) are on the item, a bounding box encompassing the entire image is implicitly passed to the model.\n",
    "   - The model typically returns a binary mask for the most prominent object it identifies in the image.\n",
    "\n",
    "2. **Box-guided Segmentation:**\n",
    "   - Users can provide bounding box annotations around specific areas of interest.\n",
    "   - The model generates segmentation masks for objects within these specified bounding boxes.\n",
    "\n",
    "3. **Point-based Segmentation:**\n",
    "   - Users can input point annotations indicating specific locations inside different objects.\n",
    "   - The model returns a segmentation mask for each object that contains an indicated point.\n",
    "\n",
    "4. **Multi-points Segmentation (Advanced):**\n",
    "   - Users can provide multiple points, labeled as \"inside\" (positive) or \"outside\" (negative) the object of interest.\n",
    "   - The model uses these labeled points to generate more refined segmentation masks.\n",
    "   - This mode may require specific model configuration, such as `multi_points_prediction=true`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-configure-sam2",
   "metadata": {},
   "source": [
    "### 5.1 Install and Configure SAM2 Model\n",
    "\n",
    "First, we need to get the SAM2 DPK (Dataloop Package), install it as an app in our project (if not already installed), and then retrieve the associated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-sam2-app",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SAM2 DPK (Dataloop Package)\n",
    "sam2_dpk = dl.dpks.get(dpk_name='sam2')\n",
    "\n",
    "# Install the SAM2 app from the DPK if not already installed in the project\n",
    "try:\n",
    "    sam2_app = project.apps.install(app_name=sam2_dpk.display_name, dpk=sam2_dpk, custom_installation=sam2_dpk.to_json())\n",
    "except dl.exceptions.BadRequest as e: # Catch error if app is already installed\n",
    "    print(f\"App {sam2_dpk.display_name} already installed, getting existing app instance.\")\n",
    "    sam2_app = project.apps.get(app_name=sam2_dpk.display_name)\n",
    "\n",
    "# Get the SAM2 model associated with the installed app\n",
    "# Models are typically created when an app is installed or can be managed separately.\n",
    "filters = dl.Filters(resource=dl.FiltersResource.MODEL)\n",
    "filters.add(field='app.id', values=sam2_app.id)\n",
    "model = project.models.list(filters=filters)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy-configure-model",
   "metadata": {},
   "source": [
    "### 5.2 Deploy and Configure Model\n",
    "\n",
    "Ensure the model is deployed before making predictions. If the model is not deployed, the following cell will attempt to deploy it. The model's configuration is also displayed for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deploy-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nLoaded model: {model.name}')\n",
    "\n",
    "# Check model status and deploy if not already deployed\n",
    "if model.status != 'deployed':\n",
    "    print('Deploying model...')\n",
    "    model.deploy() # This deploys the model with default settings\n",
    "print(f'Model status: {model.status}')\n",
    "\n",
    "# Display model configuration for reference\n",
    "print('\\nModel configuration:')\n",
    "for key, value in model.configuration.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-prediction",
   "metadata": {},
   "source": [
    "### 5.3 Run Prediction\n",
    "\n",
    "Now, let's make a prediction on the randomly selected item. If the item has no annotations, SAM2 will perform zero-shot segmentation. If it has box or point annotations, those will guide the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "make-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "prediction = model.predict(item_ids=[item.id])\n",
    "prediction.wait() # Wait for the prediction process to complete\n",
    "print('Prediction completed successfully')\n",
    "\n",
    "# Get updated item with predictions\n",
    "# The item in the dataset will now have new annotations generated by the model.\n",
    "updated_item = dataset.items.get(item_id=item.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-prediction-results",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Prediction Results\n",
    "\n",
    "Let's visualize the item with the newly added prediction annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-predictions-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the item and item annotations (including predictions) in the notebook\n",
    "display_item_with_annotations(updated_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-predictions-web",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the item and item annotations (including predictions) in the platform\n",
    "updated_item.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-prompts",
   "metadata": {},
   "source": [
    "### 5.5 Experiment: Prediction with Prompts\n",
    "\n",
    "The predictions above show the model's segmentation results, which will typically be mask annotations. You can experiment with different prediction modes by:\n",
    "\n",
    "- Providing bounding boxes for box-guided segmentation (as we'll try next).\n",
    "- Using point-based segmentation by adding point annotations to an item before prediction.\n",
    "- Testing multi-point predictions with appropriately labeled points, if supported by your model configuration.\n",
    "\n",
    "Remember to check the [SAM2 model documentation](https://github.com/dataloop-ai-apps/grounded-sam-adapter/tree/main/adapters/sam_adapter) for detailed configuration options and best practices for different modes.\n",
    "\n",
    "#### Option 1: Adding a Bounding Box via the Dataloop UI\n",
    "\n",
    "1. Run the cell below to open the selected item in the Dataloop annotation studio.\n",
    "2. Use the bounding box tool in the UI to draw a box around an object of interest.\n",
    "3. Assign a label to the box (e.g., \"target_object\").\n",
    "4. Save the annotation.\n",
    "5. Once saved, continue to the cells below to run prediction on this modified item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "open-ui-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to open the item in the Dataloop UI for manual annotation\n",
    "# After adding and saving a bounding box in the UI, come back to this notebook.\n",
    "item.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "programmatic-bbox",
   "metadata": {},
   "source": [
    "#### Option 2: Adding a Bounding Box Programmatically\n",
    "\n",
    "Alternatively, use the Dataloop SDK to add a bounding box annotation to the item. Adjust the coordinates and label as needed. Note that the coordinates `(left, top, right, bottom)` are in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-bbox-programmatically",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and upload a bounding box annotation\n",
    "image_annotation = dl.AnnotationCollection()\n",
    "image_annotation.add(annotation_definition=dl.Box(left=float(100),  # left coordinate in pixels from the left edge of the image\n",
    "                                                top=float(100),    # top coordinate in pixels from the top edge of the image\n",
    "                                                right=float(200),  # right coordinate in pixels from the left edge of the image\n",
    "                                                bottom=float(200), # bottom coordinate in pixels from the top edge of the image\n",
    "                                                label='your_label' # Assign a meaningful label to your prompt box\n",
    "                                                ))\n",
    "item.annotations.upload(image_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-with-prompts",
   "metadata": {},
   "source": [
    "### 5.6 Run Prediction with Prompts\n",
    "\n",
    "Now, let's run a prediction on the item, which now includes a bounding box prompt (either added via UI or code). We first refetch the item to ensure we have the latest version with the prompt annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict-with-prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the item again to get any annotations added via UI or code\n",
    "annotated_item = dataset.items.get(item_id=item.id)\n",
    "\n",
    "# Make prediction using the item with the prompt\n",
    "prediction = model.predict(item_ids=[annotated_item.id])\n",
    "prediction.wait()\n",
    "print('Prediction completed successfully')\n",
    "\n",
    "# Get updated item with predictions based on the prompt\n",
    "updated_item = dataset.items.get(item_id=item.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-prompt-results",
   "metadata": {},
   "source": [
    "### 5.7 Visualize Prompt-Guided Results\n",
    "\n",
    "Visualize the result of the prompt-guided prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-prompt-results-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the item and its annotations (including new predictions) in the notebook\n",
    "display_item_with_annotations(updated_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-prompt-results-web",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the item and its annotations in the platform\n",
    "updated_item.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sam2-training",
   "metadata": {},
   "source": [
    "## <a id='sam2-training'></a>6. SAM2 Training\n",
    "\n",
    "To fine-tune SAM2 on your own dataset, we first need to clone the pre-trained model. This creates a new model artifact that you own and can modify. We will then associate our dataset with this cloned model for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clone-model",
   "metadata": {},
   "source": [
    "### 6.1 Clone Model for Training\n",
    "\n",
    "Clone the base SAM2 model to create a trainable version. It's good practice to give the cloned model a descriptive name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-sam2-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the base SAM2 model to create a trainable version\n",
    "# It's good practice to give the cloned model a descriptive name.\n",
    "clone_model = model.clone(model_name=f\"{model.name}-finetuned\", # Or provide a new name like f\"{model.name}-finetune\"\n",
    "                          dataset=dataset, # Associate the dataset for training context\n",
    "                          description='Cloned model for training')\n",
    "\n",
    "print('Model cloned successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configure-training-params",
   "metadata": {},
   "source": [
    "### 6.2 Configure Training Parameters\n",
    "\n",
    "We can inspect and modify the configuration of this cloned model. For example, training parameters like the number of epochs, batch size, or learning rate can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-clone-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the configuration of the cloned model\n",
    "print('\\nCloned model configuration:')\n",
    "for key, value in clone_model.configuration.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "update-training-config",
   "metadata": {},
   "source": [
    "### 6.3 Update Training Configuration\n",
    "\n",
    "For instance, let's say we want to change the number of training epochs. You can do this directly in the Dataloop platform UI by opening the model's page, or programmatically via the SDK.\n",
    "\n",
    "To change configuration in the UI, run the cell below to open the cloned model's page in your browser. Navigate to the configuration settings and make your edits (e.g., set 'num_epochs' or 'epochs' to 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "open-model-ui",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Open the cloned model in the Dataloop UI to modify configuration\n",
    "clone_model.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "programmatic-config-update",
   "metadata": {},
   "source": [
    "Alternatively, update the configuration using the SDK. The exact key for the number of epochs might vary (e.g., `num_epochs`, `epochs`). Check your model's specific configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "update-config-programmatically",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Modify configuration programmatically (e.g., number of epochs)\n",
    "# Make sure the key 'num_epochs' (or similar, like 'epochs') exists in your model's configuration.\n",
    "clone_model.configuration['num_epochs'] = 10 # Set desired number of epochs\n",
    "clone_model.update() # Save the changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-config-changes",
   "metadata": {},
   "source": [
    "### 6.4 Verify Configuration Changes\n",
    "\n",
    "Let's verify the configuration change by displaying it again. If you changed it via UI, re-fetch the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec39a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_model = dl.models.get(model_id=clone_model.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model configuration again to confirm changes\n",
    "print('\\nUpdated model configuration:')\n",
    "for key, value in clone_model.configuration.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare-data-subsets",
   "metadata": {},
   "source": [
    "### 6.5 Prepare Data Subsets for Training\n",
    "\n",
    "For effective model training, it's crucial to split your dataset into training, validation, and (optionally) test subsets. Dataloop provides tools to manage these splits. Here, we'll split our dataset items based on percentages. The items will be tagged accordingly (e.g., 'train', 'validation', 'test').\n",
    "\n",
    "For more information on dataset splitting and subsets, refer to the [Dataloop documentation on ML Subsets](https://developers.dataloop.ai/tutorials/data_management/datasets_and_versioning/chapter#ml-subsets-your-datasets-secret-sauce-)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subsets to the dataset\n",
    "# This operation tags items with 'train', 'validation', or 'test'.\n",
    "filters = dl.Filters(field='type', values='file') # Filter to select only 'file' type items (e.g., images)\n",
    "dataset.split_ml_subsets(\n",
    "    # Define the query to filter items (only files in this case)\n",
    "    items_query=filters,\n",
    "    # Split the dataset into train (60%), validation (20%), and test (20%) subsets\n",
    "    # These will be used for training, validating, and testing the model respectively\n",
    "    # Adjust percentages as needed for your dataset size and requirements.\n",
    "    percentages={'train': 60, 'validation': 20, 'test': 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configure-model-subsets",
   "metadata": {},
   "source": [
    "### 6.6 Configure Model Subsets and Start Training\n",
    "\n",
    "Next, we need to inform the cloned model which items belong to the training and validation sets. We do this by creating filters based on the subset tags (e.g., `metadata.system.tags.train`) and adding these subsets to the model. Then, we initiate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "start-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters to select items tagged as 'train' and 'validation'\n",
    "# These tags are automatically added by the split_ml_subsets method.\n",
    "train_filters = dl.Filters(field=\"metadata.system.tags.train\", values=True)\n",
    "validation_filters = dl.Filters(field=\"metadata.system.tags.validation\", values=True)\n",
    "\n",
    "# Add the train subset to the model, specifying the dataset and the filter\n",
    "# This tells the model which items to use for training\n",
    "clone_model.add_subset(subset_name=\"train\", subset_filter=train_filters) # dataset_id is taken from clone_model.dataset_id if not specified\n",
    "\n",
    "# Add the validation subset to the model\n",
    "# This tells the model which items to use for validation during training\n",
    "clone_model.add_subset(subset_name=\"validation\", subset_filter=validation_filters)\n",
    "\n",
    "# Update the model to save the subset configurations\n",
    "# The system_metadata=True parameter ensures system metadata (like subset definitions) is updated.\n",
    "clone_model.update(system_metadata=True)\n",
    "\n",
    "# Start the model training process\n",
    "# This will use the items defined in the 'train' and 'validation' subsets.\n",
    "# This returns an execution object that can be used to track training progress.\n",
    "execution = clone_model.train()\n",
    "\n",
    "# Print confirmation message\n",
    "print('Model training started successfully')\n",
    "print(f\"Training initiated. Execution ID: {execution.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-for-training",
   "metadata": {},
   "source": [
    "### 6.7 Wait for Training Completion\n",
    "\n",
    "Training can take a significant amount of time, depending on the dataset size, model complexity, and hardware resources. The `execution.wait()` call will block the notebook until training is finished or fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the training execution to complete\n",
    "execution.wait() \n",
    "print('Model training finished successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitor-training",
   "metadata": {},
   "source": [
    "### 6.8 Monitor Training Progress\n",
    "\n",
    "You can navigate to the 'Training' tab on the model's page in the Dataloop UI to view detailed progress, logs, and performance metrics as the training runs. The cell below will open this page for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "open-training-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the model page in the Dataloop platform to monitor training\n",
    "clone_model.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-evaluation",
   "metadata": {},
   "source": [
    "## <a id='model-evaluation'></a>7. Model Evaluation\n",
    "\n",
    "After the training process is complete, it's essential to evaluate the performance of your fine-tuned model. We'll use the 'test' subset of our data, which the model has not seen during training or validation, for this purpose. The evaluation process runs predictions on the test set and compares them to ground truth annotations to calculate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "start-evaluation",
   "metadata": {},
   "source": [
    "### 7.1 Start Model Evaluation\n",
    "\n",
    "Create filters to select items tagged as 'test' and start the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters to select items tagged as 'test'\n",
    "# This filter will identify all items in the dataset that have been tagged for testing.\n",
    "test_filters = dl.Filters(field='metadata.system.tags.test', values=True)\n",
    "\n",
    "# Start the model evaluation process on the test subset\n",
    "# - dataset_id: Specifies which dataset to use for evaluation (taken from clone_model.dataset_id by default if associated).\n",
    "# - filters: Applies the test filters to only evaluate on test items.\n",
    "execution = clone_model.evaluate(dataset_id=dataset.id, # Explicitly providing dataset.id\n",
    "                                 filters=test_filters)\n",
    "\n",
    "# Print confirmation message\n",
    "print('Model evaluation started successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-for-evaluation",
   "metadata": {},
   "source": [
    "### 7.2 Wait for Evaluation Completion\n",
    "\n",
    "Similar to training, evaluation can take some time. The `execution.wait()` call will block until it's complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the evaluation execution to complete\n",
    "execution.wait()\n",
    "print('Model evaluation finished successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view-evaluation-results",
   "metadata": {},
   "source": [
    "### 7.3 View Evaluation Results\n",
    "\n",
    "Once the evaluation is finished, you can access detailed performance metrics in the Dataloop platform. These are typically found under a 'Metrics' or 'Precision-Recall' tab on the model's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "open-evaluation-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_model.open_in_web()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## <a id='conclusion'></a>8. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully walked through the process of using SAM2 with Dataloop, including:\n",
    "\n",
    "### Summary of What You've Accomplished:\n",
    "- Set up your environment and connected to Dataloop\n",
    "- Configured your project and dataset for segmentation tasks\n",
    "- Visualized data and annotations using helper functions\n",
    "- Performed predictions with a pre-trained SAM2 model using various prompting techniques\n",
    "- Fine-tuned SAM2 on your custom dataset with proper data splitting\n",
    "- Evaluated the performance of your fine-tuned model on test data\n",
    "\n",
    "### Next Steps:\n",
    "- **Explore Advanced Configurations:** Dive deeper into the SAM2 model's configuration options for both prediction and training to optimize performance for your specific use case\n",
    "- **Iterate on Training:** Adjust training parameters (epochs, learning rate, batch size), dataset composition, and augmentation strategies to improve your fine-tuned model\n",
    "- **Deploy Your Trained Model:** Once satisfied with your fine-tuned model, deploy it as a service to use it for inference on new, unseen data within your Dataloop workflows or external applications\n",
    "- **Analyze Evaluation Metrics:** Thoroughly review the evaluation metrics in the Dataloop platform to understand your model's strengths and weaknesses\n",
    "- **Integration with Pipelines:** Incorporate your trained SAM2 model into automated annotation pipelines for production use\n",
    "\n",
    "This notebook provides a foundational guide. The Dataloop platform offers many more features for comprehensive AI data management and model development. We encourage you to explore the [Dataloop documentation](https://dataloop.ai/docs) for more advanced topics and functionalities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
