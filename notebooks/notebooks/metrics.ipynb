{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c835d7",
   "metadata": {},
   "source": [
    "# Annotation Consensus and IoU Metrics Tutorial\n",
    "\n",
    "This notebook provides a comprehensive guide on calculating consensus metrics and IoU (Intersection over Union) scores for annotations using the Dataloop platform and its Python SDK. It covers various annotation types including classifications, bounding boxes, polygons, and segmentation masks.\n",
    "\n",
    "Consensus metrics help evaluate the agreement between multiple annotators working on the same data, which is crucial for quality assurance and understanding annotation reliability.\n",
    "\n",
    "### Prerequisites:\n",
    "* **Dataloop Account:** You should have access to a Dataloop platform account.\n",
    "* **Python Environment:** Ensure you have Python 3.7+ installed with pip.\n",
    "* **Annotated Data:** Access to items with annotations from multiple annotators.\n",
    "\n",
    "### Navigate through the following sections:\n",
    "1. [Dependencies & Setup](#dependencies-setup)\n",
    "2. [Understanding IoU and Consensus](#understanding-iou)\n",
    "3. [Classification Consensus and Majority Vote](#classification-consensus)\n",
    "4. [Bounding Box IoU Matching](#box-iou-matching)\n",
    "5. [Polygon and Segmentation IoU](#polygon-segmentation-iou)\n",
    "6. [Multi-Annotator Comparison](#multi-annotator-comparison)\n",
    "7. [Conclusion and Next Steps](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## <a id='dependencies-setup'></a>1. Dependencies & Setup\n",
    "\n",
    "First, let's ensure all required Python packages are installed and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtlpy matplotlib seaborn --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-section",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "Now, we import all the Python libraries that will be used throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "from dtlpy.ml import metrics, predictions_utils\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-iou",
   "metadata": {},
   "source": [
    "## <a id='understanding-iou'></a>2. Understanding IoU and Consensus\n",
    "\n",
    "### What is IoU (Intersection over Union)?\n",
    "\n",
    "IoU is a metric used to evaluate the accuracy of object detection and segmentation models by measuring the overlap between predicted and ground truth annotations.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://storage.googleapis.com/kaggle-media/competitions/rsna/IoU.jpg\" width=\"350\" title=\"IoU\">\n",
    "</p>\n",
    "\n",
    "**Consensus** in Dataloop supports the following annotation types:\n",
    "- Classification (Label IoU)\n",
    "- Bounding Box (IoU)\n",
    "- Polygon (IoU)\n",
    "- Semantic Segmentation (IoU)\n",
    "- Point (distance scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-consensus",
   "metadata": {},
   "source": [
    "## <a id='classification-consensus'></a>3. Classification Consensus and Majority Vote\n",
    "\n",
    "### 3.1 Load Sample Data\n",
    "\n",
    "We'll demonstrate consensus calculation using 5 annotators working on the same items. First, let's retrieve the annotated items and their annotations.\n",
    "\n",
    "**Note:** Replace the item IDs below with your own annotated items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-classification-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve items with annotations from multiple annotators\n",
    "first_item = dl.items.get(item_id='6215d3f73750a54742c4d33d')\n",
    "second_item = dl.items.get(item_id='6215d3fee2b78c63e4ae501f')\n",
    "third_item = dl.items.get(item_id='6215d4053750a5fe47c4d343')\n",
    "fourth_item = dl.items.get(item_id='6215d40ce2b78c3b85ae5022')\n",
    "fifth_item = dl.items.get(item_id='6215d415e2b78c36b7ae5028')\n",
    "\n",
    "# Get annotations for each item\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()\n",
    "third_annotations = third_item.annotations.list()\n",
    "fourth_annotations = fourth_item.annotations.list()\n",
    "fifth_annotations = fifth_item.annotations.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examine-labels",
   "metadata": {},
   "source": [
    "### 3.2 Examine Annotator Labels\n",
    "\n",
    "Let's see which labels each annotator assigned to their respective items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{first_item.name} annotations: {[annotation.label for annotation in first_annotations]}')\n",
    "print(f'{second_item.name} annotations: {[annotation.label for annotation in second_annotations]}')\n",
    "print(f'{third_item.name} annotations: {[annotation.label for annotation in third_annotations]}')\n",
    "print(f'{fourth_item.name} annotations: {[annotation.label for annotation in fourth_annotations]}')\n",
    "print(f'{fifth_item.name} annotations: {[annotation.label for annotation in fifth_annotations]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "calculate-consensus",
   "metadata": {},
   "source": [
    "### 3.3 Calculate Consensus Matrix\n",
    "\n",
    "To create the annotators' scoring matrix, we calculate IoU between each pair of annotators and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consensus-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_list = [first_item, second_item, third_item, fourth_item, fifth_item]\n",
    "n_annotators = len(items_list)\n",
    "items_scores = np.zeros((n_annotators, n_annotators))\n",
    "\n",
    "for i_item in range(n_annotators):\n",
    "    for j_item in range(n_annotators):\n",
    "        # Note: the results matrix is symmetric so calculation can be done only on one side of the diagonal\n",
    "        # We do both sides to show that the score is the same: measure_item(x, y) == measure_item(y, x)\n",
    "        success, results = predictions_utils.measure_item(items_list[i_item], items_list[j_item], ignore_labels=False)\n",
    "        items_scores[i_item, j_item] = results['total_mean_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-results",
   "metadata": {},
   "source": [
    "### 3.4 Examine Detailed Results\n",
    "\n",
    "The returned Result object contains a pandas DataFrame with all matching details and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-detailed-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item, ignore_labels=False)\n",
    "results[dl.AnnotationType.CLASSIFICATION].to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-consensus",
   "metadata": {},
   "source": [
    "### 3.5 Visualize Consensus Matrix\n",
    "\n",
    "We'll create a heatmap to visualize the consensus scores between annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(items_scores, \n",
    "            annot=True, \n",
    "            cmap='Blues',\n",
    "            xticklabels=['Annotator A','Annotator B','Annotator C', 'Annotator D', 'Annotator E'],\n",
    "            yticklabels=['Annotator A','Annotator B','Annotator C', 'Annotator D', 'Annotator E'])\n",
    "plt.title('Annotator Consensus Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "majority-vote",
   "metadata": {},
   "source": [
    "### 3.6 Calculate Majority Vote\n",
    "\n",
    "Count the appearances of each label across all annotators to determine majority consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each label\n",
    "all_annotations = [first_annotations, second_annotations, third_annotations, fourth_annotations, fifth_annotations]\n",
    "all_labels = [annotation.label for annotations in all_annotations for annotation in annotations]\n",
    "counter = Counter(all_labels)\n",
    "\n",
    "print(\"Label frequency across all annotators:\")\n",
    "for label, count in counter.items():\n",
    "    print(f'{label}: {count}')\n",
    "\n",
    "# Identify majority labels (3 or more annotators)\n",
    "majority_threshold = 3\n",
    "majority_labels = [label for label, count in counter.items() if count >= majority_threshold]\n",
    "print(f\"\\nMajority labels (â‰¥{majority_threshold} annotators): {majority_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "box-iou-matching",
   "metadata": {},
   "source": [
    "## <a id='box-iou-matching'></a>4. Bounding Box IoU Matching\n",
    "\n",
    "### 4.1 Load Bounding Box Data\n",
    "\n",
    "Box matching follows the same principles as classification consensus. Let's retrieve items with bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-box-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = dl.items.get(item_id='6214bc0d3750a50f50c44841')\n",
    "second_item = dl.items.get(item_id='6214be90fed92a9f043ba217')\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-box-annotations",
   "metadata": {},
   "source": [
    "### 4.2 Visualize Annotations\n",
    "\n",
    "Display the annotations for each item to understand the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-box-annotations",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.title('First Annotator')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(second_annotations.show())\n",
    "plt.title('Second Annotator')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.imshow(second_annotations.show(), alpha=0.7)\n",
    "plt.title('Overlay Comparison')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iou-with-labels",
   "metadata": {},
   "source": [
    "### 4.3 Calculate IoU with Label Matching\n",
    "\n",
    "Run the comparison with label matching enabled (annotations must have the same label to be considered a match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "box-iou-with-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item, ignore_labels=False)\n",
    "print(\"Results with label matching:\")\n",
    "print(results[dl.AnnotationType.BOX].to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iou-ignore-labels",
   "metadata": {},
   "source": [
    "### 4.4 Calculate IoU Ignoring Labels\n",
    "\n",
    "Run the same comparison but ignore labels (any overlapping boxes are considered potential matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "box-iou-ignore-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item, ignore_labels=True)\n",
    "print(\"Results ignoring labels:\")\n",
    "print(results[dl.AnnotationType.BOX].to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-matching",
   "metadata": {},
   "source": [
    "### 4.5 Examine Detailed Matching Results\n",
    "\n",
    "View the detailed annotation comparison matrix and individual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-detailed-matching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed matching matrix\n",
    "print(\"Detailed annotation comparison matrix:\")\n",
    "print(results['box'].matches._annotations_raw_df[0])\n",
    "\n",
    "# Show individual annotation scores and overall mean\n",
    "print(\"\\nIndividual annotation scores:\")\n",
    "print(results['box'].to_df()['annotation_score'])\n",
    "print(f\"\\nTotal mean score: {results['total_mean_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polygon-segmentation-iou",
   "metadata": {},
   "source": [
    "## <a id='polygon-segmentation-iou'></a>5. Polygon and Segmentation IoU\n",
    "\n",
    "### 5.1 Load Segmentation Data\n",
    "\n",
    "Similar to previous examples, we'll calculate IoU scores for polygon and segmentation annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-segmentation-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = dl.items.get(item_id='6214d07599cb175c9cd73d8f')\n",
    "second_item = dl.items.get(item_id='6214d07c9d80b05b8310ba9b')\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-segmentation",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Segmentation Annotations\n",
    "\n",
    "Display the segmentation masks for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-segmentation-annotations",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.title('First Annotator')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(second_annotations.show())\n",
    "plt.title('Second Annotator')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.imshow(second_annotations.show(), alpha=0.7)\n",
    "plt.title('Overlay Comparison')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "segmentation-iou-calc",
   "metadata": {},
   "source": [
    "### 5.3 Calculate Segmentation IoU\n",
    "\n",
    "Calculate IoU scores for segmentation annotations with a match threshold of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-segmentation-iou",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item, ignore_labels=True, match_threshold=0)\n",
    "print(\"Segmentation IoU results:\")\n",
    "print(results[dl.AnnotationType.SEGMENTATION].to_df())\n",
    "\n",
    "# View detailed matching information\n",
    "print(\"\\nDetailed segmentation matching:\")\n",
    "print(results[dl.AnnotationType.SEGMENTATION].matches._annotations_raw_df[0])\n",
    "\n",
    "print(f\"\\nTotal segmentation score: {results['total_mean_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-annotator-comparison",
   "metadata": {},
   "source": [
    "## <a id='multi-annotator-comparison'></a>6. Multi-Annotator Comparison\n",
    "\n",
    "### 6.1 Load Multi-Annotator Data\n",
    "\n",
    "For comprehensive consensus analysis, let's examine annotations from three different annotators on the same content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-multi-annotator-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = dl.items.get(item_id='6214ea1d0ec695cd9c35dfbd')\n",
    "second_item = dl.items.get(item_id='6214ea29e2b78c7ca1adc6b7')\n",
    "third_item = dl.items.get(item_id='6214ea310ec695600635dfc6')\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()\n",
    "third_annotations = third_item.annotations.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-all-annotators",
   "metadata": {},
   "source": [
    "### 6.2 Visualize All Annotators\n",
    "\n",
    "Display annotations from all three annotators with different visual properties for distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-all-annotators",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.title('Annotator A')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(second_annotations.show())\n",
    "plt.title('Annotator B')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(third_annotations.show())\n",
    "plt.title('Annotator C')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(first_annotations.show(thickness=20))\n",
    "plt.imshow(second_annotations.show(thickness=10))\n",
    "plt.imshow(third_annotations.show(thickness=3))\n",
    "plt.title('All Overlaid (Different Thickness)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pairwise-consensus",
   "metadata": {},
   "source": [
    "### 6.3 Calculate Pairwise Consensus Matrix\n",
    "\n",
    "Calculate consensus scores between all pairs of annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-pairwise-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [first_item, second_item, third_item]\n",
    "n_annotators = len(items)\n",
    "items_scores = np.zeros((n_annotators, n_annotators))\n",
    "\n",
    "for i_item in range(n_annotators):\n",
    "    for j_item in range(i_item, n_annotators):\n",
    "        success, results = predictions_utils.measure_item(items[i_item], items[j_item], ignore_labels=True)\n",
    "        items_scores[i_item, j_item] = results['total_mean_score']      \n",
    "        items_scores[j_item, i_item] = results['total_mean_score']  # Symmetric matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-multi-consensus",
   "metadata": {},
   "source": [
    "### 6.4 Visualize Multi-Annotator Consensus\n",
    "\n",
    "Create a heatmap showing consensus scores between all three annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-multi-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(items_scores, \n",
    "            annot=True, \n",
    "            cmap='Blues',\n",
    "            xticklabels=['Annotator A','Annotator B','Annotator C'],\n",
    "            yticklabels=['Annotator A','Annotator B','Annotator C'])\n",
    "plt.title('Three-Annotator Consensus Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## <a id='conclusion'></a>7. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully walked through the process of calculating annotation consensus and IoU metrics using the Dataloop platform.\n",
    "\n",
    "### Summary of What You've Accomplished:\n",
    "- Understood IoU metrics and their importance in annotation quality assessment\n",
    "- Calculated classification consensus and majority voting across multiple annotators\n",
    "- Analyzed bounding box IoU matching with and without label constraints\n",
    "- Evaluated polygon and segmentation annotation agreement\n",
    "- Performed comprehensive multi-annotator consensus analysis\n",
    "- Visualized consensus results using heatmaps and overlay comparisons\n",
    "\n",
    "### Next Steps:\n",
    "- **Quality Assurance Workflows:** Use these metrics to identify items requiring re-annotation or consensus resolution\n",
    "- **Annotator Performance Analysis:** Track individual annotator consistency and accuracy over time\n",
    "- **Active Learning:** Leverage consensus scores to prioritize items for additional annotation\n",
    "- **Model Evaluation:** Apply similar IoU calculations to evaluate model predictions against ground truth\n",
    "- **Automated Quality Control:** Integrate consensus calculations into your annotation pipelines for real-time quality monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
