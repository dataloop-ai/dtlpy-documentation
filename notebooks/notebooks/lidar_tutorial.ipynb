{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "main-title",
      "metadata": {},
      "source": [
       "# LiDAR Data Processing Tutorial\n",
       "\n",
       "This notebook provides a comprehensive guide on preparing and managing LiDAR data on the Dataloop platform using its Python SDK. LiDAR (Light Detection and Ranging) data processing is essential for autonomous vehicles, robotics, and 3D mapping applications.\n",
       "\n",
       "You'll learn how to structure LiDAR data, upload it to Dataloop, construct LiDAR video items, and add 3D annotations for comprehensive spatial analysis.\n",
       "\n",
       "### Prerequisites:\n",
       "* **Dataloop Account:** You should have access to a Dataloop platform account.\n",
       "* **Python Environment:** Ensure you have Python 3.7+ installed with pip.\n",
       "* **LiDAR Data:** Access to LiDAR data files including PCD files, camera images, and calibration data.\n",
       "* **Local Data Structure:** Your LiDAR data should be organized as described in this tutorial.\n",
       "\n",
       "### Navigate through the following sections:\n",
       "1. [Dependencies & Setup](#dependencies-setup)\n",
       "2. [Environment Setup](#environment-setup)\n",
       "3. [LiDAR Data Preparation](#data-preparation)\n",
       "4. [Construct LiDAR Video Item](#construct-lidar-video)\n",
       "5. [Upload Annotations](#upload-annotations)\n",
       "6. [Conclusion and Next Steps](#conclusion)\n",
       "\n",
       "For more detailed information, refer to the official Dataloop documentation:\n",
       "- [LiDAR Data Setup](https://docs.dataloop.ai/docs/lidar-data-setup)\n",
       "- [LiDAR Video Annotations](https://developers.dataloop.ai/tutorials/annotations/lidar/chapter)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "dependencies-setup",
      "metadata": {},
      "source": [
       "## <a id='dependencies-setup'></a>1. Dependencies & Setup\n",
       "\n",
       "First, let's ensure all required Python packages are installed. The cell below will install `dtlpy` for Dataloop SDK interaction and `dtlpylidar` for LiDAR-specific functionalities. It is recommended to run this to ensure you have the latest compatible versions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-packages",
      "metadata": {},
      "outputs": [],
      "source": [
       "!pip install dtlpy git+https://github.com/dataloop-ai-apps/dtlpy-lidar.git --upgrade --quiet"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "environment-setup",
      "metadata": {},
      "source": [
       "## <a id='environment-setup'></a>2. Environment Setup\n",
       "\n",
       "### Import Required Libraries\n",
       "\n",
       "With the dependencies installed, we will now import the necessary libraries for this tutorial."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "import-libraries",
      "metadata": {},
      "outputs": [],
      "source": [
       "import dtlpy as dl\n",
       "from dtlpylidar.parsers.base_parser import LidarFileMappingParser"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "connect-dataloop",
      "metadata": {},
      "source": [
       "### Connect to Dataloop Platform\n",
       "\n",
       "To begin, we need to connect to the Dataloop platform. If you're not already logged in, running the cell below will prompt you to do so. We will then create a new project or retrieve an existing one to work in.\n",
       "\n",
       "> **Action Required:** In the cell below, replace `\"your-lidar-project-name\"` with your Dataloop project name. If a project with this name already exists, it will be retrieved; otherwise, a new one will be created."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-project",
      "metadata": {},
      "outputs": [],
      "source": [
       "if dl.token_expired():\n",
       "    dl.login()\n",
       "\n",
       "PROJECT_NAME = \"your-lidar-project-name\"\n",
       "# Check if the project exists, if not, create it\n",
       "try:\n",
       "    project = dl.projects.get(project_name=PROJECT_NAME)\n",
       "    print(f\"Successfully retrieved project: '{project.name}' (ID: {project.id})\")\n",
       "except dl.exceptions.NotFound:\n",
       "    project = dl.projects.create(project_name=PROJECT_NAME)\n",
       "    print(f\"Successfully created project: '{project.name}' (ID: {project.id})\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "data-preparation",
      "metadata": {},
      "source": [
       "## <a id='data-preparation'></a>3. LiDAR Data Preparation\n",
       "\n",
       "To construct a LiDAR video item, your dataset must contain three key components: PCD files for 3D scenes, camera images for 2D views, and a `mapping.json` file for sensor calibration. This section explains how to structure these files locally before uploading them to Dataloop."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "local-file-structure",
      "metadata": {},
      "source": [
       "### 3.1 Local File Structure\n",
       "\n",
       "Before uploading to the Dataloop platform, your LiDAR data must be arranged in a specific local directory structure. This ensures that all components (point clouds, images) are correctly associated.\n",
       "\n",
       "#### Required Directory Structure:\n",
       "\n",
       "1. **`lidar` folder:** Contains all PCD files, named numerically in sequence (e.g., `0.pcd`, `1.pcd`, ...).\n",
       "\n",
       "2. **`frames` folder:** Contains subfolders for each frame's camera images.\n",
       "   - Subfolders must be named numerically to match the PCD files (e.g., `0`, `1`, ...).\n",
       "   - Image files within each subfolder must be named sequentially (e.g., `0.jpg`, `1.jpg`, ...).\n",
       "\n",
       "3. **`mapping.json` file:** This file, located at the root of your data directory, contains the calibration data that links the 3D point clouds to the 2D images."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "mapping-json-schema",
      "metadata": {},
      "source": [
       "### 3.2 Mapping.json Schema\n",
       "\n",
       "The `mapping.json` file must follow a specific schema. Below is a template demonstrating the required structure, including paths, timestamps, and camera calibration parameters (intrinsics and extrinsics).\n",
       "\n",
       "```json\n",
       "{\n",
       "    \"frames\": {\n",
       "        \"0\": {\n",
       "            \"path\": \"lidar/0.pcd\", // Relative path from the mapping.json file\n",
       "            \"timestamp\": 1234567890,\n",
       "            \"position\": { // LiDAR sensor location (used as the center of the world)\n",
       "                \"x\": 0.0,\n",
       "                \"y\": 0.0,\n",
       "                \"z\": 0.0\n",
       "            },\n",
       "            \"heading\": { // LiDAR sensor rotation (Quaternion)\n",
       "                \"x\": 0.0,\n",
       "                \"y\": 0.0,\n",
       "                \"z\": 0.0,\n",
       "                \"w\": 1.0\n",
       "            },\n",
       "            \"images\": { // if no images are provided, add an empty dict\n",
       "                \"0\": {\n",
       "                    \"image_path\": \"frames/0/0.jpg\", // Relative path from the mapping.json file\n",
       "                    \"timestamp\": 1234567890,\n",
       "                    \"intrinsics\": { // camera intrinsic parameters\n",
       "                        \"fx\": 525.0, // Focal length in pixels\n",
       "                        \"fy\": 525.0,\n",
       "                        \"cx\": 319.5, // Optical center (the principal point), in pixels\n",
       "                        \"cy\": 239.5\n",
       "                    },\n",
       "                    \"extrinsics\": { // camera extrinsic parameters\n",
       "                        \"translation\": { // camera location in world coordinates (in relation to the lidar sensor)\n",
       "                            \"x\": 0.0,\n",
       "                            \"y\": 0.0,\n",
       "                            \"z\": 0.0\n",
       "                        },\n",
       "                        \"rotation\": { // rotation of the camera (Quaternion)\n",
       "                            \"w\": 1.0,\n",
       "                            \"x\": 0.0,\n",
       "                            \"y\": 0.0,\n",
       "                            \"z\": 0.0\n",
       "                        }\n",
       "                    },\n",
       "                    \"distortion\": { // distortion parameters\n",
       "                        \"k1\": 0.0,\n",
       "                        \"k2\": 0.0,\n",
       "                        \"p1\": 0.0,\n",
       "                        \"p2\": 0.0,\n",
       "                        \"k3\": 0.0,\n",
       "                        \"k4\": 0.0\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "upload-data",
      "metadata": {},
      "source": [
       "### 3.3 Upload Data to Dataloop\n",
       "\n",
       "You can use your own data structured as described above, or use sample data from [OSDaR23](https://data.fid-move.de/dataset/osdar23) for this tutorial.\n",
       "\n",
       "> **Action Required:** In the cell below, update `DATASET_NAME` with your desired dataset name and `DATA_PATH` with the local path to your data directory."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
       "DATASET_NAME = \"My-LiDAR-Dataset\"\n",
       "# Check if the dataset exists within the project\n",
       "# if not, create it. Dataloop automatically creates a default Recipe and Ontology.\n",
       "try:\n",
       "    dataset = project.datasets.get(dataset_name=DATASET_NAME)\n",
       "    print(f\"Successfully retrieved dataset: '{dataset.name}' (ID: {dataset.id})\")\n",
       "except dl.exceptions.NotFound:\n",
       "    dataset = project.datasets.create(dataset_name=DATASET_NAME)\n",
       "    print(f\"Successfully created dataset: '{dataset.name}' (ID: {dataset.id}) in project '{project.name}'.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "upload-items",
      "metadata": {},
      "source": [
       "### 3.4 Upload Items to Dataset\n",
       "\n",
       "Now that we created or retrieved the dataset, we can upload items to it."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "upload-data-files",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Upload the data to the dataset\n",
       "DATA_PATH = \"./data/*\"\n",
       "dataset.items.upload(local_path=DATA_PATH)\n",
       "\n",
       "print(f\"Data uploaded successfully to dataset '{dataset.name}'\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "view-dataset",
      "metadata": {},
      "source": [
       "### 3.5 View Dataset in Platform\n",
       "\n",
       "We can take a look at the items in the platform using the following command:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "open-dataset-web",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Open the dataset in the Dataloop web interface\n",
       "dataset.open_in_web()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "construct-lidar-video",
      "metadata": {},
      "source": [
       "## <a id='construct-lidar-video'></a>4. Construct LiDAR Video Item\n",
       "\n",
       "After uploading your raw data files (`.pcd`, `.jpg`, `mapping.json`), we use the Dataloop LiDAR SDK to create a single, composite LiDAR video item. The `LidarFileMappingParser` reads the `mapping.json` file and generates a `frames.json` item, which represents the synchronized 3D video sequence in the Dataloop studio."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "parse-mapping-file",
      "metadata": {},
      "source": [
       "### 4.1 Parse Mapping File\n",
       "\n",
       "Use the LiDAR parser to process the mapping file and create the LiDAR video item."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-lidar-video",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get the mapping item using filters\n",
       "filters = dl.Filters(field=dl.FiltersKnownFields.NAME, values=\"mapping.json\")\n",
       "mapping_item = dataset.items.get_all_items(filters=filters)[0]\n",
       "frames_item = LidarFileMappingParser().parse_data(mapping_item=mapping_item)\n",
       "print(f\"LiDAR Video File: ItemID: {frames_item.id}, ItemName: {frames_item.name}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "upload-annotations",
      "metadata": {},
      "source": [
       "## <a id='upload-annotations'></a>5. Upload Annotations\n",
       "\n",
       "With the `frames.json` video item created, you can now add annotations. The process is similar to annotating standard videos in Dataloop, but with support for 3D annotation types like cuboids. First, we need to retrieve the `frames.json` item we just created."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "get-frames-item",
      "metadata": {},
      "source": [
       "### 5.1 Retrieve Frames Item\n",
       "\n",
       "Get the created frames item for annotation."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "get-frames-item-code",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Get the created frames item using filters\n",
       "filters = dl.Filters(field=dl.FiltersKnownFields.NAME, values=\"frames.json\")\n",
       "frames_item = dataset.items.get_all_items(filters=filters)[0]\n",
       "print(f\"Retrieved frames item: {frames_item.name} (ID: {frames_item.id})\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "upload-cuboid",
      "metadata": {},
      "source": [
       "### 5.2 Define and Upload a 3D Cuboid\n",
       "\n",
       "#### Define the Annotation\n",
       "\n",
       "To create a 3D cuboid annotation, we define its geometric properties and any additional metadata. The key parameters are:\n",
       "\n",
       "- **Position:** The 3D coordinates of the cuboid's center (x, y, z).\n",
       "- **Rotation:** Euler angles for the cuboid's orientation (roll, pitch, yaw).\n",
       "- **Scale:** The dimensions of the cuboid (width, height, depth).\n",
       "\n",
       "The code below defines a `dl.Cube3d` annotation for a \"person\"."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "define-3d-annotation",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define Your 3D Annotation Parameters\n",
       "label = \"person\"\n",
       "position = [\n",
       "    73.9,  # x coordinate\n",
       "    -5.9,  # y coordinate\n",
       "    0.79   # z coordinate\n",
       "]\n",
       "rotation = [\n",
       "    0.0,   # roll\n",
       "    0.0,   # pitch\n",
       "    0.0    # yaw\n",
       "]\n",
       "scale = [\n",
       "    1.2,   # width\n",
       "    0.8,   # height\n",
       "    1.8    # depth\n",
       "]\n",
       "\n",
       "# Add Some Extra Details\n",
       "attributes = {\"age\": \"adult\"}\n",
       "description = \"An adult person standing on the station\"\n",
       "\n",
       "# Create the 3D Cuboid Annotation\n",
       "annotation_definition = dl.Cube3d(\n",
       "    label=label,\n",
       "    position=position,\n",
       "    scale=scale,\n",
       "    rotation=rotation,\n",
       "    attributes=attributes,\n",
       "    description=description\n",
       ")\n",
       "\n",
       "print(f\"Created 3D cuboid annotation for label '{label}'\")\n",
       "print(f\"Position: {position}\")\n",
       "print(f\"Scale: {scale}\")\n",
       "print(f\"Rotation: {rotation}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "upload-annotation",
      "metadata": {},
      "source": [
       "#### Upload the Annotation\n",
       "\n",
       "Now that the `dl.Cube3d` object is defined, we use an annotation `builder` to add it to the `frames.json` item. We specify the frame range (`frame_num` to `end_frame_num`) where the annotation should appear and then upload it to the platform."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "upload-3d-annotation",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create annotation builder for the frames item\n",
       "builder = frames_item.annotations.builder()\n",
       "\n",
       "# Define annotation parameters\n",
       "frame_num = 0\n",
       "end_frame_num = 1\n",
       "object_id = \"0\"\n",
       "metadata = {\"user\": {\"isDistracted\": True}}\n",
       "\n",
       "# Add the annotation to the builder\n",
       "builder.add(\n",
       "    annotation_definition=annotation_definition,\n",
       "    frame_num=frame_num,\n",
       "    end_frame_num=end_frame_num,\n",
       "    object_id=object_id,\n",
       "    metadata=metadata\n",
       ")\n",
       "\n",
       "# Upload the annotations\n",
       "builder.upload()\n",
       "print(f\"Successfully uploaded 3D cuboid annotation to frames {frame_num}-{end_frame_num}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "view-in-studio",
      "metadata": {},
      "source": [
       "### 5.3 View in Dataloop Studio\n",
       "\n",
       "Once the annotation is uploaded, you can open the LiDAR video item in the Dataloop platform to view it in the LiDAR Studio. The following command will open the item directly in your web browser."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "open-lidar-studio",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Open the LiDAR video item in the Dataloop LiDAR Studio\n",
       "frames_item.open_in_web()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
       "## <a id='conclusion'></a>6. Conclusion and Next Steps\n",
       "\n",
       "Congratulations! You have successfully walked through the entire process of preparing a LiDAR dataset for the Dataloop platform.\n",
       "\n",
       "### Summary of What You've Accomplished:\n",
       "- Set up your Dataloop environment and project for LiDAR data processing\n",
       "- Learned the required file structure for LiDAR data (PCDs, images, and mapping file)\n",
       "- Understood the `mapping.json` schema for sensor calibration and synchronization\n",
       "- Uploaded and structured raw LiDAR data files to Dataloop\n",
       "- Constructed a composite LiDAR video item using the Dataloop LiDAR SDK\n",
       "- Created and uploaded 3D cuboid annotations to the video item\n",
       "- Visualized the results in the Dataloop LiDAR Studio\n",
       "\n",
       "### Next Steps:\n",
       "From here, you can explore more advanced functionalities:\n",
       "- **Advanced Annotations:** Experiment with different 3D annotation types (polylines, polygons, points)\n",
       "- **Custom LiDAR Parser:** Try constructing a LiDAR video item using a [Custom LiDAR Parser](https://developers.dataloop.ai/tutorials/data_management/items_and_annotations/other_data_types/lidar/chapter#using-custom-lidar-parser) for data that doesn't fit the standard structure\n",
       "- **Automated Pipelines:** Create automated pipelines for LiDAR data processing and annotation workflows\n",
       "- **Machine Learning Integration:** Integrate LiDAR workflows with machine learning models for autonomous systems and 3D object detection\n",
       "- **Multi-Modal Analysis:** Combine LiDAR data with other sensor modalities for comprehensive scene understanding\n",
       "- **Quality Assurance:** Set up QA workflows for 3D annotation validation and consensus\n",
       "\n",
       "For more advanced LiDAR processing capabilities, explore the full [Dataloop LiDAR SDK documentation](https://github.com/dataloop-ai-apps/dtlpy-lidar) and the [LiDAR tutorials](https://developers.dataloop.ai/tutorials/data_management/items_and_annotations/other_data_types/lidar/chapter) in the Dataloop developer documentation."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }