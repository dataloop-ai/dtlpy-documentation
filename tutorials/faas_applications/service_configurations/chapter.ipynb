{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \u2699\ufe0f Service Configurations - Mastering FaaS Settings\n",
    "\n",
    "Welcome to the world of FaaS service configurations! Think of service configurations as the control panel for your serverless functions. Let's explore how to fine-tune your services for optimal performance!\n",
    "\n",
    "## \ud83c\udfaf Managing Service Configurations\n",
    "\n",
    "You can configure your services in two ways: through the DPK manifest file or directly on the deployed service.\n",
    "\n",
    "### DPK Manifest Configuration\n",
    "\n",
    "Define your service settings in the `dataloop.json` manifest:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n{\n  \"name\": \"my-package\",\n  \"components\": {\n    \"services\": [\n      {\n        \"name\": \"my-service\",\n        \"runtime\": {\n          \"podType\": \"regular-s\",\n          \"concurrency\": 10,\n          \"runnerImage\": \"python:3.9\",\n          \"autoscaler\": {\n            \"minReplicas\": 0,\n            \"maxReplicas\": 5,\n            \"queueLength\": 10,\n            \"cooldownPeriod\": 300\n          }\n        },\n        \"executionTimeout\": 3600,\n        \"initParams\": {\n          \"model_name\": \"resnet50\"\n        }\n      }\n    ]\n  }\n}\n```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Direct Service Updates\n",
    "\n",
    "Modify configurations of deployed services:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an existing service\n",
    "project = dl.projects.get('project-name')\n",
    "service = project.services.get('service-name')\n",
    "\n",
    "# Update runtime configuration\n",
    "service.runtime.concurrency = 5\n",
    "service.runtime.pod_type = dl.InstanceCatalog.REGULAR_M\n",
    "service.update()\n",
    "\n",
    "# Update autoscaler settings\n",
    "service.runtime.autoscaler.min_replicas = 1\n",
    "service.runtime.autoscaler.max_replicas = 10\n",
    "service.runtime.autoscaler.queue_length = 20\n",
    "service.update()\n",
    "\n",
    "# Update execution timeout\n",
    "service.execution_timeout = 7200  # 2 hours\n",
    "service.update()\n",
    "\n",
    "# Update service state\n",
    "service.pause()  # Pause the service\n",
    "service.resume()  # Resume the service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## \ud83d\udd27 Advanced Runtime Settings\n",
    "\n",
    "### Custom Resource Allocation\n",
    "\n",
    "Need more power? Configure your compute resources:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = dl.services.get('service-name')\n",
    "service.runtime = dl.KubernetesRuntime(\n",
    "        pod_type=dl.InstanceCatalog.HIGHMEM_L,\n",
    "        concurrency=4,\n",
    "        runner_image='python:3.9',\n",
    "        autoscaler=dl.KubernetesRabbitmqAutoscaler(\n",
    "            min_replicas=1,\n",
    "            max_replicas=10,\n",
    "            queue_length=20\n",
    "        )\n",
    "    )\n",
    "service.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Instance Types\n",
    "\n",
    "Choose the right compute power for your needs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all instance types\n",
    "[e.value for e in dl.InstanceCatalog]\n",
    ">> ['regular-xs',\n",
    " 'regular-s',\n",
    " 'regular-m',\n",
    " 'regular-l',\n",
    " 'highmem-xs',\n",
    " 'highmem-s',\n",
    " 'highmem-m',\n",
    " 'highmem-l',\n",
    " 'gpu-k80-s',\n",
    " 'gpu-k80-m',\n",
    " 'gpu-t4',\n",
    " 'gpu-t4-m']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Autoscaling Configuration\n",
    "\n",
    "Dataloop supports two autoscaling methods for services. Each method aligns with different service types and workload patterns:\n",
    "\n",
    "\n",
    "1. **Queue Length-Based Autoscaler** \u2013 scales services according to the number of pending executions in the service's queue.\n",
    "2. **HTTP Request-Based Autoscaler (RPS)** - scales services based on incoming HTTP requests.\n",
    "\n",
    "\n",
    "### Queue Length-Based Autoscaler\n",
    "\n",
    "This autoscaler scales a service based on the length of its execution queue.\n",
    "\n",
    "**Why**:\n",
    "<br>\n",
    "\n",
    "Each time an app service submits an execution\u2014such as for a model run, data processing, or pipeline step\u2014it\u2019s added to a queue. The autoscaler tracks how many executions are waiting and adjusts the number of service replicas accordingly.\n",
    "\n",
    "\n",
    "**When to Use**:\n",
    "<br>\n",
    "\n",
    "Use the queue length-based autoscaler when your service processes executions (e.g., models, pipelines, event-driven logic).\n",
    "\n",
    "*Note*: By default, Dataloop services use this queue length-based autoscaler to scale dynamically based on the number of pending executions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Set Up Autoscaler\n",
    "To configure automatic scaling for your service based on execution queue length:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a queue length-based autoscaler\n",
    "autoscaler = dl.KubernetesRabbitmqAutoscaler(\n",
    "    min_replicas=0,            # Fully scale down when idle (cost-saving)\n",
    "    max_replicas=5,            # Limits the number of concurrent replicas for the service\n",
    "    queue_length=10,           # Scale up when queue exceeds 10 messages\n",
    "    polling_interval=10,       # Check queue length every 10 seconds \n",
    "    cooldown_period=300        # Wait 5 minutes (300s) before scaling down after traffic drops\n",
    ")\n",
    "\n",
    "# Get the target service and assign runtime with autoscaler\n",
    "service = project.services.get('auto-scaling-service')\n",
    "service.runtime = dl.KubernetesRuntime(\n",
    "    pod_type=dl.InstanceCatalog.REGULAR_S,  # Small pod type\n",
    "    concurrency=2,                          # Each replica handles 2 executions in parallel\n",
    "    autoscaler=autoscaler \n",
    ")\n",
    "\n",
    "# Apply the configuration\n",
    "service.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Autoscaling Strategies (Queue-Based)\n",
    "\n",
    "You can fine-tune your autoscaling behavior depending on your operational priorities:\n",
    "\n",
    "**Performance-Optimized Scaling**\n",
    "<br>\n",
    "\n",
    "This configuration is designed to respond quickly to sudden increases in workload by scaling aggressively. Suitable for latency-sensitive or time-critical operations.\n",
    "\n",
    "*Use this strategy when*:\n",
    "- You expect short, heavy bursts of tasks.\n",
    "- Latency reduction is more important than cost.\n",
    "- You want fast provisioning of compute resources.\n",
    "\n",
    "<br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance-optimized: aggressive scaling for burst workloads\n",
    "burst_autoscaler = dl.KubernetesRabbitmqAutoscaler(\n",
    "    min_replicas=1,       # Keep at least 1 replica running at all times\n",
    "    max_replicas=20,      # Allow the system to scale up to 20 replicas\n",
    "    queue_length=5,       # Trigger scaling when there are more than 5 items in the queue\n",
    "    polling_interval=5,   # Check queue status every 5 seconds for fast reaction\n",
    "    cooldown_period=60    # Scale down quickly after 60 seconds of inactivity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Cost-Optimized Scaling**\n",
    "<br>\n",
    "\n",
    "This configuration aims to reduce compute costs by tolerating higher queue sizes while delaying scale-down events. Ideal for workloads that can afford slower processing.\n",
    "\n",
    "\n",
    "*Use this strategy when*:\n",
    "- Budget and resource efficiency is a top priority.\n",
    "- You have non-urgent workloads that tolerate queuing delays.\n",
    "- You're processing tasks in bulk with moderate urgency.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-optimized: slower scaling with conservative thresholds\n",
    "efficient_autoscaler = dl.KubernetesRabbitmqAutoscaler(\n",
    "    min_replicas=0,         # Scale down to zero when idle to save cost\n",
    "    max_replicas=3,         # Limit to 3 replicas to cap resource usage\n",
    "    queue_length=15,        # Only scale up when queue exceeds 15 messages\n",
    "    polling_interval=30,    # Check queue status every 30 seconds (less overhead)\n",
    "    cooldown_period=600     # Wait 10 minutes before scaling down (avoids flapping)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### HTTP Request-Based Autoscaler\n",
    "\n",
    "The RPS autoscaler tracks the rate of incoming HTTP requests over a defined time window and adjusts scale based on real-time user interaction.\n",
    "\n",
    "**Why**: \n",
    "<br> \n",
    "\n",
    "For UI-based services that do not produce executions, such as annotation studio panels, scaling based on queue length is not relevant. \n",
    "In these cases, autoscaling is driven by HTTP request rate instead.\n",
    "\n",
    "\n",
    "**When to Use**:\n",
    "<br> \n",
    "\n",
    "Services that serve UI components or panels, such as Annotation Studio panels.\n",
    "\n",
    "---\n",
    "\n",
    "#### Set Up Autoscaler\n",
    "\n",
    "To configure automatic scaling for your service based on HTTP requests:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an HTTP request-based autoscaler\n",
    "autoscaler = dl.KubernetesRPSAutoscaler(\n",
    "    type='rps',                # Activate HTTP request-based autoscaler (mandatory)\n",
    "    min_replicas=0,            # Fully scale down when idle (cost-saving)\n",
    "    max_replicas=5,            # Limits the number of concurrent replicas for the service\n",
    "    threshold=100,             # Scale up if more than 100 HTTP requests occur in the sliding window\n",
    "    rate_seconds=30,            # Sliding time window in which the request count is evaluated\n",
    "    polling_interval=10,       # Check request count every 10 seconds\n",
    "    cooldown_period=3600      # Wait 60 minutes (3600s) before scaling down after traffic drops\n",
    ")\n",
    "\n",
    "# Get the target service and assign runtime with autoscaler\n",
    "service = project.services.get('auto-scaling-service')\n",
    "service.runtime = dl.KubernetesRuntime(\n",
    "    pod_type=dl.InstanceCatalog.REGULAR_XS,  # Extra small pod type\n",
    "    autoscaler=autoscaler\n",
    ")\n",
    "\n",
    "# Apply the configuration\n",
    "service.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Step-by-Step Runtime Flow**\n",
    "<br>\n",
    "\n",
    "1. **Panel Activation** - A user opens a panel in the UI.\n",
    "\n",
    "2. **Request Routing** - The panel triggers HTTP requests to the service. These are monitored by the autoscaler (`\"type\": \"rps\"`).\n",
    "\n",
    "3. **Autoscaler Monitors Usage** - Every 10 seconds (`polling_interval`), the autoscaler counts HTTP requests over the last 30 seconds (`rate_seconds`). If requests > 100 (`threshold`), the service scales up (adding additional replicas).\n",
    "\n",
    "4. **Scaling Behavior** - The service can scale from 0 to 5 replicas (`min_replicas` to `max_replicas`), based on traffic. After traffic drops, it waits 3600 seconds before scaling down (`cooldown_period`).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Autoscaling Strategies (Requests-Based)\n",
    "\n",
    "You can fine-tune your autoscaling behavior depending on your operational priorities:\n",
    "\n",
    "**Performance-Optimized Scaling**\n",
    "<br>\n",
    "\n",
    "This configuration prioritizes quick responsiveness, ideal for services that must scale immediately when users interact with the UI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance-optimized: aggressive scaling for burst workloads\n",
    "burst_autoscaler = dl.KubernetesRPSAutoscaler(\n",
    "  type='rps',              # Enables HTTP request-based autoscaling (mandatory)\n",
    "  min_replicas=1,           # Always keep one replica running for faster first response\n",
    "  max_replicas=5,           # Allow scaling up to 5 replicas under high traffic\n",
    "  threshold=50,            # If more than 50 requests are received in the rate window, scale up\n",
    "  rate_seconds=15,          # Count requests within the last 15 seconds\n",
    "  polling_interval=10,      # Check request count every 10 seconds (high frequency)\n",
    "  cooldown_period=3600      # Wait 3600 seconds of inactivity before scaling down to 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Cost-Optimized Scaling**\n",
    "<br> \n",
    "\n",
    "This configuration minimizes resource usage by scaling conservatively. It's ideal when responsiveness is less critical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-optimized: conservative scaling for resource efficiency\n",
    "efficient_autoscaler = dl.KubernetesRPSAutoscaler(\n",
    "  type='rps',              # Enables HTTP request-based autoscaling (mandatory)\n",
    "  min_replicas=0,           # Allow full scale-down to zero when idle\n",
    "  max_replicas=5,           # Allow scaling up to 5 replicas under high traffic\n",
    "  threshold=100,           # Only scale when at least 100 requests arrive in the time window\n",
    "  rate_seconds=30,          # Use a longer 30-second window to count requests\n",
    "  polling_interval=20,      # Check traffic every 20 seconds (less frequent)\n",
    "  cooldown_period=3600      # Wait 3600 seconds of inactivity before scaling down to 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "# \ud83d\udd10 Security and Environment\n",
    "\n",
    "### Working with Secrets\n",
    "\n",
    "To add integrations and secrets to your organization, check out [this guide](https://developers.dataloop.ai/tutorials/data_management/integrations_and_secrets/chapter#creating-key-value-secrets-).\n",
    "\n",
    "Integrations can be added to the manifest or to the service directly.\n",
    "\n",
    "In the manifest:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n\"components\": {\n    \"modules\": [\n        // map a module to an integration in the DPK\n        {\"integrations\": [\"api_key\"]}\n    ],\n    \"integrations\": [\n        // add an integration to the DPK\n        {\n            \"env\": \"API_KEY\", // the environment variable name inside the FaaS function\n            \"key\": \"api_key\", // the key name of the integration in the DPK\n            \"value\": \"integration-id\", // the integration/secret id\n            \"type\": \"key_value\", // the type of the integration\n            \"description\": \"API key for OpenAI platform\", // the description of the integration\n        }\n    ]\n}\n```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Securely manage sensitive information:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy service with secrets\n",
    "service = project.services.get('secure-service')\n",
    "service.integrations = [{\n",
    "        \"env\": \"API_KEY\",\n",
    "        \"value\": \"integration-id\",\n",
    "        \"type\": \"key_value\",\n",
    "        \"description\": \"API key for OpenAI platform\",\n",
    "    },\n",
    "    {\n",
    "        \"env\": \"DB_PASSWORD\",\n",
    "        \"value\": \"integration-id\",\n",
    "        \"type\": \"key_value\",\n",
    "        \"description\": \"API key for OpenAI platform\",\n",
    "    }\n",
    "]\n",
    "service.update()\n",
    "\n",
    "# Access secrets in your function\n",
    "def secure_function(item: dl.Item):\n",
    "    import os\n",
    "    api_key = os.environ['API_KEY']\n",
    "    db_password = os.environ['DB_PASSWORD']\n",
    "    # Your secure code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## \ud83d\udcca Monitoring and Logging\n",
    "\n",
    "### Progress Tracking\n",
    "\n",
    "Monitor function execution progress:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(item: dl.Item, progress: dl.Progress):\n",
    "    # Initial status\n",
    "    progress.update(status='started', progress=0)\n",
    "\n",
    "    # Update progress during execution\n",
    "    progress.update(\n",
    "        status='processing',\n",
    "        progress=50,\n",
    "        message='Halfway there!'\n",
    "    )\n",
    "\n",
    "    # Final update\n",
    "    progress.update(\n",
    "        status='completed',\n",
    "        progress=100,\n",
    "        message='Successfully processed item'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Context Management\n",
    "\n",
    "Access execution context for better monitoring:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_function(item: dl.Item, context: dl.Context):\n",
    "    # Get execution details\n",
    "    execution_id = context.execution_id\n",
    "    service_id = context.service_id\n",
    "\n",
    "    # Use context logger\n",
    "    context.logger.info(f'Processing item: {item.id}')\n",
    "    context.logger.debug('Detailed debug information')\n",
    "\n",
    "    # Add custom metrics\n",
    "    context.add_metric('processing_time', 1.23)\n",
    "    context.add_metric('confidence_score', 0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Context will also provide more information about the execution, such as the trigger id, task id, and more (when relevant).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trigger id\n",
    "trigger_id = context.trigger_id\n",
    "\n",
    "# Get the task id\n",
    "task_id = context.task_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## \ud83d\udca1 Pro Tips & Best Practices\n",
    "\n",
    "### Resource Optimization\n",
    "\n",
    "- Start with smaller instance types and scale up as needed\n",
    "- Use autoscaling to handle variable workloads\n",
    "- Monitor resource usage to optimize configurations\n",
    "\n",
    "### Performance Tuning\n",
    "\n",
    "- Adjust concurrency based on function resource usage\n",
    "- Set appropriate timeouts for your workload\n",
    "- Use efficient instance types for your specific needs\n",
    "\n",
    "### Security Best Practices\n",
    "\n",
    "- Always use secrets for sensitive information\n",
    "- Implement proper error handling\n",
    "- Regular audit of service configurations\n",
    "\n",
    "### Monitoring Guidelines\n",
    "\n",
    "- Implement comprehensive logging\n",
    "- Use progress updates for long-running functions\n",
    "- Monitor autoscaling behavior\n",
    "\n",
    "Need help? Check out our other tutorials or reach out to our support team. Happy configuring! \u26a1\ufe0f\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}