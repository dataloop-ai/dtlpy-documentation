{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Pipelines: Building Automated Workflows \ud83d\udd04\n",
    "\n",
    "This tutorial will help you master pipeline creation in Dataloop, covering all node types and their advanced features.\n",
    "\n",
    "## Prerequisites \ud83c\udfaf\n",
    "\n",
    "First, log in to the platform:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "if dl.token_expired():\n",
    "    dl.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Get your project and required resources:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dl.projects.get(project_name='My Project')\n",
    "dataset = project.datasets.get(dataset_name='My Dataset')\n",
    "recipe = dataset.recipes.list()[0]\n",
    "service = project.services.get(service_name='My Service')\n",
    "function_name = 'My Function'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Creating a Pipeline \ud83d\ude80\n",
    "\n",
    "Create a new pipeline in your project:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = project.pipelines.create(name='my-pipeline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Node Types and Configuration \ud83d\udee0\ufe0f\n",
    "\n",
    "### 1. Dataset Node\n",
    "\n",
    "The Dataset Node serves as a data source or sink in your pipeline:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_node = dl.DatasetNode(\n",
    "    name='My Dataset Node',\n",
    "    project_id=project.id,\n",
    "    dataset_id=dataset.id,\n",
    "    dataset_folder='/specific/folder',  # Optional - work in specific folder\n",
    "    load_existing_data=True,  # Optional - load existing items\n",
    "    data_filters=dl.Filters(field='dir', values='/folder'),  # Optional - filter items\n",
    "    position=(1, 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Task Node\n",
    "\n",
    "The Task Node creates annotation or QA tasks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_node = dl.TaskNode(\n",
    "    name='My Task',\n",
    "    project_id=project.id,\n",
    "    dataset_id=dataset.id,\n",
    "    recipe_id=recipe.id,\n",
    "    recipe_title=recipe.title,\n",
    "    task_owner='owner@domain.com',\n",
    "    workload=[dl.WorkloadUnit(assignee_id='assignee@domain.com', load=100)],\n",
    "    task_type='annotation',  # or 'qa'\n",
    "    position=(2, 1),\n",
    "    due_date=(datetime.datetime.now() + datetime.timedelta(days=7)).timestamp() * 1000,\n",
    "    # Optional parameters\n",
    "    priority=dl.TaskPriority.MEDIUM,\n",
    "    groups=['team1', 'team2'],  # Optional - assign to specific groups\n",
    "    # Consensus parameters (optional)\n",
    "    consensus_task_type=dl.ConsensusTaskType.REGULAR,\n",
    "    consensus_percentage=20,  # Percentage of items for consensus\n",
    "    consensus_assignees=2  # Number of assignees per consensus item\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Function Node\n",
    "\n",
    "The Function Node executes service functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_node = dl.FunctionNode(\n",
    "    name=service.name,\n",
    "    service=service,\n",
    "    function_name=function_name,\n",
    "    position=(3, 1),\n",
    "    project_id=project.id,  # Optional - defaults to service project\n",
    "    project_name='MyProject'  # Optional - defaults to service project\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. Code Node\n",
    "\n",
    "The Code Node allows inline code execution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_item(item, string):\n",
    "    \"\"\"Custom processing logic\"\"\"\n",
    "    item.metadata['user'] = {'userInput': string}\n",
    "    item.update()\n",
    "    return item\n",
    "\n",
    "code_node = dl.CodeNode(\n",
    "    name='Process Item',\n",
    "    project_id=project.id,\n",
    "    method=process_item,\n",
    "    position=(4, 1),\n",
    "    project_name=project.name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Building the Pipeline Flow \ud83d\udd17\n",
    "\n",
    "### 1. Adding Nodes\n",
    "\n",
    "Add nodes to your pipeline and connect them in sequence:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.nodes.add(node=dataset_node).connect(task_node).connect(function_node).connect(code_node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Advanced Node Connections\n",
    "\n",
    "Connect nodes with filters and specific ports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with filters\n",
    "task_node.connect(\n",
    "    node=function_node,\n",
    "    filters=dl.Filters(field='status', values='completed'),\n",
    "    action='complete'  # Trigger on specific action\n",
    ")\n",
    "\n",
    "# Connect specific ports\n",
    "source_port = task_node.outputs[0]  # Get first output port\n",
    "target_port = function_node.inputs[0]  # Get first input port\n",
    "task_node.connect(\n",
    "    node=function_node,\n",
    "    source_port=source_port,\n",
    "    target_port=target_port\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Adding Triggers\n",
    "\n",
    "Add event or cron triggers to start nodes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event trigger (e.g., on item creation)\n",
    "dataset_node.add_trigger(\n",
    "    trigger_type=dl.TriggerType.EVENT,\n",
    "    resource=dl.TriggerResource.ITEM,\n",
    "    actions=dl.TriggerAction.CREATED,\n",
    "    filters=dl.Filters(field='dir', values='/incoming')\n",
    ")\n",
    "\n",
    "# Cron trigger (scheduled execution)\n",
    "dataset_node.add_trigger(\n",
    "    trigger_type=dl.TriggerType.CRON,\n",
    "    cron='0 0 * * *'  # Run daily at midnight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Finalizing and Managing the Pipeline \ud83d\udccb\n",
    "\n",
    "### 1. Update and Install\n",
    "\n",
    "Save your changes and deploy the pipeline:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.update()\n",
    "pipeline.install()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Pipeline Management\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open in web UI\n",
    "pipeline.open_in_web()\n",
    "\n",
    "# Delete pipeline\n",
    "project.pipelines.delete(pipeline_id=pipeline.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Best Practices \ud83d\udca1\n",
    "\n",
    "1. **Node Positioning**\n",
    "   - Use meaningful positions for visual clarity\n",
    "   - Keep flow direction consistent (usually left to right)\n",
    "   - Avoid overlapping nodes\n",
    "\n",
    "2. **Error Handling**\n",
    "   - Add error handling in code nodes\n",
    "   - Use filters to control flow based on success/failure\n",
    "   - Monitor node execution status\n",
    "\n",
    "3. **Resource Management**\n",
    "   - Clean up completed executions\n",
    "   - Monitor resource usage\n",
    "   - Use appropriate timeouts\n",
    "\n",
    "4. **Documentation**\n",
    "   - Add clear node names and descriptions\n",
    "   - Document expected inputs and outputs\n",
    "   - Maintain pipeline version history\n",
    "\n",
    "Need help? Check our [Pipeline documentation](https://docs.dataloop.ai/docs/pipelines-overview) for more details! \ud83d\ude80\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}