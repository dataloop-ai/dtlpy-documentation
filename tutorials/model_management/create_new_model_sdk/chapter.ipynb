{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Create your own model  \n", "  \n", "You can use your own model to use on the platform by creating an App and Model entities, and then use a model adapter to create an API with Dataloop.  \n", "  \n", "In this tutorial you will learn how to create a basic model adapter to be able to inference on platform items.  \n", "  \n", "### Create a model adapter  \n", "  \n", "In the example code below, the adapter is defined in a script saved as `adapter.py`. The SimpleModelAdapter class inherits from `dl.BaseModelAdapter`, which contains  \n", "all the Dataloop methods required to interact with the App and Model, as well as some internal wrapper functions that make it easier to use Dataloop entities (e.g. `predict_items`, `predict_datasets`).  \n", "  \n", "The minimum required functions to implement for a model to inference are `load` and `predict`.  \n", "  \n", "`load` is supposed to implement loading the model (e.g. from any weights.pth). The `load` takes a local path as input.  \n", "  \n", "In the `dl.BaseModelAdapter` we have a `load_from_model` wrapper which will download the model artifacts locally and call the custom `load` with the path containing all the files.  \n", "  \n", "`predict` is where the model will do its inference, and the predict function expects a preprocessed batch (e.g. ndarray for images), and returns a list of dl.AnnotationCollection entities.  \n", "  \n", "The wrapper for `predict` is `predict_items`, which takes a list of items from the platform and prepares everything for predicting. It uses the `prepare_item_func` to preprocess items into the a batch and calles the custom `predict`. After the prediction, it takes the ouput and uploads it to each item.  \n", "  \n", "NOTE: You can edit the preprocess function by simply override the `prepare_item_func` method. For example, to pass the items as-is you can just return the inputted item.  \n", "  \n", "in `adapter.py`, add the following model adapter:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "import torch\n", "import os\n", "", "@dl.Package.decorators.module(name='model-adapter',\n", "                              description='Model Adapter for my model',\n", "                              init_inputs={'model_entity': dl.Model})\n", "class SimpleModelAdapter(dl.BaseModelAdapter):\n", "    def load(self, local_path, **kwargs):\n", "        print('loading a model')\n", "        self.model = torch.load(os.path.join(local_path, 'model.pth'))\n", "", "    def predict(self, batch, **kwargs):\n", "        print('predicting batch of size: {}'.format(len(batch)))\n", "        preds = self.model(batch)\n", "        batch_annotations = list()\n", "        for i_img, predicted_class in enumerate(preds):  # annotations per image\n", "            image_annotations = dl.AnnotationCollection()\n", "            # in this example, we will assume preds is a label for a classification model\n", "            image_annotations.add(annotation_definition=dl.Classification(label=predicted_class),\n", "                                  model_info={'name': self.model_name})\n", "            batch_annotations.append(image_annotations)\n", "", "        return batch_annotations\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Please see an example [here](https://github.com/dataloop-ai-apps/torch-models/blob/main/adapters/resnet/resnet_adapter.py) (for PyTroch's Resnet) in Github of a working model adapter and see how to construct Annotation Collections.  \n", "  \n", "### Publish the App  \n", "  \n", "In order to create the app for publishing, installing and using the model created from the Model Adapter, a DPK Manifest is required.  \n", "For more information regarding our apps please see the Applications chapter [here] (https://developers.dataloop.ai/tutorials/applications/).  \n", "  \n", "A Dataloop Manifest example:  \n", "``` json  \n", "    {  \n", "    \"name\": \"<your app name>\",  \n", "    \"displayName\": \"<the app display name>\",  \n", "    \"version\": \"<current version>\",  \n", "    \"scope\": \"project\",  \n", "    \"description\": \"\",  \n", "    \"codebase\": {  \n", "        \"type\": \"git\",  \n", "        \"gitUrl\": \"<your-url>\",  \n", "        \"gitTag\": \"\"  \n", "    },  \n", "    \"components\": {  \n", "        \"computeConfigs\": [  \n", "            {  \n", "                \"name\": \"<service-name>\",  \n", "                \"runtime\": {  \n", "                    \"podType\": \"regular-xs\",  \n", "                    \"concurrency\": 1,  \n", "                    \"autoscaler\": {  \n", "                        \"type\": \"rabbitmq\",  \n", "                        \"minReplicas\": 0,  \n", "                        \"maxReplicas\": 2,  \n", "                        \"queueLength\": 100  \n", "                    }  \n", "                }  \n", "            }  \n", "        ],  \n", "        \"modules\": [  \n", "            {  \n", "                \"name\": \"<module-name>\",  \n", "                \"entryPoint\": \"model_adapter.py\",  \n", "                \"className\": \"Adapter\",  \n", "                \"computeConfig\": \"<service-name>\",  \n", "                \"description\": \"\",  \n", "                \"initInputs\": [  \n", "                    {  \n", "                        \"type\": \"Model\",  \n", "                        \"name\": \"model_entity\"  \n", "                    }  \n", "                ],  \n", "                \"functions\": [  \n", "                    {  \n", "                        \"name\": \"evaluate_model\",  \n", "                        \"input\": [  \n", "                            {  \n", "                                \"type\": \"Model\",  \n", "                                \"name\": \"model\",  \n", "                                \"description\": \"Dataloop Model Entity\"  \n", "                            },  \n", "                            {  \n", "                                \"type\": \"Dataset\",  \n", "                                \"name\": \"dataset\",  \n", "                                \"description\": \"Dataloop Dataset Entity\"  \n", "                            }  \n", "                        ],  \n", "                        \"output\": [  \n", "                            {  \n", "                                \"type\": \"Model\",  \n", "                                \"name\": \"model\",  \n", "                                \"description\": \"Dataloop Model Entity\"  \n", "                            },  \n", "                            {  \n", "                                \"type\": \"Dataset\",  \n", "                                \"name\": \"dataset\",  \n", "                                \"description\": \"Dataloop Dataset Entity\"  \n", "                            }  \n", "                        ],  \n", "                        \"displayName\": \"Evaluate a Model\",  \n", "                        \"displayIcon\": \"\",  \n", "                        \"description\": \"Function to evaluate model performance\"  \n", "                    },  \n", "                    {  \n", "                        \"name\": \"predict_items\",  \n", "                        \"input\": [  \n", "                            {  \n", "                                \"type\": \"Item[]\",  \n", "                                \"name\": \"items\",  \n", "                                \"description\": \"List of items to run inference on\"  \n", "                            }  \n", "                        ],  \n", "                        \"output\": [  \n", "                            {  \n", "                                \"type\": \"Item[]\",  \n", "                                \"name\": \"items\",  \n", "                                \"description\": \"The same input images for prediction.\"  \n", "                            },  \n", "                            {  \n", "                                \"type\": \"Annotation[]\",  \n", "                                \"name\": \"annotations\",  \n", "                                \"description\": \"The predicted annotations.\"  \n", "                            }  \n", "                        ],  \n", "                        \"displayName\": \"Predict Items\",  \n", "                        \"displayIcon\": \"\",  \n", "                        \"description\": \"Function to run inference on items\"  \n", "                    },  \n", "                    {  \n", "                        \"name\": \"predict_dataset\",  \n", "                        \"input\": [  \n", "                            {  \n", "                                \"type\": \"Dataset\",  \n", "                                \"name\": \"dataset\",  \n", "                                \"description\": \"\"  \n", "                            },  \n", "                            {  \n", "                                \"type\": \"Json\",  \n", "                                \"name\": \"filters\",  \n", "                                \"description\": \"Dataloop Filter DQL\"  \n", "                            }  \n", "                        ],  \n", "                        \"output\": [],  \n", "                        \"displayName\": \"Predict Dataset\",  \n", "                        \"displayIcon\": \"\",  \n", "                        \"description\": \"Function to run inference on a dataset.\"  \n", "                    },  \n", "                    {  \n", "                        \"name\": \"train_model\",  \n", "                        \"input\": [  \n", "                            {  \n", "                                \"type\": \"Model\",  \n", "                                \"name\": \"model\",  \n", "                                \"description\": \"Dataloop Model Entity\"  \n", "                            }  \n", "                        ],  \n", "                        \"output\": [  \n", "                            {  \n", "                                \"type\": \"Model\",  \n", "                                \"name\": \"model\",  \n", "                                \"description\": \"Dataloop Model Entity\"  \n", "                            }  \n", "                        ],  \n", "                        \"displayName\": \"Train a Model\",  \n", "                        \"displayIcon\": \"\",  \n", "                        \"description\": \"Function to train model\"  \n", "                    },  \n", "                    {  \n", "                        \"name\": \"predict_dataset\",  \n", "                        \"input\": [  \n", "                            {  \n", "                                \"type\": \"Dataset\",  \n", "                                \"name\": \"dataset\",  \n", "                                \"description\": \"\"  \n", "                            },  \n", "                            {  \n", "                                \"type\": \"Json\",  \n", "                                \"name\": \"filters\",  \n", "                                \"description\": \"Dataloop Filter DQL\"  \n", "                            }  \n", "                        ],  \n", "                        \"output\": [],  \n", "                        \"displayName\": \"Predict Items\",  \n", "                        \"displayIcon\": \"\",  \n", "                        \"description\": \"Function to run inference on a whole dataset\"  \n", "                    }  \n", "                ]  \n", "            }  \n", "        ],  \n", "        \"models\": [  \n", "             {  \n", "                \"name\": \"<model name>\",  \n", "                \"moduleName\": \"<module name>\",  \n", "                \"scope\": \"project\",  \n", "                \"status\": \"pre-trained\",  \n", "                \"configuration\": {  \n", "                    \"weights_filename\": \"<weights file name>\",  \n", "                    \"epochs\": 10,  \n", "                    \"batch_size\": 4,  \n", "                    \"imgsz\": 640,  \n", "                    \"conf_thres\": 0.25,  \n", "                    \"iou_thres\": 0.45,  \n", "                    \"max_det\": 1000  \n", "                },  \n", "                \"inputType\": \"image\",  \n", "                \"outputType\": \"box\",  \n", "                \"description\": \"\",  \n", "                \"labels\": [< a list of your pre-trained labels>]  \n", "            }  \n", "        ]  \n", "    }  \n", "}  \n", "```  \n", "  \n", "To change the service configurations, see the documentation on [service types](https://dataloop.ai/docs/service-runtime).  \n", "  \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then we can publish and install the app in our project.  \n", "  \n", " ```bash  \n", "dlp app publish --project-name \"<your project name>\"  \n", "```  \n", "  \n", "To install the app from the UI, find the published app in the Models Marketplace and click Install:  \n", "  \n", "![Marketplace Model Installation](../../../assets/images/model_management/market_place_install.png)  \n", "  \n", " To install from the SDK, running the following lines with python from the directory where the manifest is located:  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["project = dl.projects.get(project_name=\"<your-project-name>\")\n", "dpk = dl.dpks.get(dpk_name='<app-name>')\n", "project.apps.install(dpk=dpk)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Get the model and upload artifacts  \n", "  \n", "Now you can get the model created by installing the app and upload pretrained model weights with an Artifact Item.  \n", "Here, the weights will be uploaded as an Item Artifact connected to the model.  \n", "You can upload any weights file here and use the artifact filename to update the `weights_filename` field in the model configuration (in the manifest).  \n", "  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["project = dl.projects.get(project_name=\"<your-project-name>\")\n", "model = project.models.get(\"<model-name>\")  # From the manifest's models.name\n", "artifact = model.artifacts.upload(filepath='/path/to/model_weights.pth')\n", "model.configuration['weights_filename'] = artifact.filename\n", "", "# to deploy the model\n", "model.deploy()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Checking that your model works  \n", "  \n", "NOTE: The deployed service must be up and ready in order to run the predictions (including in the test tab). You can check the status of the deployed service in the services page.  \n", "  \n", "### Via the UI  \n", "  \n", "You should now be able to see the model in the \u201cDeployed\u201d tab. After clicking on your model, you should see a \u201cTest\u201d tab where you can drag and drop an image, click \u201cTest\u201d and see the results of your model prediction.  \n", "  \n", "If you get timeouts or error predicting, check that the service is up and is functioning as expected.  \n", "  \n", "![Screenshot of deployed model test tab](../../../assets/images/model_management/test_tab.png)  \n", "  \n", "### Via the SDK  \n", "  \n", "To test whether your function was successfully uploaded and deployed onto the platform, you can use the `model.predict()` function to predict on a list of item IDs.  \n", "The function will return an Execution entity, which you can use to check the status of the prediction execution.  \n", "Once the execution is completed, the annotation will be uploaded to each item.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["model = dl.models.get(model_id='<model_id>')\n", "item = dl.items.get(model_id='<item_id>')\n", "", "execution = model.predict(item_ids=[item.id])\n", "# wait for the execution to complete and get an updated execution\n", "execution.wait()\n", "execution = dl.executions.get(execution_id=execution.id)\n", "# print the most recent status\n", "print(execution.status[-1]['status'])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["  \n", "If you encounter errors, you will need to look at the logs to see where the error occurred.  Go to \"Model Management\", under the \"Deployed\" tab, click on the number in the \"Executions\" column for the appropriate model, and then click on the \"Execution\" log icon on the right side of the screen (the paper icon). Here you can see the output of the cloud machine. You can also access this page via the \"Application Hub\", under \"Executions\".  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}