{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Create your own model  \n", "  \n", "You can use your own model to use on the platform by creating Package and Model entities, and then use a model adapter to create an API with Dataloop.  \n", "  \n", "In this tutorial you will learn how to create a basic model adapter to be able to inference with a pretrained model and how to fine-tune a pretrained model with your custom dataset.  \n", "  \n", "  \n", "### Inference from a pre-trained model  \n", "  \n", "To use a pretrained model to inference on a new item, you will create a model adapter, push the package, upload the weights as an artifact, and create the model entity.  \n", "  \n", "  \n", "#### Create a model adapter  \n", "  \n", "In the example code below, the adapter is defined in a script saved as \"adapter_script.py\". The SimpleModelAdapter class inherits from dl.BaseModelAdapter, which contains all the Dataloop methods required to interact with the Package and Model, as well as some helper functions that make it easier to use Dataloop entities (e.g. predict_items, predict_datasets).  \n", "  \n", "The minimum required functions to implement for a model to inference are _load_ and _predict_.  \n", "  \n", "\u201cLoad\u201d will load a model from a saved model weights file.  If the model is instantiated with a model entity (as it is here), the load function is expected to input the local path for the weights file.  \n", "  \n", "If the weights file is a link, it can be uploaded as a LinkArtifact entity during model creation. If the file is saved locally, enter the appropriate name in the configurations (e.g. default_configuration=\u2019weights_filename\u2019 : \u2018model.pth\u2019). Helper functions in the BaseModelAdapter will download the weights file locally and load it based on the name listed here.  \n", "  \n", "\u201cPredict\u201d is where the model will do its inference, and the predict function expects input images as ndarrays, and returns a list of dl.AnnotationCollection entities.  \n", "  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "import torch\n", "import os\n", "", "@dl.Package.decorators.module(name='model-adapter',\n", "                              description='Model Adapter for my model',\n", "                              init_inputs={'model_entity': dl.Model})\n", "class SimpleModelAdapter(dl.BaseModelAdapter):\n", "    def load(self, local_path, **kwargs):\n", "        print('loading a model')\n", "        self.model = torch.load(os.path.join(local_path, 'model.pth'))\n", "", "    def predict(self, batch, **kwargs):\n", "        print('predicting batch of size: {}'.format(len(batch)))\n", "        preds = self.model(batch)\n", "        batch_annotations = list()\n", "        for i_img, predicted_class in enumerate(preds):  # annotations per image\n", "            image_annotations = dl.AnnotationCollection()\n", "            # in this example, we will assume preds is a label for a classification model\n", "            image_annotations.add(annotation_definition=dl.Classification(label=predicted_class),\n", "                                  model_info={'name': self.model_name})\n", "            batch_annotations.append(image_annotations)\n", "", "        return batch_annotations\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Please see an example [here](https://github.com/dataloop-ai/yolov5/blob/master/dataloop/model_adapter.py) (for YOLOv5) in Github of a working model adapter and see how to construct Annotation Collections.  \n", "  \n", "#### Push the package  \n", "  \n", "To create our Package entity, we first need to retrieve the metadata and indicate where the entry point to the package is within the codebase. If you\u2019re creating a Package with code from Git, change the codebase type to be dl.GitCodebase. If the code is somewhere other than the root directory, you can pack the codebase with project.codebases.pack(directory=\u2019<path to local dir>\u2019).  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "from adapter_script import SimpleModelAdapter\n", "", "project = dl.projects.get(project_name='<project_name>')\n", "dataset = project.datasets.get(dataset_name='<dataset_name')\n", "", "# codebase = project.codebases.pack(directory='<path to local dir>')\n", "# codebase: dl.GitCodebase = dl.GitCodebase(git_url='github.com/mygit', git_tag='v25.6.93')\n", "metadata = dl.Package.get_ml_metadata(cls=SimpleModelAdapter,\n", "                                      default_configuration={},\n", "                                      output_type=dl.AnnotationType.CLASSIFICATION\n", "                                      )\n", "module = dl.PackageModule.from_entry_point(entry_point='adapter_script.py')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then we can push the package and all its components to the cloud. To change the service configurations, see the documentation on [service types](https://dataloop.ai/docs/service-runtime).  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["package = project.packages.push(package_name='My-Package',\n", "                                src_path=os.getcwd(),\n", "                                package_type='ml',\n", "                                # codebase=codebase,\n", "                                modules=[module],\n", "                                is_global=False,\n", "                                service_config={\n", "                                    'runtime': dl.KubernetesRuntime(pod_type=dl.INSTANCE_CATALOG_GPU_K80_S,\n", "                                                                    autoscaler=dl.KubernetesRabbitmqAutoscaler(\n", "                                                                        min_replicas=0,\n", "                                                                        max_replicas=1),\n", "                                                                    concurrency=1).to_json()},\n", "                                metadata=metadata)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Upload artifacts and create the model  \n", "  \n", "Now you can create a model and upload pretrained model weights with an Artifact Item. Here, the Artfiact item is where the saved model weights are. You can upload any weights file here and name it according to the 'weights_filename' in the configuration.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["artifact = dl.LocalArtifact(local_path='<path to weights>')\n", "model = package.models.create(model_name='tutorial-model',\n", "                              description='first model we are uploading',\n", "                              tags=['pretrained', 'tutorial'],\n", "                              dataset_id=None,\n", "                              configuration={'weights_filename': '<weights filename and extension>'\n", "                                             },\n", "                              project_id=package.project.id,\n", "                              model_artifacts=[artifact],\n", "                              labels=['car', 'fish', 'pizza']\n", "                              )\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To deploy a model, its status must be set to trained so you can deploy a model by updating the status to trained and then deploy it.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["model.status = 'trained'\n", "model.update()\n", "model.deploy()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Checking that your model works  \n", "  \n", "#### Via the UI  \n", "  \n", "You should now be able to see the model in the \u201cDeployed\u201d tab. After clicking on your model, you should see a \u201cTest\u201d tab where you can drag and drop an image, click \u201cTest\u201d and see the results of your model prediction.  \n", "  \n", "#### Via the SDK  \n", "  \n", "To test whether your function was successfully uploaded and deployed onto the platform, you can use the `model.predict()` function to predict on a list of item IDs. The function will return an Execution entity, which you can use to check the status of the prediction execution.  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["model = dl.models.get(model_id='<model_id>')\n", "item = dl.items.get(model_id='<item_id>')\n", "", "execution = model.predict(item_ids=[item.id])\n", "# after a few seconds, update your execution from the cloud\n", "execution = dl.executions.get(execution_id=execution.id)\n", "# print the most recent status\n", "print(execution.status[-1]['status'])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you encounter errors, you will need to look at the logs to see where the error occurred.  Go to \"Model Management\", under the \"Deployed\" tab, click on the number in the \"Executions\" column for the appropriate model, and then click on the \"Execution\" log icon on the right side of the screen (the paper icon). Here you can see the output of the cloud machine. You can also access this page via the \"Application Hub\", under \"Executions\".  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}