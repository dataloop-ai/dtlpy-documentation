{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Dataloop Dataset Generator  \n", "The DatasetGenerator is a helper class for the dl.Dataset object.  \n", "  \n", "The generator will to list, get, batch and visualize images and annotations easily.  \n", "  \n", "Here are some use-cases and some usage options for the DatasetGenerator:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["from dtlpy.utilities import DatasetGenerator\n", "import dtlpy as dl\n", "dataset = dl.datasets.get(dataset_id='611b86e647fe2f865323007a')\n", "datagen = DatasetGenerator(data_path='train',\n", "                           dataset_entity=dataset,\n", "                           annotation_type=dl.AnnotationType.BOX)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Object detection examples  \n", "We can visualize a specific item using its index:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["datagen.visualize(idx=10)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Or visualize five random items from the dataset (if `idx` input is None) :  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["for i in range(5):\n", "    datagen.visualize()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To add augmentations, we use [imgaug](https://github.com/aleju/imgaug):  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["from imgaug import augmenters as iaa\n", "import numpy as np\n", "", "augmentation = iaa.Sequential([\n", "    iaa.Resize({\"height\": 256, \"width\": 256}),\n", "    # iaa.Superpixels(p_replace=(0, 0.5), n_segments=(10, 50)),\n", "    iaa.flip.Fliplr(p=0.5),\n", "    iaa.flip.Flipud(p=0.5),\n", "    iaa.GaussianBlur(sigma=(0.0, 0.8)),\n", "])\n", "tfs = [\n", "    augmentation,\n", "    np.copy,\n", "    # transforms.ToTensor()\n", "]\n", "", "datagen = DatasetGenerator(data_path='train',\n", "                           dataset_entity=dataset,\n", "                           annotation_type=dl.AnnotationType.BOX,\n", "                           transforms=tfs)\n", "datagen.visualize()\n", "datagen.visualize(10)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["All of the `DataGenerator` options (from the function docstring):  \n", "  \n", ":param dataset_entity: dl.Dataset entity  \n", ":param annotation_type: dl.AnnotationType - type of annotation to load from the annotated dataset  \n", ":param filters: dl.Filters - filtering entity to filter the dataset items  \n", ":param data_path: Path to Dataloop annotations (root to \"item\" and \"json\").  \n", ":param overwrite:  \n", ":param label_to_id_map: dict - {label_string: id} dictionary  \n", ":param transforms: Optional transform to be applied on a sample. list or torchvision.Transform  \n", ":param num_workers:  \n", ":param shuffle: Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.  \n", ":param seed: Optional random seed for shuffling and transformations.  \n", ":param to_categorical: convert label id to categorical format  \n", ":param class_balancing: if True - performing random over-sample with class ids as the target to balance training data  \n", ":param return_originals: bool - If True, return ALSO images and annotations before transformations (for debug)  \n", ":param ignore_empty: bool - If True, generator will NOT collect items without annotations  \n", "  \n", "  \n", "The output of a single element is a dictionary holding all the relevant information.  \n", "the keys for the DataGen above are: ['image_filepath', 'item_id', 'box', 'class', 'labels', 'annotation_filepath', 'image', 'annotations', 'orig_image', 'orig_annotations']  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["print(list(datagen[0].keys()))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll add the flag to return the origin items to understand better how the augmentations look like.  \n", "Let's set the flag and we can plot:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "datagen = DatasetGenerator(data_path='train',\n", "                           dataset_entity=dataset,\n", "                           annotation_type=dl.AnnotationType.BOX,\n", "                           return_originals=True,\n", "                           shuffle=False,\n", "                           transforms=tfs)\n", "fig, ax = plt.subplots(2, 2)\n", "", "for i in range(2):\n", "    item_element = datagen[np.random.randint(len(datagen))]\n", "    ax[i, 0].imshow(item_element['image'])\n", "    ax[i, 0].set_title('After Augmentations')\n", "    ax[i, 1].imshow(item_element['orig_image'])\n", "    ax[i, 1].set_title('Before Augmentations')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Segmentation examples  \n", "First we'll load a semantic dataset and view some images and the output structure  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["dataset = dl.datasets.get(dataset_id='6197985a104eb81cb728e4ac')\n", "datagen = DatasetGenerator(data_path='semantic',\n", "                           dataset_entity=dataset,\n", "                           transforms=tfs,\n", "                           return_originals=True,\n", "                           annotation_type=dl.AnnotationType.SEGMENTATION)\n", "for i in range(5):\n", "    datagen.visualize()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Visualize original vs augmented image and annotations mask:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(2, 4)\n", "for i in range(2):\n", "    item_element = datagen[np.random.randint(len(datagen))]\n", "    ax[i, 0].imshow(item_element['orig_image'])\n", "    ax[i, 0].set_title('Original Image')\n", "    ax[i, 1].imshow(item_element['orig_annotations'])\n", "    ax[i, 1].set_title('Original Annotations')\n", "    ax[i, 2].imshow(item_element['image'])\n", "    ax[i, 2].set_title('Augmented Image')\n", "    ax[i, 3].imshow(item_element['annotations'])\n", "    ax[i, 3].set_title('Augmented Annotations')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Converting to 3d one-hot encoding to visualize the binary mask per label. We will plot only 8 labels (there might be more on the item):  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["item_element = datagen[np.random.randint(len(datagen))]\n", "annotations = item_element['annotations']\n", "unique_labels = np.unique(annotations)\n", "one_hot_annotations = np.arange(len(datagen.id_to_label_map)) == annotations[..., None]\n", "print('unique label indices in the item: {}'.format(unique_labels))\n", "print('unique labels in the item: {}'.format([datagen.id_to_label_map[i] for i in unique_labels]))\n", "plt.figure()\n", "plt.imshow(item_element['image'])\n", "fig = plt.figure()\n", "for i_label_ind, label_ind in enumerate(unique_labels[:8]):\n", "    ax = fig.add_subplot(2, 4, i_label_ind + 1)\n", "    ax.imshow(one_hot_annotations[:, :, label_ind])\n", "    ax.set_title(datagen.id_to_label_map[label_ind])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setting a label map  \n", "One of the inputs to the DatasetGenerator is 'label_to_id_map'. This variable can be used to change the label mapping for the annotations  \n", "and allow using the dataset ontology in a greater variety of cases.  \n", "For example, you can map multiple labels so a single id or add a default value for all the unlabeled pixels in segmentation annotations.  \n", "This is what the annotation looks like without any mapping:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# project = dl.projects.get(project_name='Semantic')\n", "# dataset = project.datasets.get(dataset_name='Hamster')\n", "# dataset.items.upload(local_path='assets/images/hamster.jpg',\n", "#                      local_annotations_path='assets/images/hamster.json')\n", "dataset = dl.datasets.get(dataset_id='621ddc855c2a3d151451ec58')\n", "datagen = DatasetGenerator(data_path='semantic',\n", "                           dataset_entity=dataset,\n", "                           return_originals=True,\n", "                           overwrite=True,\n", "                           annotation_type=dl.AnnotationType.SEGMENTATION)\n", "datagen.visualize()\n", "data_item = datagen[0]\n", "plt.imshow(data_item['annotations'])\n", "print('BG value: {}'.format(data_item['annotations'][0, 0]))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, we'll map both the 'eye' label and the background to 2 and the 'fur' to 1:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["dataset = dl.datasets.get(dataset_id='6197985a104eb81cb728e4ac')\n", "label_to_id_map = {'cat': 1,\n", "                   'dog': 1,\n", "                   '$default': 0}\n", "dataloader = DatasetGenerator(data_path='semantic',\n", "                              dataset_entity=dataset,\n", "                              transforms=tfs,\n", "                              return_originals=True,\n", "                              label_to_id_map=label_to_id_map,\n", "                              annotation_type=dl.AnnotationType.SEGMENTATION)\n", "for i in range(5):\n", "    dataloader.visualize()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Batch size and batch_size and collate_fn  \n", "If batch_size is not None, the returned structure will be a list with batch_size data items.  \n", "Setting a collate function will convert the returned structure to a tensor of any kind.  \n", "The default collate will convert everything to ndarrays. We also have tensorflow and torch collate to convert to the corresponding tensors.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["dataset = dl.datasets.get(dataset_id='611b86e647fe2f865323007a')\n", "datagen = DatasetGenerator(data_path='train',\n", "                           dataset_entity=dataset,\n", "                           batch_size=10,\n", "                           annotation_type=dl.AnnotationType.BOX)\n", "batch = datagen[0]\n", "print('type: {}, len: {}'.format(type(batch), len(batch)))\n", "print('single element in the list: {}'.format(batch[0]['image']))\n", "# with collate\n", "from dtlpy.utilities.dataset_generators import collate_default\n", "datagen = DatasetGenerator(data_path='train',\n", "                           dataset_entity=dataset,\n", "                           collate_fn=collate_default,\n", "                           batch_size=10,\n", "                           annotation_type=dl.AnnotationType.BOX)\n", "batch = datagen[0]\n", "print('type: {}, len: {}, shape: {}'.format(type(batch['images']), len(batch['images']), batch['images'].shape))\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}