{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Dataset Binding with Azure  \n", "  \n", "We will create an Azure Function App to continuously sync a blob with Dataloop's dataset  \n", "  \n", "If you want to catch events from the Azure blob and update the Dataloop Dataset you need to set up a blob function.  \n", "The function will catch the blob storage events and will reflect them into the Dataloop Platform.  \n", "  \n", "If you are familiar with [Azure Function App](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python), you can just use our integration function below.  \n", "  \n", "We assume you already have an Azure account with resource group and storage account. If you don't, follow the [Azure docs](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create) and create them.  \n", "  \n", "### Create the Blob Function  \n", "1. Create a Container in the created Storage account  \n", "   * Public access level -> Container OR Blob  \n", "NOTE: This container should be used as the external storage for the Dataloop dataset.  \n", "2. Go back to Function App and click Create -> to create a new function  \n", "   * Choose Subscription  \n", "   * Choose your Resource Group  \n", "   * Choose Function Name  \n", "   * Publish -> Code  \n", "   * Runtime stack -> Python  \n", "   * Version -> 3.10 >= Version >= 3.7  \n", "   NOTE: when choosing python 3.7 please pay attention to the AOL warning  \n", "   * Choose Region  \n", "   * Use default values for all other options (OS and Plan ...)  \n", "   * Press next and choose your Storage account  \n", "   * Review and create  \n", "  \n", "### Deploy your function  \n", "In VS code, flow the instructions in [azure docs](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python) to configure your environment and deploy the function:  \n", "1. Configure your environment  \n", "2. Sign in to Azure  \n", "3. Create your local project  \n", "   * On VS code go to Azure panel -> workspace (bottom left panel)-> create a function  \n", "   * Choose the directory location for your project workspace and choose Select.  \n", "    You should either create a new folder or choose an empty folder for the project workspace.  \n", "    Don't choose a project folder that is already part of a workspace.  \n", "   * In Select a template for your project's first function choose -> Azure Event Grid trigger  \n", "   * Open the code file  \n", "   * In the requirements.txt file -> add ```dtlpy```  \n", "   * Replace the code on \\_\\_init\\_\\_.py file with the presented code snippet  \n", "   NOTE: Make sure you **save** the file on Vs code - If not it **will not be deployed** to Azure  \n", "  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import azure.functions as func\n", "import os\n", "", "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n", "", "import dtlpy as dl\n", "", "dataset_id = os.environ.get('DATASET_ID')\n", "dtlpy_username = os.environ.get('DTLPY_USERNAME')\n", "dtlpy_password = os.environ.get('DTLPY_PASSWORD')\n", "container_name = os.environ.get('CONTAINER_NAME')\n", "", "def main(event: func.EventGridEvent):\n", "    url = event.get_json()['url']\n", "    if container_name in url:\n", "        dl.login_m2m(email=dtlpy_username, password=dtlpy_password)\n", "        dataset = dl.datasets.get(dataset_id=dataset_id)\n", "        driver_path = dl.drivers.get(driver_id=dataset.driver).path\n", "        # remove th Container name from the path\n", "        file_name_to_upload = url.split(container_name)[1]\n", "        if driver_path == '/':\n", "            driver_path = None\n", "        if driver_path is not None and driver_path not in url:\n", "            return\n", "        if driver_path:\n", "            remote_path = file_name_to_upload.replace(driver_path, '')\n", "        else:\n", "            remote_path = file_name_to_upload\n", "        if 'BlobCreated' in event.event_type:\n", "            file_name = 'external:/' + file_name_to_upload\n", "            dataset.items.upload(local_path=file_name, remote_path=os.path.dirname(remote_path))\n", "        else:\n", "            dataset.items.delete(filename=remote_path)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Deploy the code to the function app you created - For more info take a look at [azure docs](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python?pivots=python-mode-configuration#deploy-the-project-to-azure)  \n", "5. In VS code go to view tab -> Command Palette -> Azure Functions: Upload Local Settings  \n", "6. Go to the Function App -> Select your function -> Configuration (Under Settings section)  \n", "       * Add the 4 secrets vars `DATASET_ID`, `DTLPY_USERNAME`, `DTLPY_PASSWORD`, `CONTAINER_NAME` (the container to add a trigger)  \n", "    To populate the values for the vars: `DTLPY_USERNAME`, `DTLPY_PASSWORD` you'll need to create a **DataLoop Bot** on your Dataloop project using the following code:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "dl.login()\n", "project = dl.projects.get(project_name='project name')\n", "bot = project.bots.create(name='serviceAccount', return_credentials=True)\n", "print('username: ', bot.id)\n", "print('password: ', bot.password)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["7. Go to Function App -> Select your function -> Navigate in the sidebar to the functions tab and select your function ->  \n", "Integration -> Select the trigger -> Create Event Grid subscription  \n", "    * Event Schema -> Event Grid Schema  \n", "    * Topic Types -> Storage Account (Blob & GPv2)  \n", "    * Select your Subscription, Resource Group, Resource  \n", "    * System Topic Name -> your Event Grid Topic (if you do not have one create it)  \n", "    * Filter to Event Types -> Create and Delete  \n", "    * Endpoint Type -> Function App (Azure function)  \n", "    * Endpoint -> your function  \n", "  \n", "**NOTE:** It will take up to 5 minutes when you deploy using auto upstream  \n", "  \n", "  \n", "**Done! Now your storage blob will be synced with the Dataloop dataset**  \n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}