{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Connect Cloud Storage  \n", "  \n", "  \n", "If you already have your data managed and organized on a cloud storage service, such as GCS/S3/Azure, you may want to  \n", "utilize that with Dataloop, and not upload the binaries and create duplicates.  \n", "  \n", "## Cloud Storage Integration  \n", "  \n", "Access & Permissions - Creating an integration with GCS/S2/Azure cloud requires adding a key/secret with the following  \n", "permissions:  \n", "  \n", "List (Mandatory) - allowing Dataloop to list all of the items in the storage.  \n", "Get (Mandatory) - get the items and perform pre-process functionalities like thumbnails, item info etc.  \n", "Put / Write (Mandatory) - lets you upload your items  \n", "directly to the external storage from the Dataloop platform.  \n", "Delete - lets you delete your items directly from the external storage using the Dataloop platform.  \n", "  \n", "## Create Integration With GCS  \n", "  \n", "### Creating an integration GCS requires having JSON file with GCS configuration.  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "if dl.token_expired():\n", "    dl.login()\n", "organization = dl.organizations.get(organization_name=org_name)\n", "with open(r\"C:\\gcsfile.json\", 'r') as f:\n", "    gcs_json = json.load(f)\n", "gcs_to_string = json.dumps(gcs_json)\n", "organization.integrations.create(name='gcsintegration',\n", "                                 integrations_type=dl.ExternalStorage.GCS,\n", "                                 options={'key': '',\n", "                                          'secret': '',\n", "                                          'content': gcs_to_string})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create Integration With S3  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "if dl.token_expired():\n", "    dl.login()\n", "organization = dl.organizations.get(organization_name='my-org')\n", "organization.integrations.create(name='S3integration', integrations_type=dl.ExternalStorage.S3,\n", "                                 options={'key': \"my_key\", 'secret': \"my_secret\"})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create Integration With Azure  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import dtlpy as dl\n", "if dl.token_expired():\n", "    dl.login()\n", "organization = dl.organizations.get(organization_name='my-org')\n", "organization.integrations.create(name='azureintegration',\n", "                                 integrations_type=dl.ExternalStorage.AZUREBLOB,\n", "                                 options={'key': 'my_key',\n", "                                          'secret': 'my_secret',\n", "                                          'clientId': 'my_clientId',\n", "                                          'tenantId': 'my_tenantId'})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Storage Driver  \n", "  \n", "Once you have an integration, you can set up a driver, which adds a specific bucket (and optionally with a specific  \n", "path/folder) as a storage resource.  \n", "  \n", "## Create Drivers in the Platform (browser)  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# param name: the driver name\n", "# param driver_type: ExternalStorage.S3, ExternalStorage.GCS , ExternalStorage.AZUREBLOB\n", "# param integration_id: the integration id\n", "# param bucket_name: the external bucket name\n", "# param project_id:\n", "# param allow_external_delete:\n", "# param region: relevant only for s3 - the bucket region\n", "# param storage_class: relevant only for s3\n", "# param path: Optional. By default, path is the root folder. Path is case sensitive.\n", "# return: driver object\n", "import dtlpy as dl\n", "project = dl.projects.get('prject_name')\n", "driver = project.drivers.create(name='driver_name',\n", "                                driver_type=dl.ExternalStorage.S3,\n", "                                integration_id='integration_id',\n", "                                bucket_name='bucket_name',\n", "                                allow_external_delete=True,\n", "                                region='eu-west-1',\n", "                                storage_class=\"\",\n", "                                path=\"\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once the integration and drivers are ready, you can create a Dataloop Datsaset and sync all the data:  \n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# create a dataset from a driver name, you can also create by the driver ID\n", "import dtlpy as dl\n", "project: dl.Project\n", "dataset = project.datasets.create(dataset_name=dataset_name,\n", "                                  driver=driver)\n", "", "dataset.sync()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.7"}}, "nbformat": 4, "nbformat_minor": 4}