{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting Dataloop with AWS S3 üåâ\n",
    "\n",
    "Ever wondered how to connect your AWS S3 buckets seamlessly with Dataloop? You're in the right place! Let's walk through the process step by step, from setting up your integration to automating your data sync.\n",
    "\n",
    "## Step 1: Setting Up AWS Integrations üîó\n",
    "\n",
    "Before we can start moving data around, we need to establish a secure connection between Dataloop and AWS. You've got two options:\n",
    "\n",
    "### Option 1: AWS Access-Key Integration üîë\n",
    "\n",
    "Here's the quick way to set up using access keys:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "organization = dl.organizations.get(organization_name='my-org')\n",
    "organization.integrations.create(\n",
    "    name='S3integration',\n",
    "    integrations_type=dl.ExternalStorage.S3,\n",
    "    options={'key': \"my_key\", 'secret': \"my_secret\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Option 2: AWS Cross Account Integration ü§ù\n",
    "\n",
    "For those who prefer the more secure cross-account approach, here's your step-by-step guide:\n",
    "\n",
    "1. Create your S3 bucket\n",
    "2. Set up an IAM policy\n",
    "3. Create the integration and get your Dataloop IAM user:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "if dl.token_expired():\n",
    "    dl.login()\n",
    "# Get your project\n",
    "project = dl.projects.get(project_id='<YOUR_PROJECT_ID>')\n",
    "\n",
    "# Create the integration\n",
    "integration: dl.Integration = project.integrations.create(\n",
    "    integrations_type=dl.IntegrationType.AWS_CROSS_ACCOUNT,\n",
    "    name='<YOUR_INTEGRATION_NAME>',\n",
    "    option={}\n",
    ")\n",
    "\n",
    "# Get the IAM user ARN (you'll need this!)\n",
    "for metadata in integration.meatadata:\n",
    "    if metadata['name'] == 'userArn':\n",
    "        iam_user_arn_value = metadata['value']\n",
    "    elif metadata['name'] == 'status':\n",
    "        integration_status = metadata['value']\n",
    "\n",
    "print('üéØ IAM User ARN:', iam_user_arn_value)\n",
    "print('üìä Integration Status:', integration_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. Create your IAM role\n",
    "5. Add the IAM user to your role's trust relationship\n",
    "6. Update the integration with your role ARN:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the setup with your role ARN\n",
    "integration.update(new_options={'roleArn': '<YOUR_IAM÷π÷π÷π÷π÷π_ROLE_÷π÷πARN>'})\n",
    "updated_integration = project.integrations.get(integrations_id=integration.id)\n",
    "\n",
    "# Verify everything's working\n",
    "for metadata in updated_integration.meatadata:\n",
    "    if metadata['name'] == 'status':\n",
    "        if metadata['value'] != 'trust-established':\n",
    "            raise ValueError('‚ö†Ô∏è Oops! Integration setup needs attention - check your IAM Role trust relationship')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 2: Creating Your Storage Driver üéØ\n",
    "\n",
    "Now that we have our integration set up, let's create a storage driver - think of it as your personal data concierge that connects your specific S3 bucket to Dataloop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "project = dl.projects.get('<project_name>')\n",
    "driver = project.drivers.create(\n",
    "    name='s3_driver',\n",
    "    driver_type=dl.ExternalStorage.S3,\n",
    "    integration_id='<integration_id>',\n",
    "    bucket_name='<bucket_name>',\n",
    "    allow_external_delete=True,\n",
    "    region='eu-west-1',\n",
    "    storage_class=\"\",\n",
    "    path=\"/path/in/bucket\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Once your driver is ready, you can create a dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = project.datasets.create(dataset_name=dataset_name, driver=driver)\n",
    "# sync the dataset \n",
    "dataset.sync()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Step 3: Syncing Your Data üîÑ\n",
    "\n",
    "### Manual Sync ‚ö°\n",
    "\n",
    "Need to sync a specific item? Here's your shortcut:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "# Use the full item path in AWS context\n",
    "item_path = 'external://' + '<Your_item_full_name>'\n",
    "# Optional: specify a destination folder\n",
    "remote_path = '/'\n",
    "dataset = dl.datasets.get(dataset_id='<dataset-id>')\n",
    "dataset.items.upload(local_path=item_path, remote_path=remote_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Automatic Sync with AWS Lambda ‚ú®\n",
    "\n",
    "Want your Dataloop dataset to automatically stay in sync with your S3 bucket? Let's set up a Lambda function!\n",
    "\n",
    "#### Setting Up Your Lambda Function üõ†Ô∏è\n",
    "\n",
    "1. Create a new Lambda function\n",
    "2. Extend the timeout to 1 minute (the default 3 seconds won't cut it!)\n",
    "3. Add these environment variables:\n",
    "   - `DATASET_ID`\n",
    "   - `DTLPY_USERNAME`\n",
    "   - `DTLPY_PASSWORD`\n",
    "\n",
    "Need those credentials? Here's how to get them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "project = dl.projects.get(project_name='project name')\n",
    "bot = project.bots.create(name='serviceAccount', return_credentials=True)\n",
    "print('ü§ñ Bot username:', bot.id)\n",
    "print('üîë Bot password:', bot.password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. The Lambda Code \n",
    "\n",
    "Copy the following code to your lambda function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "# Set dataloop path to tmp (to read/write from the lambda)\n",
    "os.environ[\"DATALOOP_PATH\"] = \"/tmp\"\n",
    "import dtlpy as dl\n",
    "DATASET_ID = os.environ.get('DATASET_ID')\n",
    "DTLPY_USERNAME = os.environ.get('DTLPY_USERNAME')\n",
    "DTLPY_PASSWORD = os.environ.get('DTLPY_PASSWORD')\n",
    "def lambda_handler(event, context):\n",
    "    dl.login_m2m(email=DTLPY_USERNAME, password=DTLPY_PASSWORD)\n",
    "    dataset = dl.datasets.get(dataset_id=DATASET_ID)\n",
    "    driver_path = dl.drivers.get(driver_id=dataset.driver).path\n",
    "    for record in event['Records']:\n",
    "        # Get the bucket name\n",
    "        bucket = record['s3']['bucket']['name']\n",
    "        # Get the file name\n",
    "        filename = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')\n",
    "        remote_path = None\n",
    "        if driver_path == '/':\n",
    "            driver_path = None\n",
    "        if driver_path is not None and driver_path not in filename:\n",
    "            return\n",
    "        if driver_path:\n",
    "            remote_path = filename.replace(driver_path, '')\n",
    "        else:\n",
    "            remote_path = filename\n",
    "        if 'ObjectRemoved' in record['eventName']:\n",
    "            # On delete event - delete the item from Dataloop\n",
    "            try:\n",
    "                dtlpy_filename = '/' + remote_path\n",
    "                filters = dl.Filters(field='filename', values=dtlpy_filename)\n",
    "                dataset.items.delete(filters=filters)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        elif 'ObjectCreated' in record['eventName']:\n",
    "            # On create event - add a new item to the Dataset\n",
    "            try:\n",
    "                # upload the file\n",
    "                path = 'external://' + filename\n",
    "                # dataset.items.upload(local_path=path, overwrite=True) # if overwrite is required\n",
    "                dataset.items.upload(local_path=path, remote_path=remote_path)\n",
    "            except Exception as e:\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "5. Add a Layer to the Lambda  \n",
    "\n",
    "We have created an AWS Layer with the Dataloop SDK ready. Click [here](https://storage.googleapis.com/dtlpy/aws-python3.8-lambda-layer/layer.zip) to download the zip file.  \n",
    "Because the layer's size is larger than 50MB you cannot use it directly (AWS restrictions), but need to upload it to a bucket first.  \n",
    "Once uploaded, create a new layer for the dtlpy env:  \n",
    "a. Go to the layers screen and \"click Add Layer\".  \n",
    "![add_layer](../../../../../assets/bind_aws/create_layer.png)  \n",
    "b. Choose a name (dtlpy-env).  \n",
    "c. Use the link to the bucket layer.zip.  \n",
    "d. Select the env (x86_64, python3.8).  \n",
    "e. Click \"Create\" and the bottom on the page.  \n",
    "  \n",
    "Go back to your lambda and add the layer:  \n",
    "a. Select the \"Add Layer\".  \n",
    "![add_layer](../../../../../assets/bind_aws/add_layer.png)  \n",
    "b. Choose \"Custom layer\" and select the Layer you've added and the version.  \n",
    "c. click \"Add\" at the bottom.  \n",
    "  \n",
    "6. Create the Bucket Events  \n",
    "Go to the bucket you are using, and create the event:  \n",
    "a. Go to Properties ‚Üí Event notifications ‚Üí Create event notification  \n",
    "b. Choose a name for the Event  \n",
    "c. For Event types choose: All object create events, All object delete events  \n",
    "d. Destination - Lambda function ‚Üí Choose from your Lambda functions ‚Üí choose the function you build ‚Üí SAVE  \n",
    "\n",
    "\n",
    "## Need More Help? ü§î\n",
    "\n",
    "For detailed AWS cross-account integration setup, check out our [comprehensive documentation](https://docs.dataloop.ai/docs/integrations-overview).\n",
    "\n",
    "Happy data syncing! üöÄ\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
